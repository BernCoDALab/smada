[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods for Archaeological Data Analysis",
    "section": "",
    "text": "1 Preface\nHallo Welt!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Methods for Archaeological Data Analysis</span>"
    ]
  },
  {
    "objectID": "02-chapter.html",
    "href": "02-chapter.html",
    "title": "2  Introduction into R",
    "section": "",
    "text": "2.1 Start R-Studio\nWhen we first start R Studio, we see a screen divided into several windows. On the left-hand side, directly after the start, we are greeted by the large R window, the Console. This is where the actual R programme is located. On the right, there are windows that provide further helpful functions. In the upper area we have the window in which we can see the working environment. On the one hand, there is the actual environment, marked by the tab ‘Environment’. Next to this, perhaps of interest to us at the moment, is the ‘History’ tab, in which we can see the sequence of commands entered so far. The file manager is located in the lower right-hand corner. Other tabs contain information about diagrams (plots), packages and a window in which we can use the R help system.\nOne important window is still missing: the code or script window. This only appears when we open a new R file. To do this, either click on the plus symbol at the top left or select ‘File -&gt; New File’ from the menu. This opens another window which is placed in the top left by default and in which you enter your programme code for the analyses. This window functions as a normal text editor window, i.e. if you press Enter here, the text is not directly executed, but a new line is created. To actually execute a command, you can either click on the Run symbol in the upper area or use the keyboard shortcut Control Enter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#using-r",
    "href": "02-chapter.html#using-r",
    "title": "2  Introduction into R",
    "section": "2.2 Using R",
    "text": "2.2 Using R\n\n2.2.1 Start of the system:\nAfter R is started, you end on the prompt.\n&gt;\nThis prompt expects your commands. It can be used to directly enter commands or conduct calculations like with a normal calculator. We mainly will not use R in this way.Most of the real work is done using the script window. But we can start trying out our directly using the console window.\n\n\n2.2.2 Simplest way to use: R as calculator\nAs R is an statistical program, of course it can do calculations. We can try that out by entering some basic calculations using the well-known mathematical operators.\n\n2+2\n\n[1] 4\n\n2^2\n\n[1] 4\n\n\n\n\n2.2.3 Multiple commands are separated by ;\nIf we want to enter multiple commands in one line, Either in the console or in the script window, we can separate them by using a semicolon. Each part divided by a; is treated like an individual command and is executed before the next in turn is then executed.\n\n(1 - 2) * 3; 1 - 2 * 3\n\n[1] -3\n\n\n[1] -5\n\n\n\n\n2.2.4 Using functions:\nBeside the basic calculations R also offer us the possibility to do more complex calculations. Here we start using functions in R for the first time. Functions are commands that produce a certain output, most of the time requiring a certain input.The input usually is given by writing it in between the rounds brackets that distinguish a function call from a variable which we will see later. Functions can sometimes take more than one parameter these are then divided by, within the round brackets.\nIn the following example in the first line of “we calculate the square root, In the second example the natural logarithm of 10. If we would like to calculate the living room to the base of 10, we have to specify that using a second parameter.\n\nsqrt(2) #square root\n\n[1] 1.414214\n\nlog(10) #logarith base e\n\n[1] 2.302585\n\nlog(10, 10) #logarith base 10, like log(10, base=10)\n\n[1] 1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#getting-help",
    "href": "02-chapter.html#getting-help",
    "title": "2  Introduction into R",
    "section": "2.3 Getting help",
    "text": "2.3 Getting help\nThere is a specific function for getting help. Not surprisingly this function is called help. It takes as a parameter the name of the function for which you would like to get some information.\nCall of the help function:\n\nhelp(sqrt)\n\nLike it even simpler? You can also use the ‘?’ For getting help instead of writing the function name ‘help’. The name of the function for which you would like to have help it’s written after that ‘?’ .\n\n? sqrt\n\nYou can also search within the help files of R. Research capabilities are a limited only a fulltext search is conducted and you will not get any semantic relevant results. This means that if you would like to search for a specific topic, you probably already should know basically what you are searching for. More complicated searches probably better take place in the Internet. There are plenty of sites where you could get help or explanation how certain analyses are conducted.\nSearching the help:\n\nhelp.search('logarithm')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#assignment-of-data-to-variables",
    "href": "02-chapter.html#assignment-of-data-to-variables",
    "title": "2  Introduction into R",
    "section": "2.4 Assignment of data to variables",
    "text": "2.4 Assignment of data to variables\nA very essential concept in R is the concept of a variable. Variable can be seen as a kind of labelled drawer or replacement for an actual value that can be changed. It can become quite handy if for example you are writing a script or analyses, In which certain values might be changed in individual runs. Here you can define a replacement for the actual value that is the variable and specify the content of the variable for example in the beginning of the analyses. Here it can easily be changed if necessary.\nSetting the value of a variable is also called assignment. If we assign a value to a variable are is not reporting any message back. If we want to see the content of the variable we have to enter this variable itself without any other additions.\nThere are some data shipped with our. We will talk about datasets later. Some inbuilt constants are the letters of the alphabet, the names of the month and also the value of pi.\n\nx &lt;- 2 # no message will be given back\n\nx\n\n[1] 2\n\n\nThere are some data shipped with our. We will talk about datasets later. Some inbuilt constants are the letters of the alphabet, the names of the month and also the value of pi.\n\npi # build in variable\n\n[1] 3.141593\n\n\nWhen selecting variable names you’re quite free to choose. It is necessary, that the name of the variable starts with the letter. You should avoid using mathematical signs, because they could be interpreted as actual calculation. This means, you should not use the minus sign, but you’re perfectly free to use the underscore “_” or the dot “.”.\n\n2.4.1 Arrow or equal sign?\nThere are different options for the assignment sign in our. The traditional one is the arrow composed of a ‘smaller than’ sign and minus sign. Most other programming languages and now also our takes the = as an assignment. What you would like to use as a matter of taste. Personally I’d like the Aero more because it is more speaking and more clear.\nClassic assignment symbol in R is the arrow. Also possible:\n\nx=2\n\nBoth are possible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#working-with-variables",
    "href": "02-chapter.html#working-with-variables",
    "title": "2  Introduction into R",
    "section": "2.5 Working with variables",
    "text": "2.5 Working with variables\nAnd this is helpful to get an overview about which variables we have already Defined. For this in our studio in the right hand area there is the environment window. If we want to get an overview about the assigned variables in our itself, we can use the command ls(). Currently there is only one variable in our environment. That is the variable X that we just assigned.\nDisplay of already uses variables:\n\nls()\n\n[1] \"x\"\n\n\nSometimes it might be helpful to get rid of one of the variables. To do this you can use the rm() command. This stands for remove. The name of the variable that has to be deleted is given within the round brackets ending the function call. If we after the removal of a variable get a listing of the variable environment again the variable should have gone.\nDelete a variable:\n\nrm(x) # no message will be given back\nls()\n\ncharacter(0)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#using-variables",
    "href": "02-chapter.html#using-variables",
    "title": "2  Introduction into R",
    "section": "2.6 Using variables",
    "text": "2.6 Using variables\nAlready have been said a variable can be used instead of an actual value. To do this we simply replace the use of the value with the name of the variable. For example if we want to use a variable when we calculate 2×2 we can at first assign 2 to one Variable and use it instead of actually writing to in our calculation. An important concept is also that the result of the calculation can also be assigned to a variable. With this we can chain analyses together and use the output of one of the functions as the input of the next function. In our example we assign to to the variable x, then we double its value and assign the results to the variable y. The result of this calculation is then used to calculate the square roots using the function sqrt().\nCalculations with variables:\n\nx &lt;- 2\ny &lt;- 2 * x\nz &lt;- sqrt(y) # no message will be given back\n\nNo using the function ls(), We can’t get an overview over our current environment. We should see now the 3 variable that we have created. Additionally if we inspect the individual variables, we shall see that y contains the value of four while z contains the value of two.\n\nls()\n\n[1] \"x\" \"y\" \"z\"\n\ny\n\n[1] 4\n\nz\n\n[1] 2\n\n\n\nGiven is a circle with the radius r=5. Calculate the diameter d (2 \\* r), the circumference u (2 \\* π \\* r) and the area a (π \\* r^2).\n\nAdd area a and circumference u, assign the result to the variable v and delete u and a.\n\n\n\nSolution\n\n\nr &lt;- 5\nd &lt;- 2 * r\nu &lt;- 2 * pi * r\na &lt;- pi * r^2\nv &lt;- a + u\nrm(u)\nrm(a)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#data-types-in-r-variables",
    "href": "02-chapter.html#data-types-in-r-variables",
    "title": "2  Introduction into R",
    "section": "2.7 Data types in R Variables",
    "text": "2.7 Data types in R Variables\nThere are four main data types in R: Scalars, vectors, matrices, data frames.\n\n2.7.1 Scalar\nScalar are individual values. This can be numbers text strings or true/false values. The essential characteristic is that it is only one value that is represented by a scalar.\nExamples of Scalar are all those variables that we used until now.\n\npi\n\n[1] 3.141593\n\n\nAll these variables stored only one value at the time.\n\n\n2.7.2 Vector\nA vector is a variable that holds multiple values at the time in a one-dimensional data structure. You can’t imagine it as a kind of list where every item off the list again is a scalar.\nWe have already seen an example of a vector: the result of the listing of the variables, resulting from the command ls() represents a vector, where every position in this vector holds a scalar information, that is the name of the variable.\n\nls()\n\n[1] \"d\" \"r\" \"v\" \"x\" \"y\" \"z\"\n\n\n\n\n2.7.3 Matrix:\nA vector is a one-dimensional data structure. If we add more dimensions to this idea, we end up with a Matrix. In the simplest implementation you can imagine a matrix as a table with rows and columns. That we have rows and columns represents the two-dimensionality of this data structure. Matrices with more dimensions are easily implementable, although our imagination probably will stop with three dimensions. Most of the time we will use two-dimensional matrices.\nAs with vectors, each element in a matrix represents a scalar value. One of the specific features of the data type matrix in R is, that all values have to be of the same kind. That means with in one and the same metrics, there can only be numbers, characters, or true and false values at once. We can’t mix these types of information in a matrix, which is the difference from the next data structure that we will learn.\nThere are also inbuilt matrices in R, for example are matrix holding the transfer rates between different European currencies. Of course these are restoring values and not updated online all the time\n\neuro.cross\n\n             ATS         BEF         DEM         ESP         FIM         FRF\nATS  1.000000000  2.93161486 0.142135709  12.0917422 0.432093050 0.476702543\nBEF  0.341108927  1.00000000 0.048483759   4.1246012 0.147390797 0.162607493\nDEM  7.035529673 20.62546336 1.000000000  85.0718109 3.040003477 3.353854885\nESP  0.082701069  0.24244768 0.011754775   1.0000000 0.035734557 0.039423810\nFIM  2.314316324  6.78468413 0.328946992  27.9841163 1.000000000 1.103240477\nFRF  2.097744212  6.14977811 0.298164361  25.3653822 0.906420695 1.000000000\nIEP 17.471976881 51.22110711 2.483391826 211.2666399 7.549519785 8.328935807\nITL  0.007106602  0.02083382 0.001010102   0.0859312 0.003070713 0.003387735\nLUF  0.341108927  1.00000000 0.048483759   4.1246012 0.147390797 0.162607493\nNLG  6.244151907 18.30544854 0.887516960  75.5026750 2.698054644 2.976603092\nPTE  0.068636087  0.20121457 0.009755639   0.8299299 0.029657176 0.032718997\n             IEP         ITL         LUF         NLG         PTE\nATS 0.0572345080  140.714229  2.93161486 0.160149851  14.5695951\nBEF 0.0195232016   47.998880  1.00000000 0.054628544   4.9698190\nDEM 0.4026750791  989.999131 20.62546336 1.126739032 102.5048189\nESP 0.0047333550   11.637217  0.24244768 0.013244564   1.2049211\nFIM 0.1324587561  325.657236  6.78468413 0.370637415  33.7186519\nFRF 0.1200633578  295.182459  6.14977811 0.335953424  30.5632839\nIEP 1.0000000000 2458.555749 51.22110711 2.798134501 254.5596294\nITL 0.0004067429    1.000000  0.02083382 0.001138121   0.1035403\nLUF 0.0195232016   47.998880  1.00000000 0.054628544   4.9698190\nNLG 0.3573809621  878.641019 18.30544854 1.000000000  90.9747653\nPTE 0.0039283527    9.658074  0.20121457 0.010992059   1.0000000\n\n\nYou can see that we have rows and columns here and both the rows and columns have names. In this example row and column names are the same because we have a special kind of matrix. But in general the names of the rows and columns can differ from each other. Matrices are specific data types with which you can conduct matrix algebra, which is a specific branch of mathematics that is also used in statistics. We will not deal with this very much. That’s why we most of the time will probably work more with the next data type.\n\n\n2.7.4 Data frame:\nThe fourth of our data types is the data type data.frame. Similar to the matrix, this datatype represents a more than one dimensional data storage unit. Different from the matrix, in data frames values of different kinds can be stored. More specifically the different columns of the data frame can differ in respect of the contains data type. That means we can combine columns that have character values with columns that hold numeric values.\nTables in data frames are usually structured in a specific way: the rules usually hold the item off on investigation or the observations, while the columns usually holds the different features or variables of interest.\nOne example of such a data frame that is inbuilt in our is the data frame mtcars. This data frame contains the technical details of different cars. Also this is more a historical dataset.\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nYou can see, that the real names contains the names of the different cars, identifying them. The column names contains different measurements or information is, there are specific for the individual cars. The individual values identified by row and column then holds to specific values that are unique for this individual item or car.\nData frames are the data type that we will use most of the time, especially if we import data from other sources. How we can do that, will be shown subsequently. But at first we have to make sure, that we get our data from the right location on our computer. For that we need to concept of the working directory.\n\n\n\n2.7.5 The working directory\nHistorically, R is a software that has always been run within the console. Therefore it expects all its inputs from a specific folder on your computer, the working directory. Also, if any output is written to the disk on your computer, this also will take place in the specified working directory.\nOf course this working directory is not fixed, but you can specify that according to your specific workflow. At first we can use the command getwd() to see where on the computer I will working direct with currently is located.\n\ngetwd()\n\nThen we can use the command setwd(\"your/working/directory\") to set this working directory to a specific folder of your computer.\n\nsetwd(\"U:\\R\") # or something else\n\nHow specific folder has to be addressed, depends on the operating system. While Linux and macOS computers treat directory name is more or less the same, in Windows computers the path is prepared by the volume letter. With RStudio, there are different other options how are you can use the graphical user interface to specify the working directory. This might be more convenient than typing the path, especially if you are not used to it. You will find options for this in the files window of our studio, under the icon ‘More’, or in the main menu under the item ‘session’. Change the path according to your needs. Also you can make it a habit to check in the beginning of every R session, what do you work in directory is and if it is correctly specified.\n\n\n2.7.6 Download data for further tasks into your working directory\nIn the reminder of the chapter we will need some files that can be downloaded using the following links:\n\nheight.RData\nkursmatrix.txt\nkursdata.txt\nkursdata.csv\n\nPlease save these files to the directory that you have defined as your working directory. In the following the used example will assume that the files are accessible directly, as they should be if they are placed in the working directory.\nRemember:\n\ngetwd()\nsetwd(\"my/location/of/my/working/directory\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#data-import-through-reading-of-files",
    "href": "02-chapter.html#data-import-through-reading-of-files",
    "title": "2  Introduction into R",
    "section": "2.8 Data import through reading of files",
    "text": "2.8 Data import through reading of files\nData can be imported into R from different formats and sources. The most straightforward version is to directly scan a text file and read it into an R variable. For directly reading in a file we can use the function scan(). The file kursmatrix.txt is a simple text file in which ages and bodies sizes of individuals are listed consecutively. Scan reads in each item and translate it to a position in a vector.\n\nscan(\"kursmatrix.txt\")\n\n [1]  39  34  23  38  23  21  23  31  25  31  24  23  23  39 181 170 185 163 175\n[20] 163 162 172 172 180 187 158 184 156\n\n\nIf we, for example, want to turn this factor into a two-dimensional structure, like a matrix, we can use the command matrix to define such a structure and then use as an input the scanned content of the file. For the command matrix(), One of its parameters is the content that should be turned into a matrix, the second parameter is the number of columns that this matrix should have in end.\n\nkursmatrix &lt;- matrix(scan(\"kursmatrix.txt\"),ncol=2)\n\nThe result is a two-dimensional structure, with two columns, in which body height and age are listed in different columns.\n\nkursmatrix\n\n      [,1] [,2]\n [1,]   39  181\n [2,]   34  170\n [3,]   23  185\n [4,]   38  163\n [5,]   23  175\n [6,]   21  163\n [7,]   23  162\n [8,]   31  172\n [9,]   25  172\n[10,]   31  180\n[11,]   24  187\n[12,]   23  158\n[13,]   23  184\n[14,]   39  156\n\n\nThe file kursdata.txt contains a more complicated data structure. Here we have information is of different kinds, for example strings, but also numeric values. This kind of data can be imported into an data frame. The most general function to import table data is the function read.table().\n\nkursdata &lt;- read.table(\"kursdata.txt\")\n\nOne of the most widely used text file for exchange of numerical and other data are those in the CSV format. This format comes into flavours, Differentiated by the character that separates the columns. The original CSV format has a column separator “,” and a decimal separator using “.”. In European and other countries the “,” it’s often used as decimal separator. Therefore also a CSV2 format exists. Here the column separator is a “;”, while the decimal separator is, “. In Switzerland most of the time we will probably use the CSV2 format. In this format we have the same data available like we have in the kursdata.txt, the file is now called kursdata.csv.\n\nkursdata &lt;- read.csv2(\"kursdata.csv\")\nkursdata\n\n               X age height sex\n1          Bilbo 181     39   m\n2          Frodo 170     34   m\n3        Aragorn 185     23   m\n4        Boromir 163     38   m\n5         Pippin 175     23   m\n6   Gandalf grey 163     21   m\n7          Merry 162     23   m\n8        Samwise 172     31   m\n9        Theoden 172     25   m\n10         Eowyn 180     31   f\n11         Arwen 187     24   f\n12 Gandalf white 158     23   m\n13         Gimly 184     23   m\n14        Gollum 156     39   m\n\n\nIf we read in the data like this, you will realise, that there is a numeric naming, that is automatically given by R. If the dataset already consists of a unique identifier, that is a value, that is not repeated within the whole dataset, and that uniquely identify every individual item of the dataset, this can be used instead of the numeric identifier. This unifier of individual items is called row names in R. So if we specify in the read.CSV2 command, that we want to use for example the first column as row names, we can do it like this.\n\nkursdaten &lt;- read.csv2(\"kursdata.csv\",row.names = 1)\nkursdaten\n\n              age height sex\nBilbo         181     39   m\nFrodo         170     34   m\nAragorn       185     23   m\nBoromir       163     38   m\nPippin        175     23   m\nGandalf grey  163     21   m\nMerry         162     23   m\nSamwise       172     31   m\nTheoden       172     25   m\nEowyn         180     31   f\nArwen         187     24   f\nGandalf white 158     23   m\nGimly         184     23   m\nGollum        156     39   m",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#using-c-for-data-entry",
    "href": "02-chapter.html#using-c-for-data-entry",
    "title": "2  Introduction into R",
    "section": "2.9 Using c() for data entry",
    "text": "2.9 Using c() for data entry\nNow we know how we can assign more complicated data sets two variables by loading them from the file system. Sometimes, it might also be necessary, to directly assign more than one value to a variable. Let’s start with the example of a vector. A vector is created in R using the command c(). This ‘c’ stands for combine, and enables us to combine multiple values to be assigned to a variable, but also for different purposes.\nLet’s assume that we would like to make a vector of different Bronze Age sites. We assign the result to a variable called places.\n\nplaces &lt;- c(\"Leubingen\", \"Melz\", \"Bruszczewo\")\n\nAs in every other situation, in R actual value can be replaced with a variable. Also when we combine values we can not only combine actual values, in this case strings, but we also could use variables and combined them with other variables. To demonstrate that let’s make another vector of side categories that we call categories.\n\ncategories &lt;- c(\"burial\", \"depot\", \"settlement\")\ncategories\n\n[1] \"burial\"     \"depot\"      \"settlement\"\n\n\nNow we can combine these two factors into one.\n\nc(places, categories)\n\n[1] \"Leubingen\"  \"Melz\"       \"Bruszczewo\" \"burial\"     \"depot\"     \n[6] \"settlement\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#named-vectors",
    "href": "02-chapter.html#named-vectors",
    "title": "2  Introduction into R",
    "section": "2.10 Named vectors",
    "text": "2.10 Named vectors\nWe already learnt to concept of row names and column names. Also places in a vector can have a specific identifier, the name. Since vectors do not have rows and columns, this feature is called only called ‘name’. We can use another vector to assign names, or we could directly enter names for the individual positions. In this case we use our category vector as base vector and the sites in the places vector as identifiers.\n\nnames(categories) &lt;- places\ncategories\n\n   Leubingen         Melz   Bruszczewo \n    \"burial\"      \"depot\" \"settlement\" \n\n\nThe result is a vector, in which every position has the name of the site is unique identifier, and where the values are the site categories for this specific archaeological sites.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#applying-functions-to-more-complex-variables",
    "href": "02-chapter.html#applying-functions-to-more-complex-variables",
    "title": "2  Introduction into R",
    "section": "2.11 Applying functions to more complex variables",
    "text": "2.11 Applying functions to more complex variables\nAlso variables with more complex content can, of course, be used in calculations and other functions. Due to their nature, and the fact that they contain more than one value, this of course changes the range of functions that can be applied to them. I will demonstrate that with a reduced version of our data. We will use only a vector of the body height of the individuals.\nFor this we explore a way of loading data into R. This time we use the need to data storage option of R. This format is called ‘RData’, and different from other loading or saving, we do not have to specify a variable name. In this case the variable is stored with its content, and if we load this dataset again, the variable is restored with the same name.\n\nload(\"height.RData\")\nheight\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n          181           170           185           163           175 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n          163           162           172           172           180 \n        Arwen Gandalf white         Gimly        Gollum \n          187           158           184           156 \n\n\nNow we can use this vector that is assigned to the variable name height, to demonstrate some functions that make calculations over all the values that are stored in this vector. The first step probably comes to mind, is to sum up all the values. This can be done in R own using the function sum().\n\n# Sum:\nsum(height)\n\n[1] 2408\n\n\nWe can also count the number of values in the vector. The command for this is length().\n\n# Count:\nlength(height)\n\n[1] 14\n\n\nIf we have the number of cases, and to some of their individual values, we easily can calculate the arithmetic mean.\n\n# Mean:\nsum(height)/length(height)\n\n[1] 172\n\n\nSince this is a very essential statistical value or parameter, of course there exists a specific command for this in R. There is no big surprise that this function is called mean().\n\n# Or more convenient:\nmean(height)\n\n[1] 172\n\n\nOther possible functions might for example be related to the order and the extremes of the values within our dataset. We can sort the dataset according to the values, using the function sort(). In case of numerical values, the items will be sorted according to the numerical order. In case of characters, the items will be sorted according to the character. Our height data on numerical, therefore we will get them sorted from the smallest to the largest person.\n\n# sort:\nsort(height)\n\n       Gollum Gandalf white         Merry       Boromir  Gandalf grey \n          156           158           162           163           163 \n        Frodo       Samwise       Theoden        Pippin         Eowyn \n          170           172           172           175           180 \n        Bilbo         Gimly       Aragorn         Arwen \n          181           184           185           187 \n\n\nImmediately we can identify the smallest and the largest person. But we can also explicitly get the values using the function min() for minimum, and max() for maximum. The function range() gives both values at the same time.\n\n# minimum:\nmin(height)\n\n[1] 156\n\n# maximum:\nmax(height)\n\n[1] 187\n\n# Or both at the same time:\nrange(height)\n\n[1] 156 187",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#calculations-with-vectors",
    "href": "02-chapter.html#calculations-with-vectors",
    "title": "2  Introduction into R",
    "section": "2.12 Calculations with vectors",
    "text": "2.12 Calculations with vectors\nNot only can we use functions on more complex variables like vectors, we also can do calculations. If, for example, we combine a scalar value With a mathematical expression with a vector, the calculation is done at every position of this vector. For example, if we want our height vector in metre, we have to divided by 100. We can directly apply this calculation to the whole variable, and the results will change every individual position in that vector. That means, we divide the variable by 100, and all the items in the variable are then divided by 100, causing every value to be in meter instead of centimeter.\n\nheight.in.m &lt;- height/100\nheight.in.m\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n         1.81          1.70          1.85          1.63          1.75 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n         1.63          1.62          1.72          1.72          1.80 \n        Arwen Gandalf white         Gimly        Gollum \n         1.87          1.58          1.84          1.56 \n\n\nThe case is different if we combine to vectors with a mathematical expression. In this case, the first value of the first vector is combined with the first value of the second vector.The second value of the first vector is then combined with the second value of the second vector, and so forth.\n\ntest&lt;-c(1,2,3,4,5,6,7,8,9,10,11,12,13,14)\nheight.in.m + test\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n         2.81          3.70          4.85          5.63          6.75 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n         7.63          8.62          9.72         10.72         11.80 \n        Arwen Gandalf white         Gimly        Gollum \n        12.87         13.58         14.84         15.56 \n\n\nIn case, that we have different number of positions in the individual factors vectors, the short one is “recycled”. That means, it starts again from the beginning. You can try that out yourself, if you take the example above and remove some items from the test vector.\n\nAn excavation produced the following numbers of flint artefacts:\n\n| flakes | blades | cores | debris |\n| ------ | ------ | ----- | ------ | \n| 506    | 104    | 30    | 267    |\n  \nAssign the values to a named vector, calculate the proportion of the artefacts and sort the vector according to their percentage\n\nDuring the data collection on box with artefacts was missing, the following numbers has to be added to the vector:\n\n| flakes | blades | cores | debris |\n| ------ | ------ | ----- | ------ | \n| 52     | 24     | 15    | 83     |\n\nMoreover were 10 items each artefact type missing. Make a vector for the box, add it and the 10 missing to the original data and repeat the calculations.\n\n\n\nSolution\n\n\nartefacts &lt;- c(506, 104, 30, 267)\nnames(artefacts) &lt;- c(\"flakes\", \"blades\", \"cores\", \"debris\")\n\nprop &lt;- artefacts/sum(artefacts)\nsort(prop)\n\n     cores     blades     debris     flakes \n0.03307607 0.11466373 0.29437707 0.55788313 \n\nmissing_box &lt;- c(52,24,15,83)\nall_artefacts &lt;- artefacts + missing_box + 10\n\nprop &lt;- all_artefacts/sum(all_artefacts)\nsort(prop)\n\n     cores     blades     debris     flakes \n0.04906334 0.12310437 0.32114184 0.50669045 \n\n\nVariant:\nWe also could have over written the content of the artefact variable with the new values including the missing box and the 10 additional items. In that case the court would look like this:\n\nartefacts &lt;- artefacts + missing_box + 10\n\nprop &lt;- artefacts/sum(artefacts)\nsort(prop)\n\n     cores     blades     debris     flakes \n0.04906334 0.12310437 0.32114184 0.50669045 \n\n\nYou see, that artefact is twice present in the first line. This is possible, because the right-hand side of the assignment is evaluated first, and then the result is assigned to the actual variable.\nThis technique can also be used in actual scripts if you don’t need the intermediate values of the variable. It can become quite handy, to reduce the amount of variables and doing names. But you always will have to take care: you lose the intermediate values! So if you have to repeat any step in between, or later you would need some of the intermediate values you will not have them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#sequences-and-repeated-data",
    "href": "02-chapter.html#sequences-and-repeated-data",
    "title": "2  Introduction into R",
    "section": "2.13 Sequences and repeated data",
    "text": "2.13 Sequences and repeated data\nNow we have seen, how we can produce vectors ourselves, and how we can use them in calculations. There are some specific vectors, either consisting of the repetition of an individual value, or sequences of values. There are some inbuilt functions in R that can help you producing these kinds of vectors fast.\nLet’s start with a simple sequence. Let’s assume, that we need the values from 1 to 10. We can produce such a simple sequence rather easily like this:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nBut also more complicated sequences are possible. For this we need an explicit function call for the function seq(). This command takes several parametres, the first one is the starting value, the second one the end value. You can also define the increment using the parameter by, or the desired length of the resulting vector, using the power meter length.\n\nseq(1,10,by=2)\n\n[1] 1 3 5 7 9\n\nseq(1,20,length=5)\n\n[1]  1.00  5.75 10.50 15.25 20.00\n\n\nYou can check out other options and use cases indeed help documentation for this command.\nThe other mentioned option, the repetition, works for letters as well as for numeric values. The command here is rep(). Here, the first parameter is the value that should be repeated. This value can also be a vector. The second para meter is the number of times, that this value should be repeated. Also hear further options can be found in the documentation of the command.\n\nrep(1,10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nrep(1:3,3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\nrep(c(\"Anton\",\"Berta\",\"Claudius\"),3)\n\n[1] \"Anton\"    \"Berta\"    \"Claudius\" \"Anton\"    \"Berta\"    \"Claudius\" \"Anton\"   \n[8] \"Berta\"    \"Claudius\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#data-access",
    "href": "02-chapter.html#data-access",
    "title": "2  Introduction into R",
    "section": "2.14 Data access",
    "text": "2.14 Data access\n\n2.14.1 by index/position\nAnd important possibility is to access data with in such a complex structure like for example a vector. By convention, for accessing data in R, square brackets are used. Indicates of a one-dimensional data structure, within the brackets you can give the position of the item that you would like to access. This can be an individual number, a vector of numbers, Or, by using the minus sign, you can also exclude eighter individual value or a range of values. Here, sequences can become very handy.\n\nheight[1]\n\nBilbo \n  181 \n\nheight[5]\n\nPippin \n   175 \n\nheight[1:3]\n\n  Bilbo   Frodo Aragorn \n    181     170     185 \n\nheight[-(1:3)]\n\n      Boromir        Pippin  Gandalf grey         Merry       Samwise \n          163           175           163           162           172 \n      Theoden         Eowyn         Arwen Gandalf white         Gimly \n          172           180           187           158           184 \n       Gollum \n          156 \n\n\nIf we have a named vector, like for example with our heigth data, these positions have also and unique identifier. In that case, we can also use the unique identifier, to access a specific position in our data storage vector.\n\nheight[\"Arwen\"]\n\nArwen \n  187 \n\n\nThis data access is two ways: not only can we get the values at a specific position, but we can also change the values, given that we indicate a specific position in the vector. In the following example at first the content of the vector height is shown, then we change the entry in the first value, and you can inspect the effect.\n\nheight\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n          181           170           185           163           175 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n          163           162           172           172           180 \n        Arwen Gandalf white         Gimly        Gollum \n          187           158           184           156 \n\nheight[1] &lt;- 168\nheight\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n          168           170           185           163           175 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n          163           162           172           172           180 \n        Arwen Gandalf white         Gimly        Gollum \n          187           158           184           156 \n\n\nOf course the same is true for the access by name.\n\nheight[\"Gimly\"] &lt;- 181\nheight\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n          168           170           185           163           175 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n          163           162           172           172           180 \n        Arwen Gandalf white         Gimly        Gollum \n          187           158           181           156",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#logical-values",
    "href": "02-chapter.html#logical-values",
    "title": "2  Introduction into R",
    "section": "2.15 Logical values",
    "text": "2.15 Logical values\nUntil now we had only vectors or other variables that stored either numeric values or strings. No we learn another category of data type: the logical values. These are also called binary, boolean, or true/false values. These values can result from inequations or checks:\n\npi&gt;4\n\n[1] FALSE\n\nheight &gt; 175\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n        FALSE         FALSE          TRUE         FALSE         FALSE \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n        FALSE         FALSE         FALSE         FALSE          TRUE \n        Arwen Gandalf white         Gimly        Gollum \n         TRUE         FALSE          TRUE         FALSE \n\n\nbut you can also enter them yourself. Logical values are entered as ‘TRUE’ or ‘FALSE’. But there is also a shortcut, ‘T’ or ‘F’ would be enough.\n\nlogic_test &lt;- c(T,F)\nlogic_test == T\n\n[1]  TRUE FALSE\n\nlogic_test == F\n\n[1] FALSE  TRUE\n\n\nAbove you can also see another specific way of how an equation sign is used in our in a comparison. In this situation, two ‘=’ are used to distinguish it from the assignment situation.\nComparisons, and the resulting logical values, can become very helpful when selecting specific values in a dataset. For example, if you want to select all the individuals that are larger than 1 m 75, you can do that by including a comparison in the square brackets used for accessing data. You can also use the command which() to identify in which cases a certain comparison would be true. Lastly, logical values are internally sorted as 0 and 1, and can therefore also be used in calculations or counts. For example, if we want to identify, how many percent of our individuals are larger than 1 m 75, we can sum the results from this comparison. In case that this comparison would return true, it would also return one. By summing up the ones, we get a count. Dividing the count by the number of cases, we get the percentage.\n\nheight[height&gt;175]\n\nAragorn   Eowyn   Arwen   Gimly \n    185     180     187     181 \n\nwhich(height&gt;175)\n\nAragorn   Eowyn   Arwen   Gimly \n      3      10      11      13 \n\nsum(height&gt;175)/length(height)\n\n[1] 0.2857143",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#factors",
    "href": "02-chapter.html#factors",
    "title": "2  Introduction into R",
    "section": "2.16 Factors",
    "text": "2.16 Factors\nThe last type of information are factors. A factor is a codified textual information that is within a very specific range of values. An example for a factor might be the sex of an individual. From the biological determination, this can result in male, female, or undetermined. This means we have only three values. The difference between a factor variable and character variable is, that internally the values are stored as numbers. The table translates then the number to the actual textual representation.\n\nsex &lt;- factor(c(\"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\",\n                \"m\", \"m\", \"f\", \"f\", \"m\", \"m\", \"m\"))\nsex\n\n [1] m m m m m m m m m f f m m m\nLevels: f m\n\n\nAnother specific feature of factor variables is that they can also represent ordered values. We might see this later.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#missing-na-values",
    "href": "02-chapter.html#missing-na-values",
    "title": "2  Introduction into R",
    "section": "2.17 missing (NA) values",
    "text": "2.17 missing (NA) values\nMissing values are annoying in every kind of investigation. They have to be treated in a specific way, distinguishing them from the situation where the value is zero. If we have a value that is zero, this means we have information that the value is actually zero.\nIn our example you can see the effect. If we set the height of an individual person to 0, and then calculate the mean, we get the wrong result.\n\nheight[\"Eowyn\"] &lt;- 0\n\nmean(height)\n\n[1] 158\n\nsum(height)/14\n\n[1] 158\n\n\nSo this can cause problems, if we would use the 0 as an encoding for missing information. For this purpose there is a specific value called ‘not available’ or NA. If we set the value of an individual item to not available NA, and then calculate the mean, the result is NA. This is a warning sign, that in the dataset there are missing cases. We can use the parameter na.rm=T, read NA remove it’s true, to ignore all the NAs and to conduct the calculation of the mean value. This is true for a lot of other functions.\n\nheight[\"Eowyn\"] &lt;- NA\n\nmean(height)\n\n[1] NA\n\nmean(height, na.rm=T)\n\n[1] 170.1538",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#matrices",
    "href": "02-chapter.html#matrices",
    "title": "2  Introduction into R",
    "section": "2.18 Matrices",
    "text": "2.18 Matrices\nWe initially have already talked about the matrices, Two or more dimentional data storage, which also can be used in mathematical procedures. This of course is only true, if the matrix contains numerical values only. And, as we have already seen, do matrices also have names. Since we talk about more dimensional objects, we have to be specific, about which names we talk. That is because in the case of matrices, but also in the case of data frames, we talk about row names and column names.\nWe already have loaded the information about people in the form of the kursmatrix.\n\nkursmatrix\n\n      [,1] [,2]\n [1,]   39  181\n [2,]   34  170\n [3,]   23  185\n [4,]   38  163\n [5,]   23  175\n [6,]   21  163\n [7,]   23  162\n [8,]   31  172\n [9,]   25  172\n[10,]   31  180\n[11,]   24  187\n[12,]   23  158\n[13,]   23  184\n[14,]   39  156\n\n\nThis is already in the conventional representation: the rows contain information about a specific item, the columns contain each specific variable. To make this more clear, we should assign row and column names. Also here, like with the names for vectors, we can use either variables or actual values.\n\nrownames(kursmatrix) &lt;- names(height)\ncolnames(kursmatrix)&lt;-c(\"age\", \"height\")\nkursmatrix\n\n              age height\nBilbo          39    181\nFrodo          34    170\nAragorn        23    185\nBoromir        38    163\nPippin         23    175\nGandalf grey   21    163\nMerry          23    162\nSamwise        31    172\nTheoden        25    172\nEowyn          31    180\nArwen          24    187\nGandalf white  23    158\nGimly          23    184\nGollum         39    156\n\n\nLike with vectors, mathematical operations are possible with matrices. Actually that is a their prime purpose. For example, we can divide a metrics by 100 or any other scalar value. The result will be a matrix, in which every individual value is divided by this scalar, in the specific case 100.\n\nkursmatrix / 100\n\n               age height\nBilbo         0.39   1.81\nFrodo         0.34   1.70\nAragorn       0.23   1.85\nBoromir       0.38   1.63\nPippin        0.23   1.75\nGandalf grey  0.21   1.63\nMerry         0.23   1.62\nSamwise       0.31   1.72\nTheoden       0.25   1.72\nEowyn         0.31   1.80\nArwen         0.24   1.87\nGandalf white 0.23   1.58\nGimly         0.23   1.84\nGollum        0.39   1.56\n\n\nWe can also access individual values within a matrix. This is done in the same way like with vectors. So either, using the position in the form of a number, or by name. Since now we have a more dimensional data object, we also have more dimensions to specify, if we would like to access a specific value. In the case of a two-dimensional matrix, for example, we have to give two positions to identify a specific value. These positions are separated by a comma. General, rows are the first dimension, while columns are the second dimension in our. So rows first, Callums second is a rule, that is applicable for a lot of other situations.\nIf we specify only one of the positions, we refer to either the whole column, or the whole row. The result is then again a vector. Also on this selection, like on every other vector, we can apply mathematical operations.\n\nkursmatrix[, 2] / 100\n\n        Bilbo         Frodo       Aragorn       Boromir        Pippin \n         1.81          1.70          1.85          1.63          1.75 \n Gandalf grey         Merry       Samwise       Theoden         Eowyn \n         1.63          1.62          1.72          1.72          1.80 \n        Arwen Gandalf white         Gimly        Gollum \n         1.87          1.58          1.84          1.56 \n\n\nAlso in this case, if we combine a matrix with a vector, the same logic is to like if we combine to vectors. So if we combine a mattress and a vector, Every value of the vector is combined with every value of the Matrixx starting with the first vector within the matrix. If we combine a matrix and the matrix, then the first value in the first column of the first matrix is combined with the first value of the first column in the second matrix, and so on, equivalent to the way in which vectors are combined.\n\nkursmatrix / c(1:14, rep(2, 14))\n\n                    age height\nBilbo         39.000000   90.5\nFrodo         17.000000   85.0\nAragorn        7.666667   92.5\nBoromir        9.500000   81.5\nPippin         4.600000   87.5\nGandalf grey   3.500000   81.5\nMerry          3.285714   81.0\nSamwise        3.875000   86.0\nTheoden        2.777778   86.0\nEowyn          3.100000   90.0\nArwen          2.181818   93.5\nGandalf white  1.916667   79.0\nGimly          1.769231   92.0\nGollum         2.785714   78.0\n\n\nTo get a feeling for these rules, it is best that you try out different combinations, and observe the results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#data-frames",
    "href": "02-chapter.html#data-frames",
    "title": "2  Introduction into R",
    "section": "2.19 Data frames",
    "text": "2.19 Data frames\nThe last of the major data types, that we have already seen, is the data frame. A data frame results either from the import of a CSV file, or it can be created on the spot in R by combining different vectors in a more dimensional table. These factors also can come from a matrix. For this we used to command data.frame(), which constructs a data frame. The columns are their names are given in this construction, and their values are assigned with an = after this. You aware, that we do not assign actually in this example a variable age with the values of the matrix, but only a column within the data frame. That is one of the reasons, why the syntax using the assignment arrow is more clear, because it differentiate from this construction of the data frame.\n\nkursdata &lt;- \n  data.frame(age = kursmatrix[,1],\n             height = kursmatrix[,2],\n             sex=sex)\nkursdata\n\n              age height sex\nBilbo          39    181   m\nFrodo          34    170   m\nAragorn        23    185   m\nBoromir        38    163   m\nPippin         23    175   m\nGandalf grey   21    163   m\nMerry          23    162   m\nSamwise        31    172   m\nTheoden        25    172   m\nEowyn          31    180   f\nArwen          24    187   f\nGandalf white  23    158   m\nGimly          23    184   m\nGollum         39    156   m\n\n\nAlso in the case of a data frame, very similar to the situation with the matrix, we can access individuals rows or columns by either index or name. In case of the data frame there is specific notation for accessing the content of a specific column: for this we can use the $.\n\nkursdata[,\"age\"]\n\n [1] 39 34 23 38 23 21 23 31 25 31 24 23 23 39\n\nkursdata$age\n\n [1] 39 34 23 38 23 21 23 31 25 31 24 23 23 39\n\n\nLike with matrices, we can use data frames in calculations. Since in a DataFrame also non-numerical values can be stored, this is not always make sense. But we can use the notation above to specify individual columns and assign calculations to them.\nAdditionally a very useful command can be the command summary(). This gives you a summary of the individual columns of data frame, but can also be used with other objects in R. The way in which the summary is conducted might depend on the specific object.\n\nkursdata$height / 100\n\n [1] 1.81 1.70 1.85 1.63 1.75 1.63 1.62 1.72 1.72 1.80 1.87 1.58 1.84 1.56\n\nsummary(kursdata)\n\n      age            height      sex   \n Min.   :21.00   Min.   :156.0   f: 2  \n 1st Qu.:23.00   1st Qu.:163.0   m:12  \n Median :24.50   Median :172.0         \n Mean   :28.36   Mean   :172.0         \n 3rd Qu.:33.25   3rd Qu.:180.8         \n Max.   :39.00   Max.   :187.0         \n\ntapply(kursdata$height, kursdata$sex, mean, na.rm=T)\n\n       f        m \n183.5000 170.0833 \n\n\nThe last line in this piece of code above is an example, how are you can use this $notation very handy in function calls. This example applies to the vector height in the dataset the calculation of the mean, differentiated by the sex, and also ignores potentially NA values. This kind of notation is very close to what you probably will use later on a lot in your actual analyses.\nThere are several datasets inbuilt in R that can be used for experimentation or testing out certain functionalities. You can get a list of this using the command data(). The resulting list might be very long, its length depends on the number of packages that you have installed. The list below only serves as an example. You have to try it out yourself, if you want to have the full list.\n\ndata()\n\n\nData sets in package 'datasets':\n\nAirPassengers           Monthly Airline Passenger Numbers 1949-1960\nBJsales                 Sales Data with Leading Indicator\nBJsales.lead (BJsales)\n                        Sales Data with Leading Indicator\nBOD                     Biochemical Oxygen Demand\nCO2                     Carbon Dioxide Uptake in Grass Plants\nChickWeight             Weight versus age of chicks on different diets\nDNase                   Elisa assay of DNase\nEuStockMarkets          Daily Closing Prices of Major European Stock\n                        Indices, 1991-1998\nFormaldehyde            Determination of Formaldehyde\nHairEyeColor            Hair and Eye Color of Statistics Students\nHarman23.cor            Harman Example 2.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "02-chapter.html#data-export-through-save",
    "href": "02-chapter.html#data-export-through-save",
    "title": "2  Introduction into R",
    "section": "2.20 Data export through save",
    "text": "2.20 Data export through save\nFinally, if we finished our analyses, most of the time we also need to export some data. This is an analogy to the options to read data into our. The most basic option would be to directly write a simple text file. With this option you lose a lot of the internal structure of the dataset, and it is not for certain, that it will be imported in the right way.\n\nwrite(kursmatrix,\"kursmatrix.txt\")\n\nSpecially, if you have a data frame, it makes more sense to write it directly as a table. With the command wright.table() you can specify a lot of options how the dataset will be stored. If you’d like to know, please consult do you documentation.\n\nwrite.table(kursdata,\"kursdata.txt\")\n\nBut most of the time, you will probably not need all the flexibility, that ride table gives you. Most of the time, you will like to write a CSV file, because this is the standard exchange file between R and a lot of other software, including spreadsheet software like Microsoft Excel.\n\nwrite.csv2(kursdata,\"kursdata.csv\")\n\nAs we have said earlier, please pay attention to the language setting off your computer. Most of the time, at least in Switzerland, you will have a European continental setting, where the decimal separator is a comma. In that case, it is likely that you would like to use the CSV2 format. To try out the differences, you can run the cockpit below, and open the resulting file in your spreadsheet software. You can also inspect it with a text editor.\n\nkursdata$height &lt;- kursdata$height/100\nwrite.csv(kursdata,\"kursdata.csv\")\n\nYou very likely will have problems with Microsoft Excel. Over spreadsheet software might be smarter. Nevertheless, if you are on the continent, you would like to save your data in the way like below.\n\nwrite.csv2(kursdata,\"kursdata.csv\")\n\nOf course, R also offer us packages, that directly can save files in .XLSX format. The downside of these packages are, that most of the time they require additional dependencies or programming languages, for example perl or python. And actually using the CSV format this is not necessary at all. So it is best that you develop the habit to use CSV as you exchange format between different software.\nWith this, now I hope, that we have everything at hand, so that we can start using R as actual software. In the next chapter we will start producing that part of statistics, that most people think of when statistics are mentioned: graphs, diagrams, and tables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction into R</span>"
    ]
  },
  {
    "objectID": "03-chapter.html",
    "href": "03-chapter.html",
    "title": "3  Explorative statistics & graphical display",
    "section": "",
    "text": "3.1 Dataset used for this chapter\nThe dataset, that we are using for this chapter, comes from the burial-ground of Münsingen/Rain. Maybe in the later version of this book I will give more details to the state of said. For the time being every Swiss archaeologist should have a slight idea what is burial-ground consist of. You can download the data using the following link: muensingen_fib.csv.\nThe data is such represents different fibulae found on the side. Please download the file and save it into a directory of your choice. For this chapter, like with all of the chapters, you might like to specify a specific folder for this chapter. Save the data there, and, as we have learnt in the last chapter, select this folder as your current working directory.\nIf you have downloaded the data to this folder and correctly selected it as you’re working directory, you should be able to reach the Münsingen data and inspect its structure. Since it is a dataset in the “Continental” CSV file, you can load the data into are using to read.csv2() format. Also you will realise, that the dataset already contains row numbers. You might like to specify ‘row.names = 1’.\nmuensingen &lt;- read.csv2(\"muensingen_fib.csv\", row.names = 1)\nhead(muensingen) # For getting a glimpse to the data\n\n   Grave Mno FL BH BFA FA CD BRA ED FEL  C   BW  BT FEW Coils Length\n1    121 348 28 17   1 10 10   2  8   6 20  2.5 2.6 2.2     4     53\n2    130 545 29 15   3  8  6   3  6  10 17 11.7 3.9 6.4     6     47\n3    130 549 22 15   3  8  7   3 13   1 17  5.0 4.6 2.5    10     47\n8    157  85 23 13   3  8  6   2 10   7 15  5.2 2.7 5.4    12     41\n11   181 212 94 15   7 10 12   5 11  31 50  4.3 4.3  NA     6    128\n12   193 611 68 18   7  9  9   7  3  50 18  9.3 6.5  NA     4    110\n   fibula_scheme\n1              B\n2              B\n3              B\n8              B\n11             C\n12             C\nThe dataset originally comes from the R package archdata. It is a data frame consisting of 30 observations with some variables describing the characteristics of the fibulae. If you want to full description of what the state of me, I suggest that you consult the documentation of the ice data package. There this dataset is called Fibulae. While we will graphically display different variables, I will explain what is variables mean.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#cross-tables-contingency-tables",
    "href": "03-chapter.html#cross-tables-contingency-tables",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.2 Cross tables (contingency tables)",
    "text": "3.2 Cross tables (contingency tables)\nThe first category of visual representation of data is actually not a diagram. It is the table. Tables are today very widespread for the representation of information, so I don’t think that I will need to explain to you what the table looks and our table works. A bit formalised, how we use tables here, is that in most of the cases the rules will hold the items of investigation, while the columns will contain different variables.\nBut this is not true all of the time. For a very specific type of table, that we will learn to know now, this is not true. The kind of table, that I’m talking about, is the contingency table. This kind of table is also known as cross tabulation, or crosstab. This kind of representation is used to show the interrelation between two variables. That means, contrary to what I have stated in the paragraph above, did hear both the rows and columns represent variables.\nLet’s have a look to one of these cross tabs, so it becomes clear, what is meant by that. For this, we will tabulate the scheme of the fibula against the grave in which it was found. Fibula Scheme is a standardised way of how different types of fibulae are are produced, they represent archaeological types. In one grave there may be more than one fibula. Therefore, the great number does not represent a unique identifier of the object fibula, the item of the investigation. It is just one variable among others.\nIn R, you can use to come on table to display the number of fibula in a specific fibula scheme per grave.\n\nmy_table &lt;- table(muensingen$fibula_scheme, muensingen$Grave) \nmy_table\n\n   \n    6 23 31 44 48 49 61 68 80 91 121 130 157 181 193\n  A 1  1  1  1  0  0  0  0  0  0   0   0   0   0   0\n  B 0  0  0  0  1  1  2  1  1  1   1   2   1   0   0\n  C 0  0  0  0  0  0  0  0  0  0   0   0   0   1   1\n\n\nAs you can see, the variable muensingen$fibula_scheme is given us the first parameter of the function, while the other variable muensingen$Grave represents the second parameter. The result is the table, in which the first parameter is mapped in the rows, while the second parameter, the grave number, is mapped to the columns. Each cell now represents the number of items, in this case of specific fibula types, in each burial. More abstract speaking, crosstab is the representation in which the current occurrence of two variable values are mapped.\nIf we also want to have an idea, how many items there are per row and how many per column, we might like to add the margins to the table. Table margins gives us to sum of the values for each role, each column, and in total. An R, the command for that is addmargins().\n\naddmargins(my_table)\n\n     \n       6 23 31 44 48 49 61 68 80 91 121 130 157 181 193 Sum\n  A    1  1  1  1  0  0  0  0  0  0   0   0   0   0   0   4\n  B    0  0  0  0  1  1  2  1  1  1   1   2   1   0   0  11\n  C    0  0  0  0  0  0  0  0  0  0   0   0   0   1   1   2\n  Sum  1  1  1  1  1  1  2  1  1  1   1   2   1   1   1  17\n\n\nYou can see, that we have a new row and a new column. The row contains the sum per column, while the column contains the sum per row. In the lower right most sell we have to total sum of all items.\nIn data sets of low dimensionality, this represents a rather straightforward and convenient way to investigate the relationship between two, probably more, variables. They also represent a starting point for different other statistical approaches and techniques, for example the Chi-square test, that we will learn about later. In the context of spreadsheet software, crosstabs are often also called pivot table.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#basics-about-charts",
    "href": "03-chapter.html#basics-about-charts",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.3 Basics about charts",
    "text": "3.3 Basics about charts\nBesides the tables, which also contains some graphical elements, like lines, the visualisation, which comes to the minds of most people in respect of statistics, are charts. Most charts or diagrams contain a certain set of elements, that can repeatedly be seen even with different types of graphical display. Most of the time we have some axis, which represents the structure of the variable underlying to representation. Quite often, this access has some marks, most of the time regularly spaced, and often also with some annotation. These marks are called tick marks. For the representation of one variable, one axis might be enough. But most of the time, we have a two-dimensional visualisation. This is already necessary, if we want to represent the category of some items and the count of items in that specific category. Very often, we have charts that represents the relationship of two variables. In both cases, we have two axis. In the area, that is defined by the axis, we have to representation of the actual data. This takes place, using different symbols, lines, or other graphical elements. Here, different types of charts different. Very often, we have also label for the axis, labels for the whole plot, and sometimes some subheadings describing more in detail, what the plot is about.\n\nThere are certain rules or guidelines, how shots should be designed, to be most efficient. Edward Tufte, a professor emeritus of statistics and computer science at University, is well known for his publications in respect to data visualisation. In one of his publications, he defined the principles for a good graphical representation of information.\n\nGraphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. E. Tufte 1983\n\nJust rule of thumb it also known as the data-ink-ratio, the ‘proportion of a graphic’s ink devoted to the non-redundant display of data-information’. As these are only guidelines, it is clear, that whole details are charged will be very much depends on its use case. But it’s also clear, that one should aim for a reduced use of graphical elements, so that the information, that needs to be transmitted, is in the foreground. Also, certain bells and whistles might enhance this information transmission quite a bit. I trust your sense of style to choose the right amount of ink for the right purpose.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#the-command-plot",
    "href": "03-chapter.html#the-command-plot",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.4 The command plot()",
    "text": "3.4 The command plot()\nThe command plot() is the basic commands in R to produce a graphical visualisation. What happens, when you do use plot with different data sets very much depends on, what kind of dataset you have. This command is a kind of chameleon, changing its appearance according to the necessities. This philosophy is also true for different other commands in R. There are some standardisations, that comparable effects should result from comparable names. So, whenever you use the command plot(), a plot will be the result. How this plot looks like, depends on the package, from which the data structure is coming, that you are plotting. The same is true for example for the command summary(). This command gives a summary, least surprisingly, good structured according to the data underline the summary.\n\n3.4.1 Basic plotting\nLets come back to our command plot(). It’s a basic manifestation can be seen, when we plot one variable. For this purpose, let’s take the length of the fibulae from our dataset.\n\nplot(muensingen$Length)\n\n\n\n\n\n\n\n\nYou can see, that like with most other commands in R, within the brackets the parameters are written. Here it is a reference to the variable, that should be plotted. You also should be able to see elements from the general outline of the plot, that we just have introduced. In this basic implementation, the values to be plotted are visualised using the Y axis, while the X axis represents the order of the values in the vector. The values in the dataset themselves are represented as points, positions according to their actual value on the Y axis, and to their order in the dataset on the X axis. That is why the label of the X axis is index.\nSome standard layouts in respect to specific data can be selected using the parameter ‘type’. For example, type=\"p\" is the default setting and results in the plots that we just saw. Specifying type=\"b\" results in a plot of the points connected by lines, as you can see below.\n\nplot(muensingen$Length, type = \"b\")\n\n\n\n\n\n\n\n\nBut this kind of visualisation is not correct here. Line implies that there is a continuous process going on, which is not the case between the individual values of our unconnected similar. Better representation of the nature of our data might be, if we are using the parameter type=\"h\", That gives us vertical lines from the origin of our coordinate system to the actual value.\n\nplot(muensingen$Length, type = \"h\")\n\n\n\n\n\n\n\n\nBelow you can find a list of possible options:\n\np – points (default)\nl – solid line\nb – line with points for the values\nc – line with gaps for the values\no – solid line with points for the values\nh – vertical lines up to the values\ns – stepped line from value to value\nn – empty coordinate system\n\nThe option \"n\", although seemingly quite useless, will become one of the most interesting options to be selected here. This option can be used, to draw a coordinate system, without filling it in the first place. In this way, we can ask R to draw to coordinate system, that we can then fill with our own symbols or other graphical elements.\nAs I said, the command plot() is a chameleon that changes its appearance according to the dataset. For example, if we used to command on a dataset containing factor variables, the resulting visualisation will be a bar chart, counting the number of items per category, instead of the kind of visualisation that we have seen with the length data.\n\nplot(as.factor(muensingen$fibula_scheme))\n\n\n\n\n\n\n\n\n\n\n3.4.2 Enhancing the plot with optional components & Text\nOf course, we can influence all the different elements of shops in such a flexible software like R. For example, we can specify the size of our X and Y axis, we can change the labels of both axis and also the heading and the subtitle of the chart. This is done using different parameters. By adding up so many parameters, the commands can become quite intimidating. But it is essentially just adding up parameter by parameter. So from a structural point of view there is no complex logic behind that. Also, when writing R commands, You can always us new lines behind elements that indicate, that our command is not finished yet. Such elements might be a mathematical symbols or for example commas like you can see below.\n.tiny[\n\nplot(muensingen$Length, muensingen$FL,\n     xlim=c(0, 140), # limits of the x axis\n     ylim = c(0, 100), # limits of the y axis\n     xlab = \"Fibula Length\", # label of the y axis\n     ylab = \"Foot Length\", # label of the x axis\n     main = \"Fibula total length vs. Foot Length\", # main title\n     sub=\"example plot\" # subtitle\n     )\n\n\n\n\n\n\n\n\n]\nSo you can see the effects of the different parameters. The extent of the axis is defined by xlim and ylim. The labels of the axis is given by xlab and ylab. The explanatory headings are defined by main and sub. Also, this plot represented by various plots, in which we met do you already know length of the fibula against the length of the foot of the fibula. It is quite obvious, that the longer the fibula is, the longer also its foot by be. So using this kind of so-called scatterplot, we can visualise this relationship between the two variables.\nPlot is doing a lot for you:\n\nOpens a window for display\nDetermines the optimal size of the frame of reference\nDraws the coordinate system\nDraws the values\n\nIn the background, also the last plot is remembered, and this plot is still changeable. You can use specific commands, to add elements to the already existing plot. These elements can be:\n\nlines – additional lines to an existing plot\npoints – additional points to an existing plot\nabline – additional special lines to an existing plot\ntext – additional text on choosen position to an existing plot\n\nThis is the reason, why it sometimes might be reasonable, to plot an empty plot using the option “n”. You can use this version, to add first create a coordinate system, and then fill it up with lines or points.\nThere are some more possibilities to change the layout and style of the plots. For example, the command par() will give you the tools to change a lot of the look of and feel. I suggest that you look up the help page for this command to see which possibilities exists to change the layout of the plot.\n\n? par\n\nAs an example, how you can add elements to an existing plot, we will draw some straight lines into the plot of the total length versus the foot length of the fibula. At first, we will draw a line in red at the mean value of the length of the fibula. Since the length is on the X axis, the line with the mean of the length must be a vertical line going up. Accordingly, the mean of the foot length, that is represented by the Y axis, must be a horizontal line. For both, we are using to command abline(). The difference is, that for vertical line specify the parameter ‘v’, for a horizontal line we specified a perimeter ‘h’. In both cases the parameter ‘col’ specifies the colour of the line. The third line is somehow special: it represents the relationship between the foot length and the total length in the data. Therefore, it is a diagonal line. It is defined by linear model of these two parameters. What this means we will explain later, for now you can just keep in mind, that in this way we can represent the trends in the data, drawing a trend line.\n\nabline(v = mean(muensingen$Length), col = \"red\")         # draw a red vertical line\nabline(h = mean(muensingen$FL), col = \"green\")           # draw a green vertical line\nabline(lm(FL~Length, data = muensingen), col = \"blue\")   # draw a blue diagonal line",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#export-the-graphics",
    "href": "03-chapter.html#export-the-graphics",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.5 Export the graphics",
    "text": "3.5 Export the graphics\nOf course, if we have created a decent plot, we probably don’t want it to remain in R. Most of the time, we would probably like to use it in other contexts, be at homework, an article or a presentation. To do that, we need to wait to explore our plots.\nUsing RStudio, probably the easiest way is to be used to graphical user interface, especially when it comes to exporting only individual plots. For this, in the plots window, there is a button called export.\n\nHere, you can select if you want to export your plot as an image (roster image), or as a PDF (vector file). The export windows should be more or less self-explanatory, you can specify the size of the resulting image and also the location, where it should be saved.\n\nBefore RStudio, saving plots took place most of the time using commands from the command line. This still is very useful, if you use scripts and want to create multiple or even very many plots at once, changing the inputs data. Two very common formats when it comes to vector format or ‘PDF’ or probably more common ‘eps’ specifically for images. You can copy your current plot window to a vector file using the command dev.copy2...() and then the file format in which the result should be saved. So let’s save our current plots to both formats using to come on spill.\n\ndev.copy2eps(file=\"test.eps\")\ndev.copy2pdf(file=\"test.pdf\")\n\nThere are even more raster then vector file formats. By default, R is capable of exporting to PNG, JPEG, TIFF and BMP. You can use the command savePlot() for this.\n\nsavePlot(filename=\"test.tif\", type=\"tiff\")\n\nIf you really plan to use the graphical visualisation in R in that way, it is worthwhile to dive deeper into the export format and options then we can present here. For most of the basic use cases, exporting the files via the graphical user interface is the most convenient and most controllable way of storing your valuable plots on your file system.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#pie-chart",
    "href": "03-chapter.html#pie-chart",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.6 Pie chart",
    "text": "3.6 Pie chart\nLet’s not start discussing different plot types. We will begin with one of the most widespread used type of plots: the pie chart. You can see pie charts all over the media, newspapers, and also books, might it be scientific or popular books. Pie charts are used to display proportions of the total. For this reason, they are most suitable (if at all) for nominal data. Or, of course, percentage data, if this is the original data type. For example, results of the election that often represented in pie chart. Here, we see the percentage of voters voting for a specific party. Basically, this represents normal data, the choices of the individuals for one or the other party.\nI would like to opportunity to throw in an unnecessary formula here:\n\\[\na_i = \\frac{n_i} {N} * 360°\n\\]\nThe proportions of the categories \\(i\\) in relation to the total number, represented by the number of items of that category \\(n_i\\) divided by the total number \\(N\\) is multiplied by 360°. The resulting angle is the angle, which can be used for a circular visualisation of this amount.\nPie charts have different disadvantages, some they share with other graphical representation of data, but some are unique to this kind of display. Examples to colour selection very influential when it comes to perception. Red as an aggressive colour is perceived larger then for example grey. Very specific to a pie chart is the fact that we as humans are more trained to see differences in length then in area. And in the pie chart, the differences are visualised using the area of the different pieces of the pie. This results in effect, that small differences are not so easily visible in a pie chart, then this would be the case for example with a bar chart, a visualisation technique that we will learn about below.\nLike in many other cases, 3-D representation does not work so well on printed paper. Since the human eye and brain must compromise between both the perception that we are looking at a 3-D image but in reality it’s a 2-D object, differences are distorted. Let’s look at the following example that I took from the literature:\n\n.caption[source: http://www.lrz-muenchen.de/~wlm]\nThe pieces »viel zu wenig«, »etwas zu wenig« und »gerade richtig« have exactly the same size, the piece »viel zu viel« is a bit smaller. Perception wise, the different shares seem to be quite different because of the reasons mentioned above. So in any case, 3-D or not, pie charts are inferior to a lot of other visualisation techniques. Nevertheless, because it is a very widespread used technique, still I would like to demonstrate how are you can create them using R.\nThe actual commands to draw a pie chart in R is pie(). This command expects the number of counts, and all the normalisation to percentages and then the visualisation will take place automatically. This means, that it might be necessary to recode data. In this case, we will use the fibula schemes and visualise there ratio. This variable comes in the form of a character vector indicating to different schemes. Here, we can use the command table, to transform the nominal presence into a table of counts.\n\ntable(muensingen$fibula_scheme)\n\n\n A  B  C \n 4 11  2 \n\npie(table(muensingen$fibula_scheme))\n\n\n\n\n\n\n\n\nDo you original colour scheme of the pie() command is rather pastel. Of course, you can change the colours, using the col parameter. Here, in the order of their appearance, you can specify different colours, that will be used to represent this category.\n\npie(table(muensingen$fibula_scheme),\n    col=c(\"red\",\"green\",\"blue\"))\n\n\n\n\n\n\n\n\nThis should be enough to be set for the pie chart. It had already had the honour to be the first mentioned. That’s enough. We will from now on turn to more scientifically useful visualisations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#bar-plot",
    "href": "03-chapter.html#bar-plot",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.7 Bar plot",
    "text": "3.7 Bar plot\nA worthy replacement for any kind of pie chart is a bar chart or a plot. Most of the time it is the better alternative. Since here, the differences are represented by the length of the bars, humans can more easily perceive the differences between different categories. Also, this kind of visualisation is more flexible, because you can also represent absolute data and measurements with a bar chart, not only percentages.\nAlso, the command barplot() requires a vector containing the data. This can be the number of fibulae in the different styles, like so:\n\nbarplot(table(muensingen$fibula_scheme))\n\n\n\n\n\n\n\n\nOr it can be length of the different fibulae.\n\nbarplot(muensingen$Length)\n\n\n\n\n\n\n\n\nBoth versions are meaningful and can help visualising the data. Although especially the last plot is difficult to understand, because it it lacking some essential information. For example, what is represented here by the bars, in total, but also, what does the individual bars represent. Since the vector resulting from the table commands automatically had names, these names were used in the case of the fibula scheme. In the case of the length, we have to specify these names on our own. For this, there is the parameter “names.arg”. Also, you might like to turn the names so that they become more readable and do not flow into each other. For this, you can use the parameter “las=2”. Lastly, I’ll give you a variant of how you can put the main title to a plot using to command title().\n\npar(las=2)                          # turn labels 90°\nbarplot(muensingen$Length,          # plot fibulae length\n        names.arg=muensingen$Grave) # with names of the graves\ntitle(\"Fibulae length\")             # add title\n\n\n\n\n\n\n\n\nOf course, you can also turn the bar chart around, making it horizontal. In that case, you probably would like to turn the labels again. Also, you can influence the size of text using a parameter ‘cex’. You can also specify what should be changed in size, in this case the names.\n\npar(las=1)                          # turn labels back again\nbarplot(table(muensingen$fibula_scheme), # Plot counts fibulae scheme\n        horiz=T,                         # horizontal\n        cex.names=2)                     # make the labels bigger\n\n\n\n\n\n\n\n\nBar charts are also much more flexible compared to pie charts in so far, as you can easily display more than two variables. In this way, your plot can become 3-D or more in a meaningful way.\nLet’s assume, we want to visualise the number of coils of the fibula in relationship to the style. We can use the table command to produce a table accordingly.\n\nmy_new_table &lt;- table(muensingen$fibula_scheme,\n                      muensingen$Coils)\nmy_new_table\n\n   \n    3 4 6 10 12\n  A 1 3 0  0  0\n  B 0 3 6  1  1\n  C 0 1 1  0  0\n\n\nNo, we can directly put this table into the barplot() command. Let’s see the output:\n\nbarplot(my_new_table)\n\n\n\n\n\n\n\n\nYou can see, that the different styles (fibula schemes) get different colours. With this, we not only seeing the number of items in respect to the different number of coils, but also at the same time, in which style to fibula is produced. This way of representing subcategory it’s called ‘stacked’. If you don’t like this way of representation probably you like more the version where the different categories are put side-by-side like below.\n\nbarplot(my_new_table, beside=T, legend.text=T)\n\n\n\n\n\n\n\n\nUntil now, we have only seen bars of different height. The beauty of the pie command was, that it automatically transformed absolute values into percentages. With a slight alteration of the table commands, we can achieve the same with a bar chart, and even better. For that, we are using the prop.table() command. This stands for proportional table. In current versions of R, you can use also the command proportions(). Its first parameter is the dataset for which the proportion should be calculated, in the format of the table. That means, it takes the result of the table command, and then transform it into a proportional table. The second parameter defines, what should sum up to 100, or in other words, what is the full total to which the proportion should be calculated. As always in R rows come first, so they have the number 1, while columns come second, so they have the number 2. That means, the following command will calculate the percentages in respect to the columns, which each will sum up to 1.\n\ntable.prop&lt;-prop.table(my_new_table,2)\ntable.prop\n\n   \n            3         4         6        10        12\n  A 1.0000000 0.4285714 0.0000000 0.0000000 0.0000000\n  B 0.0000000 0.4285714 0.8571429 1.0000000 1.0000000\n  C 0.0000000 0.1428571 0.1428571 0.0000000 0.0000000\n\n\nIf we put the results into the barplot() command, we will get a result comparable or even better to the pie chart.\n\nbarplot(table.prop)\n\n\n\n\n\n\n\n\nOf course, you can also change elements of the bar plot. For example, you can get fancy with the colours. Here, are used to command rainbow, specified with the number of colours I would like to get, to create a colour spectrum from the rainbow. Also, I would like to have a legend. Additionally, I want to have a title. This title is quite long, so I divided it in two rows, using the special “\\n” sign. Since it is such a long title, I also have to use space outside of the actual plot area. You can get a feeling for the effects of this different para metres by playing around a bit with them.\n\nbarplot(table.prop,\n             legend.text=T,  # add a legend\n             col=rainbow(3)  # make it more colorful\n             )\n\n# add a title\ntitle(\"ratio of fibulae schemes \\n by number of coils\",\n      outer=TRUE,            # outside the plot area\n      line=- 3)              # on line -3 above\n\n\n\n\n\n\n\n\nBut also bar plots do not solve every problem that we have with graphical representation. For example, there is often the question, what is better: percentage or absolute numbers. If we would like to compare different situations, with different total numbers, and we are most interested in the ratios, then the percentages are a good choice. But at the same time, due to this characteristic, they can hide differences in the underlying total numbers. This can become especially problematic, if you divide your bars also in subcategories.\n\npar(mfrow=c(2,1))\nbarplot(my_new_table,beside=T)\nbarplot(table.prop,beside=T)\n\n\n\n\n\n\n\n\nJust from the visualisation of the percentages here it seems, as if there are more fibulae of scheme A with three coils then with four. So it is absolutely necessary to always provide the absolute numbers, if you present your data as percentage of the total. It can take place in the caption, or directly in the plot. And this problem or better this consideration must be taken into account not only with bar charts, but also with any other representation of percentages.\nAnother source of visual confusion can be the scales already ranges of the axis. For example, if we do not draw an access from 0 to the maximum value, but let it start at an arbitrary value, small differences can visually become very big. In the example below, I visualise the first and the second fibula respectively their length.\n\npar(mfrow=c(1,2))\nbarplot(muensingen$Length[1:2],xpd=F,ylim=c(45,55))\nbarplot(muensingen$Length[1:2],xpd=F)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nAlthough it is obvious, if you actually look at the axis, there is a difference it’s not that big (it’s only six), only from visual inspection it seem to be enormous compared to the visual representation of the same difference in the diagram to the right. Most of the time, it is better to have your axis ranging from 0 to the actual values, except for situations, where the comparison between different bars or other elements is hindered by the fact that the relative differences are so little.\nYou might have realised, that in the last two examples are used to command par(mfrow=c(..)). With this comment, you can find that plots are placed side-by-side or one on top of the other. Also here do usual rule of R present: Rows are the first number, columns are the second number. So both I said that I want to have two rows of plots with one column, below I said I want to have one row with two columns. You can use this to lay out your plots in a smarter way.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#box-plot-box-and-whiskers-plot",
    "href": "03-chapter.html#box-plot-box-and-whiskers-plot",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.8 Box-plot (Box-and-Whiskers-Plot)",
    "text": "3.8 Box-plot (Box-and-Whiskers-Plot)\nThe next type of plot is not totally dissimilar to the bar plot. Also in this case, in the case of the box plot, we have a rectangular element representing our data. But the logic behind the box plot is very different.\nA box plot, which is also called box and whisker plot, is used to describe the distribution of values in a range of data. Let’s try to explain that using the numbers from 1 to 9 as our values. If we sort these values, than we get the root of numbers from 1 to 9.\n1 2 3 4 5 6 7 8 9\n____|___|___|____\n\nNo, we divide the values by their position. That value, that is placed in the centre, is of specific importance because it is the most centred value of this dataset. Here we draw a line. If we take half the number between the first and the centremost, we get the first quarter of our data. When we do the same towards the end, we get the last quarter of the data. Where the first quarter ends, and we are the last quarter starts, we also mark this dataset. The values at these positions are 3 for the start of the first quarter, 5 for the most central value, and 7 for the start of the second quarter.\nCompare this with the following distribution of data.\n1 1 2 7 7 8 20 26 100\n____|___|___|________\n\nHere, two marks the beginning of the second quarter, seven customers sent a value, and 20 marks the beginning of the last quarter of the data. Notice, that this does not depend on their actual values, but only on the position within the ordered dataset. If we now have a Y axis, on which we have continuous scale of the actual values, and we draw a box according to the parameters we just defined (position of the first the second and the third quarter of the data), than we get a feeling for how the values of the data within our dataset are distributed.\nIn the visualisation of the box plot, the box marks to inner half of the data so the second and the third quarter of the data. The border between the second of the fourth quarter, the most central value, is marked by a line (This value is also “median”, we will learn about it soon). Beside the box itself at the thick line dividing it, there are also thin lines sticking out from the box on top and bottom. This lines are called whiskers. The end line of a whisker is drawn at that value, that is less than 1.5 times the distance of the inner half of the data away. Every other point, that is more far away, is considered to be an outlier, and is visualised by a point.\nLet’s see the actual box plot of all numbers 1 to 9.\n\nboxplot(1:9)\n\n\n\n\n\n\n\n\nHere you can see all elements I’ve just described except for the outliers. To produce an outlier, we will add a very high value to our dataset.\n\nboxplot(c(1:9,15, 20))\n\n\n\n\n\n\n\n\nYou can see, did I edit the value of 15, which is now marked by the whisker. I also edit the value 20, which is now displayed as an outlier. If we applied to our actual data, the length of the fibula at Münsingen, you will see kind of the same.\n\nboxplot(muensingen$Length)\n\n\n\n\n\n\n\n\nThe interpretation would be like following, most of the data are evenly distributed between 30 and 80 mm. The thick line is a bit lower in the box, this means, that there are slightly more low values than high values. Besides the usual values, we have two outliers at approximately 120 mm plus or minus. By inspecting this plot, I already learnt a lot about distribution of the data in our dataset.\nBox plots are especially useful, if you want to compare the distribution of data between different categories of data. For example, you might like to compare the distribution of the length of the fibula in respect to their style. For this, I will introduce to you in new syntax in how are you can formulate circumstances in R. This notation is called formula notation. It is centred around the tilde ‘~’. This sign means “in relation to”. So, if I like to draw a box plot of the length of the fibula in respect to its style, I can express it like a below:\n\nboxplot(muensingen$Length ~\n          muensingen$fibula_scheme)\n\n\n\n\n\n\n\n\nWe will work with this kind of formula notation more in more advanced topics here. This notation becomes especially helpful when it comes to modelling. Because there, you model things in relation to other things.\nBack to our box plot. Of course, we can also use parameters here, to add elements to our plot. This elements might be a title, the colour, or the labels of the axis. You might like to play around a bit with the example below to get a feeling of the effects.\n\npar(las=1)\nboxplot(Length ~ fibula_scheme,\n        data = muensingen,\n        main = \"Length by type\",\n        col=\"green\",\n        xlab=\"fibulae scheme\",\n        ylab= \"length\"\n        )\n\nAll in all, a box plot is a very helpful tool when it comes to condensed have a look on the distribution of data. As I have explained, this is specifically helpful if you want to compare different distributions with each other. If you are more the girl that likes to watch the matrix uncoded, the scatterplot is probably more your type. We will learn about that below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#scatterplot",
    "href": "03-chapter.html#scatterplot",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.9 Scatterplot",
    "text": "3.9 Scatterplot\nJust get a proper is probably that kind of plot of people (beside a pie chart) imagine most of the time when they think about statistical visualisation. It is also one of the most basic plot types. That’s why it is also the standard configuration of the command plot(). Basically it is used to display one variable in relation to another one. This other variable can also be the order of the values, as we have seen in the example above. In general, scatterplot is suited for all data types and scales of variables, although most of the time for nominal and ordinal data other chart types might be preferable.\nSince we discussed basic elements of the scatterplot already above, without mentioning the name, I would like to use the space here to show you some alternative ways how are you can produce a scatterplot in R. For this, we start with the basic plot of the similar length against the foot length, with the Trent line edit in red, like we have done before.\n\n\n\n\n\n\n\n\n\nAlthough libraries offer a different ways of displaying scatterplots. One option here is the library car, that is specifically used for regression analyses (analyses of the relationship of two variables). To get access to the functionality of another library in R, at first we have to load this library. For this we used to come on library(). As the parameter you use the name of the library that need to be loaded. Pay attention: you don’t need to use quotation mark ” to load known and installed libraries.\nThe command from ‘car’ to produce a scatterplot is scatterplot(). In here you also use the formula notation that we have seen already with the box plot, but also with the linear model for the trendline in the example above. Here you specify the names of the columns in the dataset as variables, and as data you give the name of the variable which holds the whole dataset, the data frame muensingen.\n\nlibrary(car) # library for regression analysis\nscatterplot(FL ~ Length, data = muensingen)\n\n\n\n\n\n\n\n\nAt this resulting scatterplot, you can see different elements, that we have seen before: for example, the box plots. Also, we see a trend line. But also, we see more lines. If you want to know, what is lines mean, you should consult do you help for the function scatterplot(). Luckily, you know how this can be done.\nAnother suggestion is to use the library ggplot2. This library is very powerful and it is used a lot in professional visualisation. In publications and in conference presentations you will see ggplot style visualisations very often. The reason, why we are not using it here, is, that it comes with its own syntax philosophy. To learn this, would overwhelm potentially students that already have to cope with the basic understanding of R. But once you have mastered R in general, I strongly suggest that you have a closer look to this plot library.\n\nlibrary(ggplot2) # advanced plots library\nb&lt;- ggplot(muensingen,aes(x=Length,y=FL))\ngraph&lt;-b + geom_point()\nshow(graph)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#histogramm",
    "href": "03-chapter.html#histogramm",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.10 Histogramm",
    "text": "3.10 Histogramm\nThe next type of plants that we would like to have a look to is the histogram. Here, we take a different perspective compare to the scatterplot, and more similar to the box plot. Also here, we are looking at the distribution of the data. We visualise, in which part of the value range of our data most of our data are located. So what we will see, is if most of the data rather low or rather high values, for example.\nAt first I would like to give you an example of an histogram, so that we can understand its character and its use cases. Therefore let’s again use the length of the fibula in our dataset and plot the histogram accordingly.\n\nhist(muensingen$Length)\n\n\n\n\n\n\n\n\nYou can see that on the X axis of the plot we can see the actual values. The Y axis has to label frequency. The data are represented like with a bar plot in between values of length. In the first bar, between 20 and 40, we have to representation of all fibulae with a length between 20 and 40. If we look to the frequency, we can see that there are three. The next class of fibulae is between 40 and 60. Here, we can see that we have 10. This goes on.\nSo, in histogram, we don’t see any longer the individual values, but we see, how many items have values within a certain range. With this visualisation and the perspective on the distribution we are reducing the complexity of the data. You don’t any longer see the individual item, but we get a better understanding about the distribution of the values of the individual items within the whole dataset. Quite often in visualisation, but also statistics in general, we have to compromise between the consideration of the new visual case and the extraction of the general pattern.\nIn the standard display of histogram, we can only guess the individual values or the total numbers of cases per class. If we add labels, we can also see the actual numbers represented in the plot.\n\nhist(muensingen$Length, labels = T)\n\n\n\n\n\n\n\n\nOf course, we are not forced to only use the classes between 20 and 40, 40 and 60 and so on. We can also define our own classes. For example, if we want to have a finer resolution, we could decide to display classes off with of 10. We do that, using the parameter ‘breaks’.\n\nhist(muensingen$Length,\n     labels = T,\n     breaks = 10)\n\n\n\n\n\n\n\n\nPlease note the differences: before, we had some blocks, that now are divided into finer structures. The choice of the class with can be decisive for the interpretation. Although our dataset is rather small, if you look to the highest values, just from visual inspection in the first case it seems that we have a rather constant data distribution between 100 and 140. With the smaller class with, the holes in this distribution become obvious. So also here, we have to make a compromise between visualising the individual case and total pattern.\nAgain, of course, we can use the usual suspects to change the look of our histogram.\n\n\n\n\n\n\n\n\n\nThe disadvantages of the Instagram are, that makes the data reduction necessary and therefore we lose some information in the visualisation. Also, as we have seen, is the actual display show me dependent on the choice of the class with. There are different techniques to overcome especially this problem. The first, the stem and leaf chart, comes from an age, where computers we are not able to produce plots. For reasons of completeness, but also because it’s mentioned in the Stephen Shennan’s book, we included here. The other and currently much more popular version is the kernel density estimation or kernel smoothing. We will learn about it afterwards.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#stem-and-leaf-chart",
    "href": "03-chapter.html#stem-and-leaf-chart",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.11 stem-and-leaf chart",
    "text": "3.11 stem-and-leaf chart\nThe stem and leave chart is a clever idea to at the same time represent the general pattern, but also the individual cases. On the one hand, it is also a kind of histogram. But at the same time, it shows the values of the individual cases. Nevertheless, it has become a bit out of fashion lately.\nLet’s demonstrate it with our usual example. The command is stem.\n\nstem(muensingen$Length)\n\n\n  The decimal point is 2 digit(s) to the right of the |\n\n  0 | 34444\n  0 | 5555566677\n  1 | 13\n\n\nOur dataset consists of several similar length below 100, and two above 100. The latter you can see as the last line of the result of the stem and leave plot. The line above represents all the fibulae between 50 and 100. The top line represents all those between zero and 50. You can see, that the individual cases are represented by figures. This figure indicates the next value. So for example, in the top row the first number is zero. This means, that the first number is below 100. It starts with a 3, so it’s round about 30. After that, we have four 4s. This means, we have four more fibula with length of around 40. By now, you should’ve understand the pattern.\nSo as promised, the steam and leaf plot is as such a clever idea to represent data. It’s only drawback is, that it doesn’t look very visual, and reminds a lot on the age of computers without decent graphical displays. And has also announced, there is some more modern alternative to that that we will have a look to in the next part: the kernel smoothing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#kernel-smoothing-kernel-density-estimation",
    "href": "03-chapter.html#kernel-smoothing-kernel-density-estimation",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.12 kernel smoothing (kernel density estimation)",
    "text": "3.12 kernel smoothing (kernel density estimation)\nThe last plot version that I would like to introduce to you is the kernel density estimation, quite often abbreviated as KDE. Also this visualisation is very similar to the histogram. Let’s have a look at it and then discuss its features.\nThis time, we actually have to specify two commands. The first, density() is doing the actual calculation. The second is the usual plot() command. The density command is encapsuled into the brackets of the plot command. In this way, the output of the density serves as an input for the plot.\n\nplot(density(muensingen$Length))\n\n\n\n\n\n\n\n\nYou can see, that’s the essential elements are quite comparable to the histogram. We have an X axis visualising the values within the dataset. And we have a Y axis, this time not with frequency, but with density. This concept of density is the most difficult part to understand here. Therefore, let’s postpone it for a second.\nLet’s first concentrate on this moving part. For this, let’s assume that we are not looking at the actual value, But to a more blurred representation of the value. Like, if you are looking with half closed eyes or in not fitting glasses to a point. It will be blurred. The most intensity will still be in the centre, but there will be a halo of lesser and lesser intensity the more far you will get from the centre.\nOr probably a better way to understand this is to think about the actual values as hole in a vessel filled with sand. At the hole, which is at the position of the actual value, the sand will fall down and will form a heap. This heat will be the highest in the centre so at the actual value, but it will form a small hill, starting from left and right of the actual value. If two values are nearby, the hills will merge and join in a bigger hill. This is how you can interpret the picture above. Around 40 to 50, there are the most holes in our sandbox, so here most of the sand will fall down and for the big heap. On the other hand, at 110 and 130, there are only two holes each, so they’re only small hills will form.\nWith this kind of visualisation, we avoid a problem to artificially draw boundaries between classes. Still, we can see a representation of the total distribution of the values within our dataset. And this is more precise than the box plot, because we get more information about the internal structuring of the data.\nWe can also combine the KDE with histogram. For this, we have to bring both to the same scale. The scale of the KDE is such, that the total area under the curve of the will sum up to 1. We can also be scaled histogram to be in the same scale. For this, we used to parameter ‘prob=T’.\n\nhist(muensingen$Length, prob=T)\nlines(density(muensingen$Length))\n\n\n\n\n\n\n\n\nWe later will learn more about the concept of area under the curve, for no visionalisation is in the foreground, so we will stick with that.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "03-chapter.html#guidelines",
    "href": "03-chapter.html#guidelines",
    "title": "3  Explorative statistics & graphical display",
    "section": "3.13 Guidelines",
    "text": "3.13 Guidelines\n\n3.13.1 Stay honest!\nSome final suggestions for guidelines, that you might like to consider, when you’re plotting data. The first and probably most relevant is that you should stay honest in your data representation. It is easy to cheat with different techniques. Although for example everyone probably can understand the scales of your axis, nevertheless the presentation can produce very different perceptions. The choice of the way how are you display your data has a strong influence on the statement and how it will be received.\nChoice of display has a strong influence on the statement. Let’s use the example of the Swiss stock market index to see, how different scales can influence the visualisation.\n\n\n\n\n\n\n\n\n\nThe upper left panel shows the development of the Swiss stock market within the last year. Usually you can detect the crash that took place in the course of the Corona epidemy. This is the kind of display that is quite often shown in respect to such developments. The upper limit represents the upmost value, the lower limit the lower most value. Of course, this is visible at the Y axis. In this visualisation, the development looks very dramatic. If we only change the Y axis starting from zero, this crash looks less dramatic immediately.\nIf we additionally enlarge our investigation window (or at least the window of data that we are displaying), it becomes obvious that much stronger deteriorations took place in the past. The lower right panel shows Shows the development starting from 1990. This shows, how low this value was in the beginning.\nHowever you might like to interpret this developments in respect of today’s severity, it is clear, that the different scales put this development in very different frames of references.\nSo, you could sum up the suggestions for graphical representation like so:\n\nStay honest!\n\nChoice of display has a strong influence on the statement.\n\nClear layout!\n\nMinimise Ratio of ink per shown information!\n\nUse the suitable chart for the data!\n\nConsider nominal-ordinal-interval-ratio scale\n\n\nFor the last point, I have compiled a small table that can give you an advice which kind of visualisation you should choose in which kind of situation.\n\n\n\n\n\n\n\n\nWhat to display\nsuitable\nnot suitable\n\n\n\n\nParts of a whole: few\nPie chart, stacked bar plot\n\n\n\nParts of a whole: few\nStacked bar plot\n\n\n\nMultiple answers (ties)\nHorizontal bar plot\nPie chart, stacked bar plot\n\n\nComparison of different values of different variables\nGrouped bar plot\n\n\n\nComparison of parts of a whole\nStacked bar plot\n\n\n\nComparison of developments\nLine chart\n\n\n\nFrequency distribution\nHistogram, kernel density plot\n\n\n\nCorrelation of two variables\nscatterplot\n\n\n\n\nSometimes, it is illustrative to look at bad examples. Such bad representation of data is also called chartjunk. I will link here to the respective chapter of the book of Edward Tufte to you, so if you would like to go deeper into the subject, can I have a look here. But with his keywords, entered into a search engine of your choice, you will be delighted with a lot of examples of how not to do it. Enjoy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Explorative statistics & graphical display</span>"
    ]
  },
  {
    "objectID": "04-chapter.html",
    "href": "04-chapter.html",
    "title": "4  Descriptive Statistics",
    "section": "",
    "text": "4.1 Introduction\nA distribution is the collection of all the outcomes in the sample. More precisely, you can imagine it as a way to describe the frequency of certain outcomes. Let’s imagine the following example:\nAfter their final exam, the students of a mathematical class go to the football field, and line them self up according to their scores in the exam. Starting from the 0 yard line, they move so many yards as they have scored points in the test. Now, if we count the number of students in certain areas of the football field, we get their frequency distribution among the values. So, with a distribution, we are not talking about the individual value, but about their general distribution. From this it is clear, that it only makes sense to talk about distributions if we have more than one value (although technically also individual value is a distribution).\nBut not only students can form distributions, also other data sets can. You will see, that depending on the data type, different manifestations of distributions are possible. Nevertheless, all these versions are data distributions. The characteristics of the different distribution can be described using certain parameters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-chapter.html#introduction",
    "href": "04-chapter.html#introduction",
    "title": "4  Descriptive Statistics",
    "section": "",
    "text": "source: Phillips 1997\n\n\n\n\n4.1.1 Parameters of distributions\n\n4.1.1.1 Central tendency\nThe central tendency can be understand as a description of those values, that are central or “typical” for a certain distribution. This also can be called centre or location of that distribution. There are multiple specific values, that can be used, to describe this specific centre, depending on the scientific question and the data. For example, certain data quality can be described only with certain central tendency parameters. The most commonly used central tendency parameters are the mean, median, and the mode.\n\n\n4.1.1.2 Dispersion\nWhile the central tendency describes the most centres or most typical value, the dispersion describes how much the values in the whole sample vary around this central value. It is therefore also a measure of how much variety is in the sample. The most intuitive measure of dispersion is the range. We will see, that it’s not a perfect measure for the whole dataset. Instead, in statistics often the variance or the standard deviation is used. We will also learn about the coefficient of variation as a means to compare the dispersion of different distributions.\n\n\n4.1.1.3 Shape\nAlso not so often used explicitly in quantified descriptive statistics, of course the shape of the distribution also is very important for its interpretation. For example, the question if distribution is similar shaped on both sides of the central value, might give us quite an insight into this processes leading to this distribution. There are two other parameters that can be used to describe the shape of distribution in a quantitative way: skewness and curtosis. We will see, how we can calculate these values, and what their meaning is.\n\n\n\n4.1.2 Loading data for the following steps\nAlso in this example, I would like to use the Münsingen data to demonstrate some concepts. Please download the data and then load this data into your R environment:\n\n4.1.2.1 download data\n\nmuensingen_fib.csv\n\n\n\n4.1.2.2 Read the Data on Muensingen Fibulae\n\nmuensingen &lt;- read.csv2(\"muensingen_fib.csv\")\nhead(muensingen)\n\n   X Grave Mno FL BH BFA FA CD BRA ED FEL  C   BW  BT FEW Coils Length\n1  1   121 348 28 17   1 10 10   2  8   6 20  2.5 2.6 2.2     4     53\n2  2   130 545 29 15   3  8  6   3  6  10 17 11.7 3.9 6.4     6     47\n3  3   130 549 22 15   3  8  7   3 13   1 17  5.0 4.6 2.5    10     47\n4  8   157  85 23 13   3  8  6   2 10   7 15  5.2 2.7 5.4    12     41\n5 11   181 212 94 15   7 10 12   5 11  31 50  4.3 4.3  NA     6    128\n6 12   193 611 68 18   7  9  9   7  3  50 18  9.3 6.5  NA     4    110\n  fibula_scheme\n1             B\n2             B\n3             B\n4             B\n5             C\n6             C",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-chapter.html#central-tendency-1",
    "href": "04-chapter.html#central-tendency-1",
    "title": "4  Descriptive Statistics",
    "section": "4.2 Central tendency",
    "text": "4.2 Central tendency\nLet’s start with those parameters to describe the location of the distribution. This means, with these parameters we can describe where the whole distribution is centred in the range of possible values of that variable. Some of those parameters you will probably know already very well, some might be not so familiar to you.\n\n4.2.1 Mean\nThe arithmetic mean is one of the most widespread used parameter to describe the central tendency of a dataset, also in everyday life. But that doesn’t mean, but it’s the best value for describing this feature of distribution in every individual case. It is only suitable for metric data (interval or ratio). Why this is the case, becomes apparent if you look to the formula how to calculate it.\n\\[ \\bar{x} = \\frac {\\sum_{i=1}^{n} x_i} {n} \\] In this formula, we at first sum all the n values up, starting from the first (index 1) and ending with the last (index n). After this, we divide by the number of values. As we have seen already, in R we can recreate this formula, using some of all the values and divided by the length. But of course, since it’s such an important descriptive parameter, there is a specific function for calculating the mean():\n\nsum(muensingen$Length) / length(muensingen$Length)\n\n[1] 57.58824\n\nmean(muensingen$Length)\n\n[1] 57.58824\n\n\nDividing all the values by their number means, that we compare the differences of these values. This can only meaningful be done in situations, where the distances between values have a defined metric. For this reason, we cannot use the arithmetic mean to analyse the central tendency ordinal or nominal data. Also, since with the sum every individual value is included in the way in the calculation of the mean, it describes the whole distribution and not certain values within.\nAlthough the arithmetic mean is the most commonly used central tendency value, it has certain features that are not optimal for describing the whole dataset. Especially, it is very sensitive to outliers, as we will see below. Alternative is the median, which is not used so often, but whose philosophy is also quite easy to be understood.\n\n\n4.2.2 Median\nDifferent from the arithmetic mean, the median is a parameter that can be calculated for metric, but also for ordinal variables. And this calculation is rather trivial: if we have an uneven number of values, to get the median result all the values and then selecting the middle value. That means, literally the value that is in the middle of the sorted vector.\nIn our example, we have the values from 1 to 7. If we sort them and determine the middle value, it will be 4. So 4 is the median for this distribution.\n1 2 3 4 5 6 7\n      |\nIn R, the calculation of the median is not more complicated than the calculation of the mean. Also, we don’t have to sort the vector ourselves, R is doing that for us. The command is median().\n\nmedian(c(3,2,1,7,5,4,6))\n\n[1] 4\n\n\nA bit different is the situation, when we have an even number of values. Here, there is no middle value as such. In that case, the median is calculated by taking the mean between the two middlemost values. Assume, we have to numbers from 1 to 8. The median will be the mean of the values four and five.\n1 2 3 4 5 6 7 8\n       |\nAlso in this case, the calculation of the median is not more complicated an R.\n\nmedian(c(3,2,8,1,7,5,4,6))\n\n[1] 4.5\n\n\n\n\n4.2.3 Mode\nThe last of the parameters that I want to introduce here to you for central tendency is the mode. This is simply the most frequent value of a vector. So it fulfils our definition from above, that it is the most typical value. Also, since it relies only on counting, the mode can be determined for every data quality: nominal, ordinal and metric data.\nLet’s assume, that we have the animal bones from an excavation and we can determine the minimal number of individuals for goat, sheep, cattle and pig. In that case, goat will be the most frequent value, and by that the mode of that distribution.\ngoat sheep goat cattle cattle goat pig goat\nmode: goat\nIn R, since for nominal values the mode is trivial, and for other data types it can be rather complicated, there is no specific function to calculate the mode. Instead, in the example of nominal values, we can calculate the frequency using the function table(), and then determine which of the frequencies is the biggest, using the function which.max().\n\nwhich.max(\n  table(\n    c(\"goat\", \"sheep\", \"goat\", \"cattle\",\"cattle\", \"goat\", \"pig\", \"goat\")\n    )\n  )\n\ngoat \n   2 \n\n\n\n\n4.2.4 Comparing Central tendency parameters\nAs I’ve said already, for certain data types or levels of measurement only certain central tendency parameters are available. You can see a visualisation of this in the table below:\n\n\n\nnominal\nordinal\nintervall+\n\n\n\n\nmode\nmode\nmode\n\n\n-\nmedian\nmedian\n\n\n-\n-\nmean\n\n\n\nBut every ability for different data quality is not the only reason, why are you should consider having a look to other parameters for central tendency beside the mean. What other reason is, that the mean is strongly affected by outliers. This is not so much to keep the median, and the mood is hardly affected by any outliers at all. To demonstrate that in a practical matter, let’s look at the following distribution. Here we have most of the time the value is between one and nine but only one time the value 120. This value does not very well represent any typical value of this distribution. So far a good value for central tendency it should not have so much of an influence.\n\ntest&lt;-c(1,2,2,3,3,3,4,4,5,5,6,7,8,8,8,9,120)\n\nIf we calculate the mean of this dataset, it will be above any other value then the one outlier. So in that case it’s not really the central tendency of the whole dataset.\n\nmean(test)\n\n[1] 11.64706\n\n\nOn the other hand, the median is virtually unchanged by this extreme value. Because it’s only one very high value, it also counts in the calculation of the median only as 1.\n\nmedian(test)\n\n[1] 5\n\n\nAnd the same is true for the mode: because this outlier is only one time present, it will also not affect the mode.\n\nwhich.max(table(test))\n\n3 \n3 \n\n\nAlthough the mode is very insensitive to outliers, from a practical perspective in most of the cases it also represents very well essential value of the distribution, when it comes to metric or ordinal data. Only when a more or less symmetric distribution is present, the mode might be helpful, like in the example below.\n\nwhich.max(table(c(1,2,2,3,3,3,4,4,4,4,5,5,5,6,6,7)))\n\n4 \n4 \n\n\nWe haven’t talked about symmetry so much, but also when it comes to distributions, this concept is quite intuitive. When we have a small amount of items with small values, a major amount of items with intermediate values, and again a small amount of items with small values, then we have something that can be considered as a symmetrical distribution. When we have more small values then intermediate or higher values, or more high values then intermediate in small values, then we have a skewed distribution, which is not symmetrical. You can see examples of this in the image below. Here, you can also see, what happens to the different parameters of central tendency in respect to each other, when we have a positive or negative skewed distribution.\n\nIn case of a positive skew, the median will be smaller than the mean, since the meeting is strongly influenced by outliers. So a small number of high values will ‘drag’ mean towards the higher values. Conversely, if we have more high values than low values, then the main will be smaller than the median, for the same, but inverted reasons. This means, just by comparing these two parameters of central tendency, we can already make a statement about the shape of the distribution, without even having to plot it.\n\nAnalyse the measurements of the width of cups (in cm) from the burial ground Walternienburg (Müller 2001, 534; selection):\n\ntassen.csv\n\n\ntassen&lt;-read.csv2(\"tassen.csv\",row.names=1)\ntassen$x\n\n [1] 12.0 19.5 18.6 12.9 13.2  9.9 19.5  8.4 21.0 18.9  7.5 18.9  8.1  9.0  7.8\n[16]  9.9 10.2  8.1 12.0  9.0 26.1 20.4\n\n\nIdentify the mode, median and mean and determine if the distribution is symmetric, positive or negative skewed.\n\n\n\nSolution\n\n\nmean(tassen$x)\n\n[1] 13.67727\n\nmedian(tassen$x)\n\n[1] 12\n\nwhich.max(table(tassen$x))\n\n8.1 \n  3 \n\n# The median is smaller than the mean. The distribution has a positive skew with more smaller than larger values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-chapter.html#dispersion-1",
    "href": "04-chapter.html#dispersion-1",
    "title": "4  Descriptive Statistics",
    "section": "4.3 Dispersion",
    "text": "4.3 Dispersion\nIn the previous chapter, we have learnt different ways how to express to central tendency of the dataset. But with specifying the location, we have not sufficiently described our dataset yet. One of the more relevant characteristics of the dataset is, if the data are rather close together, or if they are distributed over a larger range of the values. Consider for example the distributions below:\n\n\n\nsource: Phillips 1997\n\n\nThe upper image shows two distributions with the same centre, but with different spreads. The figures below show the same situation, but this time with actual values. You can see, that both distributions are centred around the value zero, but the upper distribution ranges from -45 to 45, while the distribution below is much more centred.\nTo describe the differences, we have parameters of dispersion, that quantify, how wide the data are distributed over the range of values.\n\n4.3.1 Range\nThe simplest of this parameters is the range. The range gives the range from minimum to the maximum value of the dataset. Practically, usually the range is constituted of two values, the minimum and maximum. In R, the function to get the vector of the range of the dataset is range().\n\nrange(muensingen$Length)\n\n[1]  26 128\n\nrange(tassen$x)\n\n[1]  7.5 26.1\n\n\nWith the range, by definition we get the highest and the lowest value. This means on the one hand, that by constructing the range only two values are considered, just these two. All the other values in the dataset do not play any role in the calculation of the range. Therefore, the range does not describe the whole dataset very well. Also, by definition we consider the most extreme values of the dataset. This means, that the range is very sensitive for outliers.\nA better measurement for the dispersion of the data should have the following characteristics:\n\nLess sensitive to outliers\nBroader representation of all the data in the dataset\n\n\n\n4.3.2 Towards a better parameter for dispersion\nWe have already seen, that the mean, although also sensitive to outliers, at least consider all the values in the dataset as constitutional elements for the parameter. A good description for the dispersion might be the mean distance of all values from the mean. The following plot shows us the values of the Münsingen dataset from a different perspective. Every line represents one fibula. We have substracted the mean of the length of the different fibulae from the individual values. Again, we can see our two unusual long fibulae as high values. The bars each represents how much smaller or bigger the fibulae are in respect to the mean.\n\n\n\n\n\n\n\n\n\nNow, to get a value for the dispersion of the dataset to consider all individual items, we have to calculate the mean distance from the main. But we cannot simply substract the mean from each value and calculate the mean from it because this would result in zero. The reason for this is the very definition of the mean.\n\\[\n\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0\n\\]\nIn our calculation, we must ignore if the difference is negative or positive. We have to consider the absolute difference, not the relative one. To get rid of the sign, easiest way is to square the result.\n\\[\n\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\] Now we get to some of the squared differences of each value to the mean. To get to mean after differences, we have to divided by the number of cases.\n\\[\n\\sigma^2 = \\frac {\\sum_{i=1}^{n} (x_i - \\bar{x})^2} {n}\n\\] With this, we have tools to calculate the mean differences from the mean. The formula above shows the calculation of the so-called variance for the population. It’s notation is using the Greek letter sigma to the square. But because we are dealing here with a sample, and not the full population, the resulting estimation of this mean difference might be biased. For that reason, this bias must be corrected. For reasons, we cannot explain fully here, the easiest way for correcting against a bias that case is to substract one from the number of cases. This gives us our first parameter for dispersion: the empirical variance.\n\n\n4.3.3 (empirical) variance\nAs explained and introduced above, the empirical variance is a measure for the variability in the data that is more insensitive against outlier then the range. It is equal to the sum of the squares distances from the main divided by the number of observations -1. Formula notation is \\(s^2\\).\n\\[\ns^2 = \\frac {\\sum_{i=1}^{n} (x_i - \\bar{x})^2} {n-1}\n\\]\nLet’s use this formula above, transform it into R code, and compare it to the results of the actual function var() from R.\n\nsum((tassen$x-mean(tassen$x))^2)/(length(tassen$x)-1)\n\n[1] 31.11136\n\nvar(tassen$x)\n\n[1] 31.11136\n\n\nIn this case, we have compared the diameter of the cups. The diameter themselves are measured in centimetres. Since in the formula above we square the measurement, the unit is square centimetre. Also the resulting unit of the variance is square centimetre. The values themselves are squared centimetres. Of course, it is much more convenient, to have a measurement of the mean distance from the mean that is in the units of the actual measurement. To get rid of this square in the units as well as in the values, of course we can just take the square root. If we are doing that, we end up with what is called standard deviation.\n\n\n4.3.4 (empirical) standard deviation\nThe formula for the standard deviation is essentially the same as for the variance. The only differences, like we have explained above, is that we take the square root.\n\\[\ns = \\sqrt{\\frac {\\sum_{i=1}^{n} (x_i - \\bar{x})} {n-1}}\n\\]\nAgain, if we compare the results from the recorded formula to the actual functioning are, the function sd(), the results should be the same. Keep in mind, that we are talking about the empirical version of these calculations, suited for samples.\n\nsqrt(sum((tassen$x-mean(tassen$x))^2)/(length(tassen$x)-1))\n\n[1] 5.577756\n\nsd(tassen$x)\n\n[1] 5.577756\n\n\nWith this, finally we have the mean differences from the mean for our dataset. This parameter of dispersion is one of the most widespread used for describing distributions. It has some specific features that make it very useful. Some of them we will learn about later in the course.\nWith what we just achieved, the measurement of the standard deviation is in the exact same units as the measurements themselves. Most of the time, this is desirable. Especially, if we would like to understand better what this mean distance mean fall our individual values. But sometimes, you might like to compare spreads of distributions, that have different central tendencies, or to put it in other words, different locations. For this case, it is more helpful, to have a measurement of dispersion that is unitless. This is the last parameter of the dispersion from this branch that we will learn about.\n\n\n4.3.5 Coefficient of variation\nLet’s assume, we wonder whether the length of the fibulae in our Münsingen case it’s more variable then the foot length or the other way round. We cannot simply compare the standard deviations, because the feet of the fibulae are shorter than the whole fibula by definition. Consequently, also the variability will have smaller values for the foot than for the total length.\nIn such situations, we need a unitless measurements. We would like to make the value independent from the location of the distribution. To make something independent, most of the time you have to divide by it. So if we divide the standard deviation (measured in the unit of the measurement) by the mean (measured in the unit of the measurement), we get a parameter that is independent from the location (and also, since the units negate each other, has no unit). This coefficient of variation can be used to compare distributions that do not share the same location in the value range. We can compare apples with oranges.\nIn R, there is no specific come out for that. But it’s easy to calculate it anyway. All we have to do, is to divide the standard deviation sd() by the mean mean().\n\nsd(muensingen$Length)/mean(muensingen$Length)\n\n[1] 0.4508988\n\nsd(muensingen$FL)/mean(muensingen$FL)\n\n[1] 0.7732486\n\n\nThe result is, that the variation coefficient for the length is smaller than the valuation coefficient from the foot length. It seems, that the feet of the fibulae differ more relatively then the total length.\n\n\n4.3.6 Quantile\nThe concept of the standard deviation and related measurements is very similar to the concept of the mean, that we have learnt as one of the parametres for location of the dataset. Another way to describe the dispersion of the dataset is the concept of quantiles, it is more related to the concept of the median that we also learnt about above. We already have introduce this concept, when we have talked about the box plot.\nQuantile in general are arbitrary points in the range of a dataset where this can be divided into parts. Specific quantiles are the quartiles. Quartiles divide the data into four equal parts, that means, parts of equal number of items. So let’s assume, we have 16 items. The first quartile consists of the first four of this sorted items and so on.\nLet’s assume, we have 13 values from 1 to 13. If we sort them and marked the beginning of each quarter of the items, We would end up dividing our dataset by one (the beginning of our dataset), 4 (beginning of the second quarter of a dataset), 7 (the median of our dataset), 10 (the beginning of the fourth quarter of our dataset), and finally 13 (the highest value at the end of our sorted the dataset). When we have an even number of items, the same rule applies like in the case of the median: we take to mean of the two centremost values.\n1 2 3 4 5 6 7 8 9 10 11 12 13\n|_____|_____|_____| _______|\n\nIf we now plot our data on an X axis with the actual values, we can mark the positions of the quantile stare. Such a plot could look like this:\n\n\n\nPhillips 1997\n\n\nIn R, the command to calculate the quantiles is quantile. To make the confusion total, the default setting of the quantiles is to calculate the quartiles.\n\nquantile(tassen$x)\n\n  0%  25%  50%  75% 100% \n 7.5  9.0 12.0 18.9 26.1 \n\n\nBut we also can parameterised it differently. For example, we can calculate the so-called percentiles. And example below, we calculated the position of the cut points in that way, that we get the 10% intervals of the data.\n\nquantile(tassen$x, probs=seq(0,1,0.1))\n\n   0%   10%   20%   30%   40%   50%   60%   70%   80%   90%  100% \n 7.50  8.10  8.52  9.27 10.02 12.00 13.08 18.81 19.38 20.31 26.10 \n\n\nThe position of the quantiles (or the quartiles) depends on the spread of the data. In case of the quintiles, the first and the last value that is calculated here represents the minimum of the maximum value. So it is clear, that their position is strongly related to the spread of the data. But also the position of the beginning of the second and the third quartile is dependent on how strong the dispersion is. If it’s higher, it’s distance will be bigger. That is the reason why the distance between the beginning of the second and beginning of the fourth quartile is a measurement of the spread. Its relationship to the standard deviation is the same like the relationship between the mean and median in respect to the location of the dataset. This measurement is also called the Interquartile Range or IQR().\n\nIQR(tassen$x)\n\n[1] 9.9\n\n\nLike the median is more insensitive to outlast in the main, the Interquartile Range is less sensitive to all clients that the standard deviation. But since we only consider the centre at half of the data for the calculation of this measure, we lose some information about the so-called tails of the distribution, that is (in this case) the lowest and highest quarter.\n\nAnalyse the sizes of areas visible from different megalithic graves of the Altmark (Demnick 2009):\n\naltmark_denis2.csv\n\n\naltmark&lt;-read.csv2(\"altmark_denis2.csv\",row.names=1)\nhead(altmark)\n\n        sichtflaeche region\nLa01            2.72  Mitte\nLg1            26.78  Mitte\nLi02           26.96  Mitte\nSa01           27.05  Mitte\nLi06           32.93  Mitte\nK\\xf601        34.76  Mitte\n\n\nEvaluate in which region the visible area is more equal (less disperse).\n\n\n\nSolution\n\n\n# There are 3 regions in the dataset:\ntable(altmark$region)\n\n\nMitte   Ost  West \n   14    23    93 \n\n# lets use the region as separator and\n# calculate the coefficient of variation\n# (because each region might have different terrain):\n\nva_east &lt;- altmark$sichtflaeche[altmark$region == \"Ost\"]\nva_west &lt;- altmark$sichtflaeche[altmark$region == \"West\"]\nva_mid &lt;- altmark$sichtflaeche[altmark$region == \"Mitte\"]\n\nsd(va_east)/mean(va_east)\n\n[1] 0.9118925\n\nsd(va_west)/mean(va_west)\n\n[1] 0.9447072\n\nsd(va_mid)/mean(va_mid)\n\n[1] 1.005665\n\n# it seems, that the visible areas differ\n# the most in the center of the working area",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-chapter.html#shape-of-the-distribution",
    "href": "04-chapter.html#shape-of-the-distribution",
    "title": "4  Descriptive Statistics",
    "section": "4.4 Shape of the distribution",
    "text": "4.4 Shape of the distribution\nAs interesting as it might be, as difficult it is to describe the shape of a distribution in a single quantitative way. There are multiple ways how distributions can differ from each other: might it be related to the skewness, to dispersion of the data, to the symmetry or to the number of peaks.\nBelow, there is an image from Bortz 2006 dead visualise some of these possible parameters.\n\n\n\nShape of distributions (after Bortz 2006)\n\n\nAs already announced, we will discuss here to shape parameters that can be calculated: First, the skewness, of which we have already seen that it can be positive or negative. But there is also a way to calculate the value of the positivity or negativity. The second parameter is called Curtosis (curvature). This parameter indicates if the distribution is a letter or a steeper, compared to the so-called standard normal distribution.\nIn both cases, it might make sense to plot data and then calculate the values to get a feeling what they do mean. On the other hand, these values are rarely used in practise, so we will use this opportunity to introduce a new concept in our: how to make your own functions.\n\n4.4.1 Skewness\n\nIt’s already introduced, skewness is a measure of how skewed your dataset is. On the one hand, it can be distinguished between positive and negative skew. This can be estimated by comparing the mean and median of the dataset. Also, you can try to describe how intense the skewing of the dataset is. For this, probably the easiest option is to look at the plot and describe it. But there is also a way to calculate that, given by the formula below:\n\\[\n\\hat{S} = \\frac {\\sum_{i=1}^n (x_i - \\bar{x})^3} {n * s^3}\n\\]\nHere we will not take a part of this formula, but use it as a cooking recipe to produce our own function. For the interpretation, it should be said, that positive values of skewness indicates a positive skew, while negative values indicate a negative skew. The reason, why it’s reasonable to create your own function here, is the fact, that there do not exist a ready-made function for this purpose in the base package of R. So lets build our own:\nAs I have already indicated, everything in R is a variable. This is also true for functions. There are specific function to feel a variable with a function. You will not be surprised that the name of the function is function(). Whatever is inside of the (this time currently) brackets off the function call will be evaluated every time the function is called. In the round bracket of the function function() there are the variables that might be evaluated inside of the function.\nSo in our case, the skewness is calculated by dividing the sum of the differences of the individual values to the power of three by the number of the values times the standard deviation, also to the power of three. This is what takes place inside of the function definition. Let’s dissect that from bottom to top. The last line skew is the value that is returned after the function has been called. This variable skew has to be filled. This takes place in the line before that. The value m3 is divided by the denominator of the formula. Of course, this value M3 is filled with the numerator of the same formula. Also, you can see inside of this function body the variable x. This variable comes from the outside of the function and represents the actual dataset. That this function expects this variable x is indicated by the fact, that in the rounds brackets of the function call function() this x is mentioned.\n\nskewness &lt;- function(x) {\n  m3 &lt;- sum((x-mean(x))^3) #numerator\n  skew &lt;- m3 / ((sd(x)^3)*length(x)) #denominator\n  skew\n  }\n\nIf now we have to find our function skewness(), we can use it as any other regular function in R.\n\ntest&lt;-c(1,1,1,1,1,1,1,1,1,1,2,3,4,5)\nskewness(test)\n\n[1] 1.406826\n\ntest&lt;-c(3,3,3,3,3,3,3,3,3,3,3,3,2,1)\nskewness(test)\n\n[1] -2.231232\n\n\nWith this basic concept, you can structure and reuse parts of your code all over in your analyses. Also, the actual result of our function seems to be fitting. What we expect to be a positive skewed distribution results in a positive number and vice versa.\n\n\n4.4.2 Kurtosis\nThe second parameter that we may talk about in this context is to kurtosis. It describes the curvature of the distribution in relationship to the standard normal distribution.\n\nPositive values of kurtosis means, that the distribution is steeper than a normal distribution, while negative values mean, that it is flatter than a standard normal distribution. Of course, you can read the curvature from a plot, but also there is a way to calculate this. If you look to the formula, it will be quite familiar to you, because it resembles very much the formula for skewness. The only difference is, that instead of to the power of three no we calculate to the power of four.\n\\[\nK = \\frac {\\sum_{i=1}^n (x_i - \\bar{x})^4} {n*s^4}\n\\]\nSo, of course, we also can write a function for this. It will be very similar to the function as we’ve just programmed: You only have to replace two numbers. Oh, and by the way probably also the name of one of the variables.\nWe write a function for that, too:\n\nkurtosis &lt;- function (x) {\n  m3 &lt;- sum((x-mean(x))^4)\n  k &lt;- m3 / ((sd(x)^4)*length(x))-3\n  k\n  }\n\nLet’s test this out with one very steep and one very flat distribution.\n\ntest&lt;-c(1,2,3,4,4,5,6,7)\nkurtosis(test)\n\n[1] -1.46875\n\ntest&lt;-c(1,2,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,6,7)\nkurtosis(test)\n\n[1] 2.011364",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-chapter.html#take-home",
    "href": "04-chapter.html#take-home",
    "title": "4  Descriptive Statistics",
    "section": "4.5 Take Home",
    "text": "4.5 Take Home\nThere are many other possibilities to describe data, values, and distribution. The most relevant and most widely applicable concepts are those of central tendency and here especially median and mean, and those of dispersion, here especially variance and standard deviation. If you have understood this computations and they are meeting, you are very well prepared for the weather, more elaborated analyses.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "05-chapter.html",
    "href": "05-chapter.html",
    "title": "5  Nonparametric Tests",
    "section": "",
    "text": "5.1 Inductive statistics or statistical inference\nUntil now, we have primarily described and presented the data of our sample using descriptive statistics and graphical visualisation. We also used graphical visualisations to explore our dataset in the sense of explorative statistics. In all of these techniques, we stayed on the page of the sample. We did not make any conclusions about the underlying population.\nThis is no different, when we come to the field of inductive statistics, or statistical inference. Here, we explicitly try to estimate characteristics of our population using our sample. Since in nearly all cases, our sample will be smaller (in archaeology actually very much smaller) then our population, we have only access to a small part of the information describing the full population. So necessarily, when we base our estimation only on a fraction of the population, our estimation will be wrong. But it doesn’t need to be true, it just needs to be sufficiently correct. Nevertheless, it is essential to keep that in mind: whatever comes out of the statistical analyses, only is to with a certain probability. The knowledge gained is always statistical meaning, there is a certain chance that the world is like we assume it. We have to have a link from the sample that we have at hand, to the population. This link is provided to us by probability theory. We will explain that a bit more detailed, when it comes to parametric tests.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#population-and-sample",
    "href": "05-chapter.html#population-and-sample",
    "title": "5  Nonparametric Tests",
    "section": "5.2 Population and sample",
    "text": "5.2 Population and sample\n\n5.2.1 Repetition:\nA small repetition of what we have learnt in the beginning: We call population the collection of all items that are relevant for our investigation. But in most cases, we cannot investigate the full population. Therefore we investigate only a fraction of it. This fraction, that we ideally select on certain criteria (represent activity), is called our sample. We always should keep in mind, that the dataset that we are analysing is only a random selection of the whole collection of items that we are really interested in. In so far, it is like our probe that we send out as spaceship Enterprise into the nebula of data that lie in front of us. This is especially true in archaeology, where neither the population nor the conditions under which our sample is created is under our control. We never will be able to access the population, and we never will be able to verify our interpretation with it.\n\n\n5.2.2 Parameter\nEven if we have no way of precisely knowing them, our population always has certain values or distributions of values, there are fixed. For example, there is a specific value for the mean foot length of all the La Tène B fibulae ever produced (given, that we can agree on what a La Tène B fibula is). This value is there. It is an actual number. But we never will know it, and it’s lost in the depth of time.\nSuch variables, that we can’t be sure exists and have a certain value we call parametres. This parameters can only be estimated by us using the sample. So, they are not accessible for us, nevertheless that they really exist and really have a specific value.\nAlso, samples have parammeters. But these parameters are accessible: we can measure them. But we have to distinguish these parameters from the actual parameters of the population. Quite often, this is already visible in the notation that is used in the formula, which differentiate between sample and population. For example:\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\n\\(\\mu:\\) mean of the population\n\\(\\bar{x}\\): mean of the sample\n\n\n\\(\\sigma\\): standard deviation of the population\n\\(s\\): standard deviation of the sample\n\n\n\nSince we can only measure the parameters of the sample, we use them, and some knowledge about the general distribution of values in random processes, to estimate those values of the population. This is done in statistical tests, where it is estimated, if and how likely certain values of a sample, under certain conditions of a random process, result from the population with a certain parameter. The quality of the statement of a test therefore depends on the choice of the sample (representativity)! For this, we make a hypothesis about the value of the population, and then we test this hypothesis using statistical hypothesis testing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#statistical-hypothesis-testing",
    "href": "05-chapter.html#statistical-hypothesis-testing",
    "title": "5  Nonparametric Tests",
    "section": "5.3 Statistical Hypothesis testing",
    "text": "5.3 Statistical Hypothesis testing\n\n5.3.1 Validation of an assumption about the population\nAs I have just explained, does statistical hypothesis testing takes place before the background of the sample. We make a hypothesis about the value of the parameter of the population, and then we estimate, how likely the sample that we have at hand could have resulted from such a population. If it is very likely, then it is an indication that our hypothesis is not too bad. If it is very unlikely, that might be indication that there is something wrong with our hypothesis.\nBefore we start to talk about actual and individual values, most of the time hypothesis actually unfold them self more as general questions like the one below:\nHow probable is it that two or more samples descend from the different/the same population?\n(eg. Is the custom of grave goods for man and women so different that two different social groups are visible?)\nHow probable is it that a given sample descend from a population with certain parameters?\n(Is the amount of grave goods random or is a pattern visible?)\nIn the first place, when we have two samples and try to estimate if they originate from the same or from different populations, we have a test for independence. That means, we test, based on the samples, if the population of both values are independent from each other or not. In the second case, which is also called goodness of fit test, we test how good our data fits to a specific assumption, like I have explained above. But it is very difficult to prove something about the population, given that we have a sample. Maybe, we just have to wrong sample? Maybe, while our sample just doesn’t fit to our assumption, the sample next to it would probably do. It is much easier to falsifier statement using a sample: only one black swan falsifies do you hypothesis, that all swans are white. Therefore, in statistical hypothesis testing, we use a detour over falsification to make our hypothesis more plausible. This detour is called null-hypothesis.\n\n\n5.3.2 Null hypothesis\nIn statistical tests most of the times not the statement is tested which one expects to be true, but one tries to disprove the statement which one expects to be wrong: the null hypothesis. This hypothesis states mostly, that a association do not exists or that there is no differences between the samples and the distribution of the observations is by chance. So, most of the time, it is the rather boring standard situation, that exciting scientific investigations try to disprove.\nAn example: Is the composition of grave goods different between male and female deceased?\nThis is the scientific question. Now, there are two possible answers to that, resulting into possible hypothesis:\n\\(H_0\\): The composition is the same\n\\(H_1\\): The composition is different\nSo instead of proving our hypothesis one, we try to disprove or falsify its opposite. When we have shown, that the assumption, that the composition is the same (or only different due to random chance), then we have shown at the same time, that the composition is different. We did not say anything about how it is different, or why it is different, we only state, that is different. As we have shown above, this has two reasons:\n\nIt is (logical) easier to prove, that a statement is wrong (falsify) then to prove that a statement is true (verify).\nMost of the times it is easier to formulate a null hypothesis (How exactly is the composition different?). It doesn’t make a assumption about how the character of a association/difference exactly is.\n\nSo the “workflow” of the statistical test is the following: At first, usually from our scientific question, we have our alternative hypothesis. At the moment, it is probably not with an alternative, because it is the only hypothesis that we have. Usually, we hope or expect to find something interesting in the data. For example in relation to gender differences in burial items:\nConstruction of a alternative hypothesis:\nThe composition of the grave goods is different between male and female deceased.\nFrom this assumption, we build our no hypothesis in such a way that we are stating the opposite of what we might expect:\nConstruction of the null hypothesis:\nThe composition of the grave goods is the same in male and female burials.\n[To be honest, this is a bit of cheating. If you get further into the discussion about the validity of statistical analyses, you will learn decides to discussion on the P value as such, that one should actually only compare real scientific hypothesis with each other, and not such a strawman like another hypothesis. Nevertheless, this is (quite successful) practice in frequentist statistical analyses since decades, and in most of the time it actually also works quite well. Only to let you know, that this is not the only way of how you can interpret this workflow.]\nNow, that we have our normal hypothesis, we can test with our data from our sample how likely it is. How do you specifically is done, depends very much on the statistical test that we are using, and the data quality, that we are investigating. But the general logic is always the same: the next step is the\nTest of the null hypothesis\nFrom this, we have two possible results. Either:\nIf the result of the test is significant:\nThen we have evidence, that our Null Hypothesis might not be true. And, depending on the level of security that we would like to have, this might be rather overwhelming evidence. In that case, we reject the null hypothesis. This also means, that we can choose the alternative hypothesis. In our example:\nThe composition of the grave goods is different between male and female deceased.\nIf the result of the test is not significant, This means, that we do not have enough evidence to securely falsify the null hypothesis. Or in other words:\nThe null hypothesis could not be rejected.\nThis now does not mean, get the null hypothesis is actually true. You see that quite often, that When statistical test is not significant, that interpretation is that there is no difference for example between two populations. But this is not correct because the null hypothesis can be kept Ivor, because it’s true (or better not wrong), or, because there are not enough data. Statistical significance doesn’t and cannot differentiate between these two possibilities.\n\n\n5.3.3 One-tailed/Two-tailed hypothesis\nSometimes, you see one-tailed or two-tailed as qualifier resulting from statistical tests. This comes from the fact that technically in most of the hypothesis testing we compare our value to a range of theoretical values. In such situations, our value can be bigger or smaller than this range. Now, it depends on the question asked and therefore on the hypothesis, in which case we can reject our null hypothesis. Probably can be best explained using example:\nIs the number of grave goods in female burials higher than in male?\nWith this type of question, there is only one way in which our hypothesis (or it’s null hypothesis) can be falsified. Only in the case, when the number of greatcoats in May burials are higher, then we have a falsification of our hypothesis. There is only one way, one tail of the values, on which it can be falsified.\nIs the number of grave goods in female burials different from male?\nHere, the number of grave goods in female burials can differ into directions: There can be more or less burial items. So on both tails of our value range (smaller, bigger) there is falsification of the null hypothesis possible. Therefore, this is a two-tailed hypothesis.\nHere, we have more chance for a random process to result in the pattern that we are observing. That’s why in statistical tests the result is often two significances (onetailed, two-tailed). .center[  ]\n\n\n5.3.4 Stat. Significance\n\n\n5.3.5 How true is true?\nNow, we quite often have used the term significant. Of course, in common language this means important. In the realm of statistical analyses, significance means a measure of how certain decision for one or the other hypothesis is. Or, to put it the other way round, it is a measure of uncertainty.\nWe have already introduced the null hypothesis, and there we have explained that statistical significance decides whether or not to accept the null hypothesis. More specifically, significance is exactly a measure of how probable an error is when we reject the null hypothesis. That is not directly the same as how likely the null hypothesis itself is, although quite often it will be mixed up with this meaning. One important factor in this difference is, that also sample size and with that certainty of evidence against the null hypothesis is factored in here. But from this perspective, it also makes quiet sense that based on this criterion the null hypothesis is rejected or not. This measure of course is a continuous variable, so there are no specific values per se that would define a threshold. Nevertheless, there are some arbitrary classical thresholds:\n\n0.05: significant, with 95% probability the decision is right.\n0.01: very significant, with 99% probability the decision is right.\n0.001: highly significant, with 99,9% probability the decision is right.\n\nIn archaeology, where no life depends on our decision, usually the 0.05 level is perfectly fine for decision-making. In medical trial or situations, where you have a lot of experimental data, you probably would like to have stricter significance levels than that. But technically, no one can stop you to introduce your own significance level if you like.\nThis measure of significance is also often called p-value in the literature in the English literature, or alpha (\\(\\alpha\\)) in other languages. If you look up these names, you probably will find that there is quite a discussion going on about the value of the p-value in scientific analyses. Since this year it’s a beginners course, we will not go into this debate. For us the p-value is significant enough, and we will stick to this arbitrary 0.05 measure. This measure also means, that in 1 out of 20 cases the null hypothesis will be rejected even though it should not be rejected. When interpreting statistical results, one should always keep this in mind: even with seemingly objective statistical methods, there is quite a big margin of error possible. Actually, there are two possible arrows here: we can except the null hypothesis, even if it is not true. Or, we can reject the null hypothesis even if it is true. We will learn about these two versions in the next section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#α--und-β-error",
    "href": "05-chapter.html#α--und-β-error",
    "title": "5  Nonparametric Tests",
    "section": "5.4 α- und β-error",
    "text": "5.4 α- und β-error\nAs I have described above, we can go wrong in two ways with the rejection of a null hypothesis in statistical investigations:\nThe null hypothesis is rejected although it is true -&gt; Type I error, false positive, \\(\\alpha\\)-error\nSuch an error would be, for example, when the result of a pregnancy test shows a pregnancy although there is none.\nThe null hypothesis is not rejected although it is wrong -&gt; Type II error, false negative, \\(\\beta\\)-error\nIn this situation, the example would be the result of a pregnancy test if it shows no pregnancy although there is one.\nSo in general, there are two situations where we can decide correct: If we accept the null hypothesis, and it is the correct choice, or we reject the null hypothesis, and that is the correct choice. If we reject the null hypothesis, but there is in reality no difference between our samples, then we make this type one error. In that case, we would think that we gained some new knowledge, so we probably will build further interpretations on top of that. That makes type one errors more severe. Because, with a Type II error, when we keep the null hypothesis although there is a difference in our data justifying the alternative hypothesis, if we correctly interpret the statistical results we just miss an opportunity to find an interesting pattern. When another hypothesis cannot be rejected, we should not learn anything about the population.So, not so much damage should result from that.\n\n\n\n\n\n\n\n\n\nTrue condition: H0 (There is no difference)\nTrue condition: H1 (There is a difference)\n\n\n\n\nBy the use of a statistical test the decision was made for: H0\nCorrect decision\nType II error\n\n\nBy the use of a statistical test the decision was made for: H1\nType I error\nCorrect decision\n\n\n\nNevertheless, both situations are annoying, so that is why statistical tests usually try to avoid both types of errors. But they can do so only in so far as they adapt their strictness. If they are too strict, they will not reject the null hypothesis to easily, leading to type 2 errors. If they are not strict enough, do you wanna risk type 1 errors. So it is always a balancing act between both error possibilities. The power of the statistical test is its capability to avoid type two errors without risking type one error. This capability depends on the amount of information that can be put into the test itself. That means, if we have not very much information not very much evidence, then type two errors are more likely. In general, this also not very desirable, because more powerful tests enable us to differentiate more clearly between random effects and actual patterns in the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#parametric-vs.-nonparametric",
    "href": "05-chapter.html#parametric-vs.-nonparametric",
    "title": "5  Nonparametric Tests",
    "section": "5.5 Parametric vs. Nonparametric",
    "text": "5.5 Parametric vs. Nonparametric\nThe question, what nonparametric tests mean, and what distinguish them from parametric tests, is strongly connected to what we have described just above. If we have very little information, because from the data themselves or from general knowledge they can’t be put so much into the equation, then we probably will have tested and not very powerful. On the other hand, this tests might be applicable in situations, we are more powerful tests are not valid, and cannot be used. On the other hand, if we can introduce more information into our test, we have more grounds to differentiate between random effects and actual patterns. This precisely is the difference between parametric and nonparametric tests.\nParametric tests, we make certain assumptions about the distributions of the values in the population. Quite often, we assume that the values follow the so-called normal distribution. But there are also other possibilities: for example we could have a independence test that require the variance of the data to be equal. We will learn about one of these tests later, and but alternatives we do have in this situation. Quite often, parametric tests also, resulting from their prerequisites, can applied only to metric variables.\nNonparametric tests on the other hand do not make any of these assumptions. They do not care, what distribution the data might have in the population. Also, most tests for nominal or even ordinal variables are nonparametric. Another benefit is that quite often they are applicable for rather a small sample sizes. But for this flexibility, of course, your pay a price: this tests are not as powerful as their parametric counterparts. Nevertheless, for the dirty and sparse data that archaeology usually has to deal with, we can declared that this kind of tests exist.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#chi2-test",
    "href": "05-chapter.html#chi2-test",
    "title": "5  Nonparametric Tests",
    "section": "5.6 \\(\\chi^2\\) test",
    "text": "5.6 \\(\\chi^2\\) test\nThe first statistical test, that we will cover here, is probably one of the most nonparametric tests there is. The Chi-Square test requires only two or more sets of nominal variables, and will try to test whether or not the court occurrence of values is related to each other or not.\nThis probably doesn’t sound so universal, but in archaeological reality you can cover already quite wide range of possible questions with this. And also, you can base your interpretations much more secure, if you have the ability to differentiate between actual patterns and random results from sampling processes. Such questions might be:\nDo settlements tend to be situated on rather good soil or is the distribution random?\nIf we can differentiate between situations, where to settlement behaviour is just independent from disorder quality, and such situations, where decide quality or a specific side quality makes a difference for the settlement behaviour, we can learn a lot about settlement behaviour in general and it economical dependencies specifically.\nDo older individuals have more shoe-last celt as grave goods than younger?\nIf we assume, and archaeology did so, that shoe-last celt (Schuhleistenkeil) are a sign of social rank in neolithic Linear Band Ceramic groups, then their distribution among the individuals of different age classes can you give us an indication about the construction and reproduction of status and role in society. If, for example these items are evenly distributed among older and younger individuals, can we have an indication that it is not activities during lifetime that define the social rank of a person, but this rank can be inherited. Or the other way round, if these objects are concentrated in order age classes, we have an indication that only with a long life you get the allowance to have one of these objects as burial item. [One problem here: Of course, this is a vicious circle, because the fact that these objects occur in older age classes primarily make them suspicious for being a social marker of rank in the first place.]\nAll these questions and the related categories are probably not the first thing, and that comes to your mind, when you think about statistical tests. Here, we talk about nominal variables. And this is especially the value of the Chi-Squared test for archaeology: Most of our variables and of our interesting questions are nominal scaled. Let’s have a look how we can perform this test.\n\n5.6.1 Facts sheet\n\n5.6.1.1 \\(\\chi^2\\) Test for independence of two distributions\nRequirements: at least 1 nominal scaled variable (one sample case) and 1 nominal scaled grouping variable (two sample case)\nProcedure with one sample: observed values are compared with expected values given a certain distribution, no expected value should be &lt; 5; n should be &gt; 50\nProcedure with two samples: observed values of both distributions are compared with expected values if the samples would be even distributed, no expected value should be &lt; 5; n should be &gt; 50\nIf sample size is too small: Fishers Exact Test\nTest statistics: \\(\\chi^2\\)\nSignificance depend on degree of freedom (df)\n\n\n5.6.1.2 What do these facts mean\nMost of this will explain itself when we come to the practical examples. The test itself requires one nominal scaled variable, that means the counts of objects within at least more than one category. So actually not the count number, but the categorisation is the original variable. The nominal scaled grouping variable describes simply the fact that we can differentiate to samples, if we would like to compare to samples. We have to be able to distinguish between apples and oranges. In both cases, we compare the observed values of our dataset with some expected values, that will result from certain assumptions or expectations about how the distribution of the values should be. Also this will be more clearly understandable with the practical example. But in the one sample as well as in the two sample version of the test, there is a requirement of having no expected value (the number of objects of a certain category that you would expect given your assumptions) should be smaller than five, and the total number of cases should be at least 50. Please note, that if this pre-requisites cannot be fulfilled, you can use the so-called Fishers Exact Test instead. Its interpretation is very similar to the one for the Chi-Square test statistic. The test itself is called after the Chi-Square distribution that it uses as test statistic. In statistical tests, often certain theoretical distributions are used as shortcuts to approximates other, more complicated distributions to decide, whether or not the values might come from random processes. The last thing we have to explain to the concept of degrees of freedom.\n\n\n\n5.6.2 Excursus degree of freedom\nVery general, degree of freedom in statistics means a number of values in a calculation or equation that can vary freely. It is the amount of choices that you have before everything is determined. Let’s assume the following numbers of burials, divided into different classes:\n\n\n\n\nmale\nfemale\ntotal\n\n\n\n\ncremation\n\n\n201\n\n\ninhumation\n\n\n197\n\n\ntotal\n216\n182\n398\n\n\n\nWe have the total sums in the margins, we have in total 216 male burials, and 182 female burials. Also, we know already that we have 201 cremation burials and 197 inhumations. But the distribution within the table is not yet determined. We don’t know, how does different values are distributed among the classes. But if we have information about just one cell:\n\n\n\n\nmale\nfemale\ntotal\n\n\n\n\ncremation\n123\n\n201\n\n\ninhumation\n\n\n197\n\n\ntotal\n216\n182\n398\n\n\n\nNow everything is determined. We can calculate the number of male inhumations, we can also calculate the number of female creation cremations, And then also the number of female inhumations. With just one value set (and it doesn’t matter which value it is actually) the whole table is determined.\n\n\n\n\nmale\nfemale\ntotal\n\n\n\n\ncremation\n123\n78\n201\n\n\ninhumation\n93\n104\n197\n\n\ntotal\n216\n182\n398\n\n\n\nThis means, that this table structure has a degree of freedom of one.\ndf=1: if one value is chosen all other can be calculated with the help of the margins\nIn general, there is a very simple formula to calculate the degrees of freedom of such tables:\n(number of columns – 1)*(number of rows – 1)\nSo we have to substract from the number of column 1, and multiplied it by the number of rows -1. You can calculate the example above with this formula, or check the sample below with the same formula and have already an estimation about how many degrees of freedom this will have.\n\n\n\n\nmale\nfemale\nuncertain\ntotal\n\n\n\n\ncremation\n\n\n\n201\n\n\ninhumation\n\n\n\n197\n\n\ntotal\n196\n179\n23\n398\n\n\n\nNo we have three columns, and still two rows. According to our formula, we should have a degree of freedom of two. Let’s try that out. If we introduce one value, there is still possibility to vary other values freely. For example, if we fixed a number of female cremations, we still don’t know or can’t calculate all the other values.\n\n\n\n\nmale\nfemale\nuncertain\ntotal\n\n\n\n\ncremation\n\n78\n\n201\n\n\ninhumation\n\n\n\n197\n\n\ntotal\n196\n179\n23\n398\n\n\n\nOnly if we add another value, for example the number of male cremations, we can calculate all the other values.\n\n\n\n\nmale\nfemale\nuncertain\ntotal\n\n\n\n\ncremation\n113\n78\n\n201\n\n\ninhumation\n\n\n\n197\n\n\ntotal\n196\n179\n23\n398\n\n\n\n\n\n\n\nmale\nfemale\nuncertain\ntotal\n\n\n\n\ncremation\n113\n78\n10\n201\n\n\ninhumation\n83\n101\n13\n197\n\n\ntotal\n196\n179\n23\n398\n\n\n\nAs expected from our formula, we have a degree of freedom of two.\ndf=2: if two values are chosen all other can be calculated with the help of the margins\n(number of columns – 1)*(number of rows – 1)\nUsing again the formula, I’ll leave the last example for you to figure out yourself.\n\n\n\n\nmale\nfemale\nuncertain\ntotal\n\n\n\n\ncremation\n\n\n\n201\n\n\ninhumation\n\n\n\n197\n\n\nother\n\n\n\n30\n\n\ntotal\n201\n187\n40\n428\n\n\n\n\n\n5.6.3 \\(\\chi^2\\) Test for one sample (example after Shennan)\nFor demonstration using example from the Stephen Shennans book on quantifying archaeology: the number of Neolithic settlements on different soil types in eastern France\n\n\n\nSoil type\nNumber of settlements\n\n\n\n\nRendzina\n26\n\n\nAlluvial\n9\n\n\nBrown earth\n18\n\n\ntotal\n53\n\n\n\nThe question that should be answered here is whether or not there is a certain preference for specific soil types. From the data themselves it seems quite obvious, that Rendzina is very much preferred for settlements. So we will not need to statistical methods to check out, which of the sort of type is this preferred, but rather, if this high number of settlements could also result from just random effects. Or to be more concrete, how likely such a random configuration might be, and if it is likely to falsely exclude the possibility of random effects with more than our standard 5% probability for statistical significance.\nThis is actually a one sample test, because we have only one sample: the amount of settlements on the different soil types. It is probably easier to understand, that this is one variable, if you imagine it is a list of settlements, where we have the values “Rendzina”, “Rendzina”, “Brown earth”, “Alluvial”, and so on. With this it becomes clear that we have a variable that is nominal scaled.\nWe will calculate two versions of this test here: at first a naive version, in which we assume, that the soil types are evenly distributed over the landscape, with the same area ratio. In the second version, we will add information about the actual distribution of the soil types in the landscape to our equation, and with that we will alter the values that we will expect for its distribution.\n\n5.6.3.1 Version 1: even distributed\nIn this version, we assume that every soil type has the same proportion area in the landscape. If we now randomly distribute our settlements over the landscape, we would expect that each soil type has the same same probability of settlement. Consequently, we would expect that the number of settlements for each side type is the same. In total, we have 53 settlements. If we would evenly distribute these settlements over our three soil types, we would end up with the expection of 17.6667 settlements per soil type.\n\n\n\n\n\n\n\n\n\nSoil type\nNumber of settlements\nProportion of soil type\nexpected number of settlements\n\n\n\n\nRendzina\n26\n1/3\n17.6667\n\n\nAlluvial\n9\n1/3\n17.6667\n\n\nBrown earth\n18\n1/3\n17.6667\n\n\ntotal\n53\n1\n53\n\n\n\nNow we can formulate a hypothesis: We are interested, if the settlements are not evenly distributed. Therefore, we need to disprove or make unlikely an even distribution. Soour hypothesis look like that:\n\\(H_0\\): The settlements are evenly distributed on all soil types.\n\\(H_1\\): The settlements are not evenly distributed on all soil types.\nOf course, from the data we already see, that they are not distributed like our expectation. But of course, there is random chance that a certain soil type gets a higher number of settlements then another. This would also be true if we would use a dice to distribute the number of settlements. We are only interested in the situation, when this rolling a dice scenario is very unlikely, and a pattern behind this distribution cannot be ignored. For that, we have to measure how far the actual data are from our expectation (you could call that measure of surprise), and how likely such a distance from our expectation would be given the possible random nature of the distribution. For the later part, we will use pre-calculated tables, or the statistical program R. But the first part we can calculate ourselves.\nPlease don’t be afraid: here come to formula! But we will gently introduce and explain this formula to you.\nFormula for \\(\\chi^2\\):\n\\(\\chi^2=\\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i}\\)\n\\(O_i\\): number of observed cases\n\\(E_i\\): number of expected cases\n\\(\\chi^2\\): symbol for the test statistic chi-squared\nThe number of observed cases the present our actual data. This is the data that we observed. Consequently, the number of expected cases is represented by our column “expected number of settlements”. In this formula, from our exact cases our expectations are substracted to calculate the difference. Because we don’t care if I were observed cases are bigger or smaller than our expectations, because we are only interested in the difference, we square them to get rid of the sign. Then we have to take care, get our measure of surprise is independent from the number of cases. This is because the difference of 10 will probably surprise us, if the total number of cases is 20, but in the situation, where the total number is 5000, this difference of 10 probably is totally irrelevant. That is why we divide by the number of expected cases again, to normalise for the expected magnitude of our data. That must be repeated for every cell of the table. Once this is done, we can sum up all over individual measures of surprise to get the total surprise for the table, or more formally does Chi-Squared value for the table.\n\n\n\n\n\n\n\n\n\n\n\nSoil type\nNumber of observed cases\nNumber of expected cases\n\\(O_i - E_i\\)\n\\((O_i - E_i)^2\\)\n\\(\\frac{(O_i - E_i)^2}{E_i}\\)\n\n\n\n\nRendzina\n26\n17.6667\n8.3333\n69,4444\n3.9308\n\n\nAlluvial\n9\n17.6667\n-8.6667\n75,1117\n4.2516\n\n\nBrown earth\n18\n17.6667\n0.3333\n0.1111\n0.0063\n\n\ntotal\n53\n53\n\n\n8.18868\n\n\n\nIn this specific case, and starting from our expectations, we are rather surprised about the value of Rendzina, Because it is much higher than our expectations and even more surprised about the value of Alluvial, but this time, because it’s much lower than our expectation. We seem not to be surprised at all by the value of brown earth, because the difference to our expectation is below one. The total measure of surprise (or Chi-Square value) is 8.18868. Currently, we do not know how to interpret that, we have no framework for it. What approach could be now to calculate several random settings and see, how because the price can be given random distribution. This easily can be done today using a computer. Before a computer was available to everyone, this was very hard, and so a lot of precalculated tables were available. One of those food precalculated tables can be found in the book of Stephen Shennan. These tables are structured according to the level of significance (in our case 0.05) and the degrees of freedom (in our case 2, because we have one column for the observed and one column for the expected cases). With degree of freedom and level of significance we will find the threshold value. If our calculated threshold value is above this threshold, we have a significant result. This means, that there is a chance below 5% to go wrong to reject the null hypothesis, based on our data. In our case, the threshold is 5.99145. The value we have calculated is much higher, therefore we have a significant result. There’s quite enough evidence to assume, that there is a preference for specific soil type visible in this settlement behaviour.\n\n\n5.6.3.2 Version 2: even distributed with consideration of the proportion of the soil types on the total area\nIn the second version, we know how much area the difference or types take up in the landscape. This does not alter our data, but our expectations. If we know, how much percentage of the landscape is constituted off the individual soils, we also would exceed proportional amount of settlements to be situated on these soils. Now in this example, the distribution it’s not so far away from the even distribution, but slightly.\n\n\n\n\n\n\n\n\n\nSoil type\nNumber of settlements\nProportion of soil type\nexpected number of settlements\n\n\n\n\nRendzina\n26\n32%\n16.69\n\n\nAlluvial\n9\n25%\n13.25\n\n\nBrown earth\n18\n34%\n22.79\n\n\ntotal\n53\n1\n53\n\n\n\nFor example, Rendzina makes 32% of the size of the landscape, resulting in an expectation of 16.69 settlements on this soil (multiplying the total number of 53 settlements by this percentage). From here on the onward, everything is the same like in the example before. We have to compare our expectation with our data, measure our surprise, and compare that to the possible surprise from random effects.\n\\(\\chi^2=\\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i}\\)\n\n\n\n\n\n\n\n\n\n\n\nSoil type\nNumber of observed cases\nNumber of expected cases\n\\(O_i - E_i\\)\n\\((O_i - E_i)^2\\)\n\\(\\frac{(O_i - E_i)^2}{E_i}\\)\n\n\n\n\nRendzina\n26\n16.69\n9.04\n81.7216\n4.8185\n\n\nAlluvial\n9\n13.25\n-4.25\n18.0625\n1.1363\n\n\nBrown earth\n18\n22.79\n-4.79\n22.9441\n1.007\n\n\ntotal\n53\n53\n\n\n7.1885\n\n\n\nThe resulting number is slightly lower, but still beyond the threshold of 5.99145. So even considering the different proportions of the soils in the landscape, the result is still significant, and we have to interpret the preference pattern for specific soils\n\n\n5.6.3.3 \\(\\chi^2\\) test in R\nLet’s recreate the example in our. At first, we have to create a vector with our data.\n\nsettlements &lt;- c(26,9,18)\nnames(settlements) &lt;- c(\"Rendzina\",\"Alluvial\",\"brown earth\")\nsettlements\n\n   Rendzina    Alluvial brown earth \n         26           9          18 \n\n\nIf we don’t have any specific expectations about the distribution, we directly can input the vector into the commands. It will default to an even distribution, and will compare it to this.\n\nchisq.test(settlements)\n\n\n    Chi-squared test for given probabilities\n\ndata:  settlements\nX-squared = 8.1887, df = 2, p-value = 0.01667\n\n\nGiven our knowledge and our own endeavours, we can easily interpret the output of the command now. The X-squared is that Chi-Squared value that we also calculated. Because it’s a one sample test, R also calculate the degrees of freedom correctly. The last return value is the p-value. Instead of just giving us significant or not significant, R calculates the error probability for rejecting the null hypothesis. Since this p-value is below 0.05, we can reject the null hypothesis and collet a significant result.\nWe can introduce our expected proportions using the parameter p. Here, we give the percentages of the sort of types on the landscape as fractions, so that they sum up to 1\n\nchisq.test(settlements,p=c(0.32,0.25,0.43))\n\n\n    Chi-squared test for given probabilities\n\ndata:  settlements\nX-squared = 7.1885, df = 2, p-value = 0.02748\n\n\nYou can read the result in the same way like the one above. You will realise that the Chi-Squared value is lower, and that the p-value is higher, meaning there is a higher chance of making an error rejected the null hypothesis.\nNow that you have seen, how are you can calculate to one sample version by hand, and using R, we will turn our attention to the two sample version of the Chi squared test.\n\n\n\n5.6.4 Two sample case (Test for independence)\nIn the examples before, we have compared one sample to a theoretical distribution, also enriched with some external knowledge. There, we have analysed how well our sample fits to some assumptions about the population. In the two sample case, we compare if the distribution of the values is independent between two populations. The data from this task may also come from the same data collection. Using a grouping variable, we divide this sample into two possible independent populations and test, if this independence is actually true.\nAs an example, I will use the distribution of amber in the sites of the Unetice culture. Here, we compare graves and settlements in relation to the presence of amber. We would like to find out, if the deposition conditions, and the causing traditions and behaviour is so different that we have to assume that different Processes must have been involved at the different site categories.\nFor this, we construct 2 x 2 table. Quite often, you will encounter situations, where this 2 x 2 set up might be applicable to your data. For example, every time you may like to find out if the presence of what artefact is connected to the presence of another artefact, this restructuring of the data might be useful.\n(example after Hinz, beautified)\n\n\n\nType of site\namber\n\ntotal\n\n\n\n\n\n+\n-\n\n\n\nsettlement\n6\n18\n24\n\n\ngrave\n132\n44\n176\n\n\ntotal\n138\n62\n200\n\n\n\nSo, our scientific question is if amber is primary a grave good in this society, or if it is distributed evenly across the different site categories. Like in most other situations, also here we aim for a certainty (level of significance) of 0.05. So, we will reject or not hypothesis only if we are 95% sure that it is wrong. Having 2 x 2 table, we have two rolls and two columns, which results in a degree of freedom of 1 (remember the formula to calculate degrees of freedom from above).\nOur first task is to establish our expectation. We have to calculate the expected number of occurrence in the different side categories, given the assumption, that the distribution is independent from the site categories. In such a case, the number of sites with amber in settlements and in burials would only depend on The total number of sites with ember, and the ratio between sides of the type of settlement and sides of the type grave. Conversely, this is also true for the number of sites without amber.\nSo in total, we have 24 sites of the category settlement. If it would not matter, if it is a settlement or a burial, then the ratio between settlements with and without amber would be the same as the total ratio of sites with and without amber. The total ratio of amber present versus amber not present is 138 to 62. So we have more sites with amber in our sample then without. Of over 200 sites in total, 24 or settlements. That means, that 24÷200, or 12% of the sites are settlements. Also, 138÷200, or 69% of the sites have amber. That means, that of our 12% settlement sites (that is 24), 69% (that is 16.56) should be settlement sites with amber, given that amber is distributed independently from site categories.\nThis means, that we can use the margins of our table to calculate the expectation values. Using Cross-multiplication, we can multiply the margins and divide the result by the total number to get the number of sites that we would expect given independence. We need to repeat this procedure for every cell in our table, to get the expectation values for every individual possibility.\n\n\n\nType of site\namber\n\ntotal\n\n\n\n\n\n+\n-\n\n\n\nsettlement\n24/200*138 = 16.56\n24/200*62=7.44\n24\n\n\ngrave\n138/200*176=121.44\n62/200*176=54,56\n176\n\n\ntotal\n138\n62\n200\n\n\n\nFrom here onwards, the procedure is essentially the same like in the one sample case. For every cell, Using the Chi-Square formula, we calculate our measure of surprise, substracting the expectation from the observation, square result and divide by expectatio. Then we have to sum this up for the total table.\n\n\n\nType of site\namber\n\ntotal\n\n\n\n\n\n+\n-\n\n\n\nsettlement\nO=6 vs. E=16.56\nO=18 vs. E=7.44\n24\n\n\ngrave\nO=132 vs. E=121.44\nO=44 vs. E=54.56\n176\n\n\ntotal\n138\n62\n200\n\n\n\n\\(\\chi^2=\\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i}\\)\n\n\n\nType of site\namber\n\ntotal\n\n\n\n\n\n+\n-\n\n\n\nsettlement\n(6-16.56)^2/16.56=6.73\n(18-7.44)^2/7.44=14.99\n24\n\n\ngrave\n(132-121.44)^2/121.44=0.92\n(44-54.56)^2/54.56=2.04\n176\n\n\ntotal\n138\n62\n200\n\n\n\nThe total \\(\\chi^2\\) is 24.68. Looking up in the table, we will find for a significant level of 0.05 and a degree of freedom of Df=1 the threshold of 3.84146. This means, we are much more surprised by the distribution of our data then our threshold value for possible random processes indicates. Therefore, the result is significant, and we can reject the null hypothesis and state, given our sample, that amber seem to be primarily a grave goods in the Unetice culture.\n\n5.6.4.1 \\(\\chi^2\\) test for indipendence in R\nTo calculate the same table in R, at first we have to enter all our data. Since now we have a 2 x 2 table, we have to produce a matrix representing all data. The calculation of course only needs the numbers, but for convenience we name our matrix with row and column names. Please note that we are using the matrix() command. In this command, as first parameter we give our values at first for the first column, then for the second column and so on in a vector. Then, we specify that we want a matrix with two columns. With that, our values are distributed between these two columns, and the result is a table representing the data.\n\namber&lt;-matrix(c(6,132,18,44),ncol=2)\ncolnames(amber)&lt;-c(\"with amber\",\"without amber\")\nrownames(amber)&lt;-c(\"settlement\",\"grave\")\namber\n\n           with amber without amber\nsettlement          6            18\ngrave             132            44\n\n\nNow we can run our Chi-Squared test light before, entering the variable into the command for the test itself.\n\nchisq.test(amber)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  amber\nX-squared = 22.402, df = 1, p-value = 2.211e-06\n\n\nIf you have a closer look to the result, you will be able to see that R is given us the correct degree of freedom, and the P-value indicating a significant result, because it is below 0.05. But you also will see that the total Chi-Square value is only 22.402, and not 24.684, as we have calculated ‘by hand’! Did we, or R, made a mistake here?\nNo, actually not. On an even closer inspection, you will see, that the headline of the test result states that it is ‘Pearson’s Chi-squared test with Yates’ continuity correction’. By default, our is introducing a correction for small sample sizes, that is known as ‘Yates’ continuity correction’. Instead of directly squaring the observed minus the expectation values, it’s abstracts 0.5 from the absolute difference of both. The formula for this is \\((|O-E|-0,5)²/E\\). Substracting the small number does make a difference in case of a small sample size, because in this situation also the results from substracting the expectation from the observed will result in a small number. But if the result is large, because the sample size is also a large, 0.5 will not change so much in the end. Of course, we can also calculate the raw Chi-Square result. In this case we have to specify that we do not want the correction to take place.\n\nchisq.test(amber,correct=F)\n\n\n    Pearson's Chi-squared test\n\ndata:  amber\nX-squared = 24.684, df = 1, p-value = 6.753e-07\n\n\nNow the result is exactly like we have calculated it on our own. In case of small sample sizes, the correction already helps a bit to make the result robust. But in such a situation, especially when the number of cases is very small, it might be a good idea to use the Fishers Exact test instead of the Chi-Squared test. If you’re interested, you may find a description of this test procedure here, and its implementation R here.\n\n\n5.6.4.2 \\(\\chi^2\\) excercise\nYou can try out your skills in Chi-Square testing yourself using the following exercise:\n\nGiven all the animal bones from the Neolithic side of welcome Wolkenwehe (Mischka et al. 2005). We have here to Neolithic layers, one from the middle and one from the late Neolithic. The minimum number of individuals of different animals are divided into wild and domestic animals. Please test, if the ratio between wild and domestic animals is independent from the layer, and with that independent from the period of use of the site.\n\n\n\nlayer\nDomestic animal\nWild animal\n\n\n\n\n202 (late neolithic)\n159\n32\n\n\n203 (middle neolithic)\n84\n54\n\n\n\n\n\n\nSolution\n\nAt first, we have to construct our matrix for the data we have.\n\nww&lt;-matrix(c(159,84,32,54),ncol=2)\ncolnames(ww)&lt;-c(\"domestic\",\"wild\")\nrownames(ww)&lt;-c(\"late_neo\",\"mid_neo\")\nww\n\n         domestic wild\nlate_neo      159   32\nmid_neo        84   54\n\n\nThen, we performed a Chi-Squared test.\n\nchisq.test(ww)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ww\nX-squared = 19.634, df = 1, p-value = 9.376e-06\n\nchisq.test(ww, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  ww\nX-squared = 20.777, df = 1, p-value = 5.159e-06\n\n\nYou can see, that now, since we have more cases, the yates correction does not change the results in the same intensity like another example. But in both situations, we have a significant result: the differences between the different layers are statistical significant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#kolmogorovsmirnov-test",
    "href": "05-chapter.html#kolmogorovsmirnov-test",
    "title": "5  Nonparametric Tests",
    "section": "5.7 Kolmogorov–Smirnov test",
    "text": "5.7 Kolmogorov–Smirnov test\nOur second nonparametric test is the Kolmogorov–Smirnov test, or short KS test. This test can be used in situations, where are you having ordinal scales or better variable. Such a variable has more informational value, and therefore the test can also perform with a higher power. That means, that this test is more capable of avoiding type 2 errors. The downside is that it is only applicable if we have this kind of data present. Also, Kolmogorov–Smirnov test can be used as an goodness of fit test in the one sample case, and as a test for independence in the two sample case.\n\n5.7.1 Facts sheet KS-Test\nrequirements: at least one ordinal scaled Variable (one sample case) and 1 nominal scaled grouping variable (two sample case)\nProcedure one sample case: the cumulative procentual frequency of the sample is compared with a standard distribution (often normal distribution)\nProcedure two sample case: the cumulative procentual frequencies of the samples is compared\n\n5.7.1.1 What do these facts mean\nWe test against a at least ordinal scaled variable. Please note, that the KS-Test is also very useful in situations where you have a metric data. When we have to distinguish between two samples, we need again a at least nominal scaled grouping variable.\nIn the test procedure, we order our values according to the ordinal scale variable, and then we calculate the procentual frequency for every cell in our table. Finally, we sum these frequencies up starting from the top to the bottom. This gives us the cumulative procentual frequency. Now we calculate again the difference to our expectation, either against a theoretical distribution, or against the other sample. Then we are looking forward to the maximum difference, and compare this to a threshold, that this time we calculate ourselves given the number of cases that we have in the different samples. If we are more surprised by the difference then a random situation could produce within 95% of the cases, we assume statistical significance. Please follow example to understand precisely what this means.\n\n\n\n5.7.2 Example (after Shennan)\nHere, again we are using an example from the Stephen Shennan book. We are analysing female Bronze Age burials from a graveyard. The individuals buried our listed according to the different age classes. This age classes represent our ordinal variable.\n\n\n\n\n\n\nrich\npoor\n\n\n\n\ninfans I\n6\n23\n\n\ninfans II\n8\n21\n\n\njuvenilus\n11\n25\n\n\nadultus\n29\n36\n\n\nmaturus\n19\n27\n\n\nsenilis\n3\n4\n\n\nSum\n76\n136\n\n\n\n\n\n\n\nLooking at the data, you will realise that there are more individuals in the poorer category that died in younger ages. But also in total, there are more individuals in the category poor. You can have the hypothesis, get people that were equipped more rich when buried also represents richer individuals during lifetime. And you can also have the assumption, that rich individuals had more access to resources, and probably also had the chance to get older. So here, the question might arise, it needs to be answered in a statistical way, if there is a significant difference between the mortality pattern in both categories.\nFrom this data and our assumption, we build up our test configuration. Our hypothesis is that there is the difference between the two mortality patterns. Therefore, another hypothesis is that there is no difference. So we assume, that the distribution of age at death is independent from the number of burial items.\n\\(H_0\\): There is no difference between rich and poor graves according to age of death.\n\\(H_1\\): There is a difference between rich and poor graves according to age of death.\nSince the difference can be that poor died earlier, or later, we have a two-tailed test here. As always, our level of significance is 0.05. We use the age classes as ordinal scaled test variable, and the wealth classes as at least nominal scaled grouping variable. Wealth proceed can be understood as an ordinal scale. But we can always scale down the variable, so we treated just as if it is an nominal scaled one. In the table, the levels are already sorted according to our ordinal scaled variable. If not, of course you can easily do it yourself.\nLevel of significance: 0.05\nvariables:\n\nordinal scaled age classes\n(at least) nominale (ordinale) scaled wealth classes\n\nAt first, we will calculate the procentual frequency. That is the ratio of the individual classes in respect to the total number. Simply, we divide every cell by the total sum. If you would multiply that by 100, you would get the percentage.\n\n\n\n\n\n\nrich\nrich_ratio\npoor\npoor_ratio\n\n\n\n\ninfans I\n6\n0.0789474\n23\n0.1691176\n\n\ninfans II\n8\n0.1052632\n21\n0.1544118\n\n\njuvenilus\n11\n0.1447368\n25\n0.1838235\n\n\nadultus\n29\n0.3815789\n36\n0.2647059\n\n\nmaturus\n19\n0.2500000\n27\n0.1985294\n\n\nsenilis\n3\n0.0394737\n4\n0.0294118\n\n\nSum\n76\n1.0000000\n136\n1.0000000\n\n\n\n\n\n\n\nYou can see, that while the actual numbers sum up to the total number for each category, the ratios some up to 1. Is the next step, we have to calculate the cumulative frequency. That means, but with sum up the values from top to bottom for each row, including all the values of the categories that are smaller than the actual category. So in the first row, infans I, the cumulative frequency is actually the same value as the original frequency. In the second row, infans II, the cumulative frequency is the value of infans I plus the value of infans II. And so on.\n\n\n\n\n\n\nrich\nrich_ratio\nrich_cumsum\npoor\npoor_ratio\npoor_cumsum\n\n\n\n\ninfans I\n6\n0.0789474\n0.0789474\n23\n0.1691176\n0.1691176\n\n\ninfans II\n8\n0.1052632\n0.1842105\n21\n0.1544118\n0.3235294\n\n\njuvenilus\n11\n0.1447368\n0.3289474\n25\n0.1838235\n0.5073529\n\n\nadultus\n29\n0.3815789\n0.7105263\n36\n0.2647059\n0.7720588\n\n\nmaturus\n19\n0.2500000\n0.9605263\n27\n0.1985294\n0.9705882\n\n\nsenilis\n3\n0.0394737\n1.0000000\n4\n0.0294118\n1.0000000\n\n\nSum\n76\n1.0000000\n3.2631579\n136\n1.0000000\n3.7426471\n\n\n\n\n\n\n\nNow you can see, that while the sum of the procentual frequency is 1, but after cumulative frequency is the number different from 1. At the same time the last real value in this column is 1 (‘senilis’). And the other values are constantly rising going down our age classes. That means, for example in the role of ‘adultus’ the valley represents all the individuals that died in that age class or younger.\n\n\n\n\n\n\nrich_cumsum\npoor_cumsum\ndifference\n\n\n\n\ninfans I\n0.0789474\n0.1691176\n0.0901703\n\n\ninfans II\n0.1842105\n0.3235294\n0.1393189\n\n\njuvenilus\n0.3289474\n0.5073529\n0.1784056\n\n\nadultus\n0.7105263\n0.7720588\n0.0615325\n\n\nmaturus\n0.9605263\n0.9705882\n0.0100619\n\n\nsenilis\n1.0000000\n1.0000000\n0.0000000\n\n\n\n\n\n\n\nNow we have to identify the situation, we are both distributions different the most. For this, we have to substract one cumulative frequency from the other, and make the values absolute removing to sign. Having done this, we can identify those at the age class ‘juvenilus’ the difference is the highest with a value of 0.178. This is our test statistic that we will compare to a calculated threshold.\nBy comparing the percentages, our degree of surprise within our data is already independent of the number of cases. Thus, we now need to find a threshold value that indicates how large the deviation could be within a sample that is only dependent on the random distribution. At the same time, however, this surprise also depends on the number of cases: if we have fewer cases, larger differences can also arise purely by chance. The law of large numbers applies here, which we will get to know in more detail later: The more cases we consider, the more purely random effects will cancel each other out. Therefore, we need more surprise for a smaller number of cases in order to be convinced that this could not have been caused by a random distribution. At the same time, the required degree of surprise also depends on the significance level that we want to achieve: the more certain we want to be, i.e. the greater our significance level must be, the greater the surprise must be in order to be considered significant.\nIn the following formula for calculating threshold, both aspects are considered.\n\\(threshold = f * \\sqrt{\\frac{n_1 + n_2}{n_1 * n_2} }\\)\nIn the formula, you can see \\(n_1\\) and \\(n_2\\). These numbers represent the number of cases in the first and the second sample. In the numerator of the fraction, you can see that both sample sizes are added, while in the denominator they are multiplied. This means, that the resulting number will be higher, when the total sample size is smaller, and vice versa. The other factor is \\(f\\), which is the constant that depends on the level of significance that we would like to achieve.\nFactor f:\n\nLevel of significance 0.05: 1.36\nLevel of significance 0.01: 1.63\nLevel of significance 0.001: 1.95\n\nSo all we have to do is to fill in the numbers from our samples and the value for the level of significance that we would like to achieve. In our case, these are the following numbers:\nTotal number rich: 76 = \\(n_1\\)\nTotal number poor: 136 = \\(n_2\\)\nSo the calculation for a threshold is unfolding like this:\n\\(threshold = 1.36 * \\sqrt{\\frac{76 + 136}{76 * 136} } = 0.1947738\\)\nNo, we can compare that to the value that we have calculated from all sample:\nDifference max (D_max): 0.178\nYou can see, that 0.1947738 &gt; 0.178. This means, that the difference can not be considered to be significant.\nBut this does not mean, that the distribution is actually equal! Since we have made is dependent on the sample size, it is clear that there are two reasons why a significant result could not have been reached: Either, the resulting difference is really not big enough, so that it could result from a random process. Or I will sample size is just too small, so that we can really differentiate between random and patterned processes.\nHere, with archaeological thinking, we can already spot a possible error in our assumption: quite big differences exist in the lowest category, that is ‘infans I’. From our science we know, that burials of children are often not be equipped with very many of your items, independent from the circumstances in life. So here is a possible source of bias, that our calculation cannot reveal, but that we have to identify using our domain knowledge. But also in this situation, statistical tests can help to identify these divergence from our expectation, and make us think about possible reasons for this.\n\n\n5.7.3 KS-Test in R\nLet’s do the same test using R. You probably like to download the data for your own experimentation.\n\ngraeberbrz.csv\n\nFirst, let’s look at the data and inspect them:\n\ngraeberbrz &lt;- read.csv2(\"graeberbrz.csv\",\n                        row.names = 1)\nhead(graeberbrz)\n\n  alter reichtum\n1     1    reich\n2     1    reich\n3     1    reich\n4     1    reich\n5     1    reich\n6     1    reich\n\n\nYou can see, that in our dataset the age classes are coded using numbers starting from one for ‘infans I’ to 6 for ‘senilus’. Quite often it makes sense to recode ordinal variables in such a numerical way to make it accessible for calculations in R. With this recording, we achieved that the ordinal character preserved. We can use the table comand to display the full dataset with the frequency of burials in the different classes:\n\ntable(graeberbrz)\n\n     reichtum\nalter arm reich\n    1   6    23\n    2   8    21\n    3  11    25\n    4  29    36\n    5  19    27\n    6   3     4\n\n\nFor the ks.test(), which is the R command for conducting this test, it is convenient to take the data set apart into two vectors. The first vector will include the list of ages, the second vector will store the association with a specific wealth class.\n\nage &lt;- graeberbrz$alter\nhead(age)\n\n[1] 1 1 1 1 1 1\n\nwealth &lt;- graeberbrz$reichtum\nhead(wealth)\n\n[1] \"reich\" \"reich\" \"reich\" \"reich\" \"reich\" \"reich\"\n\n\nUsing the wealth vector as access criterion, we will feed into the ks.test() command age classes of the rich and the poor graves separatly as two samples\n\nks.test(age[wealth==\"arm\"],\n        age[wealth==\"reich\"]\n        )\n\nWarning in ks.test.default(age[wealth == \"arm\"], age[wealth == \"reich\"]):\np-value will be approximate in the presence of ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  age[wealth == \"arm\"] and age[wealth == \"reich\"]\nD = 0.178, p-value = 0.09\nalternative hypothesis: two-sided\n\n\nYou will see, that in the output the value D represents the maximum difference between the two samples that we also have calculated by hand. Additionally, you will see the P value that here is 0.09. So given the classical 0.05 significance level, we just have not reached enough certainty that we can speak about a patterned distribution with 95% security.\nYou also will see a warning, that there are ties. This means, that there are equal values in both samples present. In that case, exact P values cannot be calculated. This is only possible, if we have a metric variable instead of an ordinal ones, or a very large sample size (beyond 10000). For us, the approximated p-value is more than enough.\nLastly, you will see that the alternative hypothesis is two sided. This means, that by default the test assumes a two-tailed hypothesis. So in total, the test using R confirms our own calculation of the result: it is not significant.\n\n5.7.3.1 KS-test excercise\nYou can try out what you have learned using this exercise:\n\nThis dataset represents cups from ‘relative closed’ finds from late neolithic inventories (Müller 2001).\nFile: mueller2001.csv\nIf you inspect the dataset, you will see that it contains the height of cups and a classification whether there profile is subdivided by a break or not. Please analyse with the Kolmogorov-Smirnov-Test if the heigths of cups with and without breaks differ significant on a 0.05-level.\n\n\n\nSolution\n\nAt first, we have to load our data and construct our two sample vectors. This time, we divide the data set in a slightly different way, to give you an alternative. But it would also work in the same manner like we divided the example of the burials. Note also, that this situation we have metric data, and not ordinal ones.\n\ncups &lt;- read.csv2(\"mueller2001.csv\")\n\ncups.with_breaks &lt;- cups$hoehe[cups$tassentyp == \"zweigliedrig\"]\ncups.without_breaks &lt;- cups$hoehe[cups$tassentyp == \"eingliedrig\"]\n\nThen, we performed a KS test.\n\nks.test(cups.with_breaks, cups.without_breaks)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  cups.with_breaks and cups.without_breaks\nD = 0.252, p-value = 0.058\nalternative hypothesis: two-sided\n\n\nAlso here, there is no significant difference between the two types of cups in respect to the height.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "05-chapter.html#interpretation-of-significance-tests",
    "href": "05-chapter.html#interpretation-of-significance-tests",
    "title": "5  Nonparametric Tests",
    "section": "5.8 Interpretation of significance tests",
    "text": "5.8 Interpretation of significance tests\n\n5.8.1 Pay attention also when the statistic seem to be clear\nAs with every version of statistical tests, the quality of the results strongly depends on:\n\nHaving the right data\nAsking the right question\n\nSo even a significant result probably doesn’t mean anything, if you have asked the wrong question. Here, science and domain knowledge is much more important then statistical knowledge!\nAfter the test as well as before the test: The interpretation determines the result!\nAlso, what do you do with the significant results strongly depends on your interpretation. For example, if you analyse a dataset, you probably will find a lot of associations, especially if it is more complex. But not every dependence, not every association it’s meaningful! Remember that by definition you will get a significant result in every 20th test even if all underlying data are just random. This means:\nStatistically significant ≠ archaeologically significant!\nYou have to make sure, that the results that you can obtain really something that could also have relevance in the life of prehistoric people to become relevant.\nAll in all:\nStatistical results stay statistical: significance is always probability that the choice of a hypothesis is correct, but there is also a probability that it is by chance…\n\n\n5.8.2 Statistical association not mean causal association!\nThe classic saying in statistics is that association does not mean causation. Even if there is really a pattern in the data, this could result from observed variables, all variables that you did not take into account when constructing your hypothesis.\nOne example according to the Stephen Shennan book is the association between grave size and sex of the buried individuals Although there might be a statistically significant association between grave size and sex, this could be caused by a third factor. Maybe mail individuals have just bigger body height, making it necessary to dig bigger burial pits so that they can fit in. A conclusion which says that grave size are causally determined by sex would be at least not correct.\n\nAlways, doing statistical analyses does not save you from making your own thoughs and trying to contact good science!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "10-chapter.html",
    "href": "10-chapter.html",
    "title": "10  Correspondence analysis",
    "section": "",
    "text": "10.1 Background",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#background",
    "href": "10-chapter.html#background",
    "title": "10  Correspondence analysis",
    "section": "",
    "text": "10.1.1 Idea and Basics\nThe basic assumption underlying correspondence analysis is the same as the one we used in cluster analysis: similar things have similar characteristics, and can therefore be grouped together based on their features. Just like cluster analysis, correspondence analysis is a Pattern Detection technique. Both could also be called exploratory methods. Their aim is to discover patterns in the data. Using correspondence analysis as a statistical procedure does not in any way prove that the patterns found are actually significant. For the purpose of significance testing, one must resort to standard procedures of statistical hypothesis testing.\nIn contrast to cluster analysis, correspondence analysis is inherently a visual procedure. It is primarily used to visualise contingency tables or presents absence matrices. Correspondence analysis belongs to the broader field of ordination methods, which involves ordering objects according to their characteristics and then visually representing this order.\nThe basic idea of visualisation by means of a correspondence analysis is the following: the representation of the objects (called ‘sites’ in vegan jargon) and the properties (mostly types in archaeology, called ‘species’ in vegan jargon) are represented in a common coordinate system. The intuition here is that objects that are closer together should also be more similar than those objects that are positioned far away from each other. For this, of course, similarities must first be determined. This similarity determination is carried out in the context of the correspondence analysis by means of the chi-square method, as we have already become familiar with in the context of the chi-square test.\nJust like the chi-square test, the correspondence analysis has very low requirements for the data quality of the variables due to the underlying mathematical method (the chi-square method). A data matrix with at least nominally scaled variables is sufficient for the procedure. Higher scaled variables are also possible, but then the information contained in this higher scale level is lost. These low demands on the scaling level make the correspondence particularly attractive for archaeological questions, as we often only have nominally scaled variables, such as different types at a site, or different nominally determined features on pottery vessels.\nThe basic sequence can be stated as follows. 1:\n\nthe data are standardised across data sets to a comparable measure\nthe data standardised in this way are projected into the multidimensional variable space.\nthose vectors are determined that gradually absorb the most information from the data and are oriented at right angles to each other at the same time.\nthe data points of the objects as well as of the properties are projected with their coordinates onto those planes which are formed by the vectors.\nthe resulting new coordinates can now be represented in a diagram in relation to the individual vectors (dimensions).\n\n .caption[.tiny[source: http://www.aapspharmscitech.org] ]\n .caption[.tiny[source: http://www.cs.mcgill.ca]]\n\n\n10.1.2 History\nCorrespondence analysis itself was developed in the context of biological and psychological studies. Its algebraic foundations were laid in the 1940s by Hartley and Guttmann. The first explicit use of correspondence analysis was by Benzéncri in the 1960s in the field of linguistic studies. Subsequently, correspondence analysis was further explored and developed by various research groups. This led to the emergence of different versions, but also to the establishment of different names for the same procedure. In 1984, Greenacre wrote the basic monograph on the method, which is still considered the standard work on the subject.\nIn archaeology, correspondence analysis appeared first and mainly in the context of seriation. Seriations were already carried out and published in 1899 by William Flinders Petri. However, these were still done by hand and eye, and were not yet supported by mathematical methods. In the 1960s, the application of various mathematical methods for seriation studies experienced a great upswing (Kendall, Münsingen). In Germany, the first experiments with seriation using computer-assisted methods were published by Goldmann in 1979 as part of his dissertation, in which the method of reciprocal averaging was used for seriation. Only subsequently did correspondence analysis emerge as a method for this purpose. This procedure was used particularly extensively in the chronological order of the Rhenish Linearband Pottery. The procedures used in this context were further developed especially by the University of Cologne. Another focus of the application of correspondence analysis in German-speaking countries is the University of Kiel.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#conducting-a-correspondence-analysis",
    "href": "10-chapter.html#conducting-a-correspondence-analysis",
    "title": "10  Correspondence analysis",
    "section": "10.2 Conducting a correspondence analysis",
    "text": "10.2 Conducting a correspondence analysis\n\n10.2.1 Data formats\nFor correspondence analysis, data is usually available in two-dimensional form: the individual rows of the data table represent the individual items. These can be individual sites, but also individual pottery vessels, depending on the scale of the respective investigation. The individual columns, in turn, represent the features that were observed on the individual items. Depending on the scale of the investigation, this can also reach very different levels of granularity.\n\n10.2.1.1 Presence-absence data\nThe table can contain presence-absence data. Here, a one in the corresponding cell indicates that the feature is present on the object, whereas the zero represents the absence of this feature.\n\nlibrary(magrittr)\nlibrary(kableExtra)\nburials %&gt;% addmargins() %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nBurial1\n1\n1\n0\n2\n\n\nBurial2\n0\n1\n1\n2\n\n\nBurial3\n1\n1\n1\n3\n\n\nBurial4\n1\n0\n1\n2\n\n\nSum\n3\n3\n3\n9\n\n\n\n\n\n\n\n\n\n10.2.1.2 Contingency table\nAnother possibility, which is particularly useful for more comprehensive analyses, is the representation as a contingency table: Here, an absolute number is used to indicate how many of the characteristics are observable on the respective item.\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nSettlements\n20\n23\n40\n83\n\n\nHoards\n23\n10\n6\n39\n\n\nBurials\n10\n56\n4\n70\n\n\nSum\n53\n89\n50\n192\n\n\n\n\n\n10.2.1.3 Burt matrices\nAnother possibility how data can be used in a correspondence analysis is the so-called Burt matrix. In this case, features and items are no longer listed with respect to each other in a table, but only the correspondence between features matching items, or the correspondence between items matching features.\nTo illustrate this, let us transform the table of grave goods and individual burials from above into one respectively two Burt matrices. Let us start with the matrix for the items. For all burials, we look in pairs at how many present attribute values they match. This number marks the cell content for a table in which all items (in our case, burials) are plotted against each other in the rows and columns.\nIn the case of burial one, pot and cup are present. This means that burial one is assigned the value two in relation to burial one itself, since we have two positive feature values present. In relation to burial two, the value one is assigned, as both are in agreement positively only in relation to the cup. In relation to burial three, the value is again two, as both top and cup match, but burial one does not include a fibula. In relation to burial four, again, only the pot matches, cup and fibula are missing in one of the two graves respectively. This now continues for all pairwise items. At the end of this procedure we should arrive at the following table:\n\n\n\n\n\n\nBurial1\nBurial2\nBurial3\nBurial4\n\n\n\n\nBurial1\n2\n1\n2\n1\n\n\nBurial2\n1\n2\n2\n1\n\n\nBurial3\n2\n2\n3\n2\n\n\nBurial4\n1\n1\n2\n2\n\n\n\n\n\n\n\nInstead of counting out the individual values directly, we can also use an operation from matrix algebra: we multiply the matrix of presence-absence data by itself, with the second instance transposed, i.e. rotated by 90°. The result is the same as that of the process we performed by hand above.\n\nburials %*% t(burials)\n\n        Burial1 Burial2 Burial3 Burial4\nBurial1       2       1       2       1\nBurial2       1       2       2       1\nBurial3       2       2       3       2\nBurial4       1       1       2       2\n\n\nIn this way we have created a Burt matrix for the sites. In the same way, we can also create a matrix for the features, i.e. the species. Only we have to exchange which of the matrices we rotate by 90°.\n\nt(burials) %*% burials\n\n       Pot Cup Fibula\nPot      3   2      2\nCup      2   3      2\nFibula   2   2      3\n\n\nThe resulting matrices each record the relevant matches, but focus only on either the characteristics or the items. The diagonal contains the number of matches observed for the individual features or items with themselves. These are not actually relevant to the original problem of the seriation. Therefore, one can also set this diagonal to zero in order to focus even more strongly on the correlations between the individual variables or items. Such a modified Burt matrix can often give a clearer seriation and correspondence analysis result.\n\nburt.s &lt;- burials %*% t(burials)\ndiag(burt.s) &lt;- 0\nburt.s\n\n        Burial1 Burial2 Burial3 Burial4\nBurial1       0       1       2       1\nBurial2       1       0       2       1\nBurial3       2       2       0       2\nBurial4       1       1       2       0\n\n\n\n\n\n10.2.2 Standardising to relative frequency\nIn the next step, we bring the values within the data table (be it the presence absence or contingency table) to a uniform measure. Here we use the relative numbers per cell, i.e. we divide the total table by the total sum of all entries in this table.\nLet us continue to illustrate this with the presence-absence table for the burials:\n\n\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nBurial1\n1\n1\n0\n2\n\n\nBurial2\n0\n1\n1\n2\n\n\nBurial3\n1\n1\n1\n3\n\n\nBurial4\n1\n0\n1\n2\n\n\nSum\n3\n3\n3\n9\n\n\n\n\n\n\n\nHere I have included the marginal sums. The total sum of the table is nine. If we now divide each individual cell of the table by nine, the total sum is only one, and the respective entries are one ninth.\n\n\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nBurial1\n0.11\n0.11\n0.00\n0.22\n\n\nBurial2\n0.00\n0.11\n0.11\n0.22\n\n\nBurial3\n0.11\n0.11\n0.11\n0.33\n\n\nBurial4\n0.11\n0.00\n0.11\n0.22\n\n\nSum\n0.33\n0.33\n0.33\n1.00\n\n\n\n\n\n\n\nIn addition to the individual values within the table, which now represent the relative frequency, the newly created marginal sums are also relevant. We will need these to calculate the expected values for the Chi Square value. Furthermore, we will need this information later for scaling the coordinates for the visual representation. These marginal sums are also called row or column profiles, the individual values within these are also called row masses or column masses.\n.tiny[ Row profile:\n\n\nBurial1 Burial2 Burial3 Burial4 \n   0.22    0.22    0.33    0.22 \n\n\nColumn profile:\n\n\n   Pot    Cup Fibula \n  0.33   0.33   0.33 \n\n\n\n\n10.2.3 Calculation of expected values\nThe correspondence analysis, as already indicated several times, is based on the chi-square value in its measure for the individual similarities. We have covered the following formula for the calculation of the chi-square value within the chi-square test:\n\\(\\chi^2=\\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i}\\)\nSo we relate the observed values to the expected values, assuming that no specific pattern exists, and thus measure the degree of our surprise with regard to the pattern in the data. For this calculation, we need the expected values as well as the observed values. In fact, the measure used in the correspondence analysis is a slight modification of the above formula: instead of the chi-squared value, we use a Z value that is not squared, which means that we can have positive and negative values.\n\\(z_{ij}=\\frac{(O_i - E_i)}{\\sqrt{E_i}}\\)\nNevertheless, we also need the expected values for the calculation of this value, which, as you will remember from the lesson on the chiquadrat test, results from the marginal sums. In the case that no pattern exists, the expected value for the individual cell is the column sum times the row sum, divided by the total sum. The latter is omitted in our case here, since the total sum is already normalised to one. Thus, we can calculate the total expected value for the table quite quickly:\n\n\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nBurial1\n0.07\n0.07\n0.07\n0.22\n\n\nBurial2\n0.07\n0.07\n0.07\n0.22\n\n\nBurial3\n0.11\n0.11\n0.11\n0.33\n\n\nBurial4\n0.07\n0.07\n0.07\n0.22\n\n\nSum\n0.33\n0.33\n0.33\n1.00\n\n\n\n\n\n\n\nNow, with the expected values at hand, we can put them together with the observed values into the formula above. I will spare you the actual calculation, as we have already done this in a similar way for the chi-square test. Instead, below is just the table with the results for the normalised values:\n\n\n\n\n\n\nPot\nCup\nFibula\nSum\n\n\n\n\nBurial1\n0.14\n0.14\n-0.27\n0\n\n\nBurial2\n-0.27\n0.14\n0.14\n0\n\n\nBurial3\n0.00\n0.00\n0.00\n0\n\n\nBurial4\n0.14\n-0.27\n0.14\n0\n\n\nSum\n0.00\n0.00\n0.00\n0\n\n\n\n\n\n\n\nThe first thing we can derive directly from this table is the measure of the overall dispersion of the data. This means how much variability (or potential for surprise, to stick with the language I have now used more often) there is in our data set. The measure of surprise is, in fact, the chi-square value. To make this comparable between different data sets, we have to divide it by the number of cases. In purely mathematical terms (I will spare us all the derivation), however, we can also sum up the normalised values over the entire table and obtain the same number as a result. This can also be expressed as a formula, which looks quite impressive due to its two sums signs, but is basically quite trivial. The second part of the formula means nothing other than that all values are simply summed up over all rows and columns of the table.\n\\(I = \\frac{\\chi^2}{n} = \\sum_i \\sum_j z_{ij}^2\\)\nThe result of this calculation is a measure of the dispersion of the data in relation to the number of cases. This measure is also called inertia. This term appears frequently when correspondence analysis is involved. It means nothing other than how volatile or dissimilar the individual data in the overall data set are to each other.\nThe total inertia in our example is: 0.3333333\nThe second thing we can derive are the coordinates of our data in the multidimensional data space. This is nothing other than the values of the individual data sets in relation to the individual variables. In our case, these variables are normalised with the normalisation we just did. The individual axes, however, still correspond to the original variables that we have provided. This corresponds to the representation we have already used above (Figure \\(\\ref{multivariablespace}\\)).\nThere are three variables in our data set. In a three-dimensional representation, we can now plot all three variables in a coordinate system and represent our points (our individual data) in it. In the HTML version of this book, the following picture is movable with the mouse: Just click on it, keep the mouse button pressed and move the mouse. I am not yet sure how to do this in the printed version.\n\n\n\n\n\n\nBut this is exactly where the problem lies. Normally, we are not able to see all variables at the same time, even with a three-dimensional representation. Therefore, one of the most important concerns in correspondence analysis is to reduce the dimensions while at the same time reflecting as much information as possible. Correspondence analysis thus belongs to the so-called dimension reduction methods. As already explained above, we now look for the vectors that run through the largest scatter in our data cloud. The first vector to be determined is the one that covers the largest extent and thus also absorbs the most variability in the data set. The next vector is determined in such a way that it lies at right angles to the first, and at the same time runs through the next largest possible extent of the data cloud. And so on.\nTwo vectors in space describe a plane. So once we have determined the first and second vectors, we can use these two to define a plane that represents our future coordinate space. The vectors themselves become the axes of the new coordinate system. Projected perpendicularly onto this plane (as if you were looking at it from above, or as if you were shining a torch on the scenery from above so that the data points cast shadows), the individual data points can now be represented on this plane. And with this, we have obtained a new view of our data, in which the greatest dissimilarities are mapped via the vectors. So we get a representation that captures as much of the information of the original data set as possible, but does it with only two dimensions. And this can be done for any combination of vectors (dimensions).\n .caption[.tiny[source: http://www.cs.mcgill.ca]] ]\n\n\n10.2.4 Extraction of dimensions\nMathematically, this determination of the largest extension of the point cloud and the projection of the points onto the resulting vectors is the solution of a complex linear equation system. And a proven means for such a task is the so-called singular value decomposition (SVD). This method is used in many fields, ranging from image compression methods such as JPEG, to the reconstruction of three-dimensional objects, to such esoteric applications as particle physics. In our humble use case, it is used to perform data reduction. But because of the importance of this process in many areas, one of its developers (Gene Golub) has appropriated the abbreviation for his car registration plate:\n .caption[.tiny[Gene Golub’s license plate, photographed by Professor P. M. Kroonenberg of Leiden University.]]\nSpecifically, the data matrix of our similarities is decomposed into three matrices.\n\\(Z=U∗S∗V'\\)\nZ : Matrix with the standardized data (our input)\nU : Matrix for the row elements\nV : Matrix for the column elements\nS : Diagonal matrix with the singular values\nZ are the normalised values we calculated above. Three matrices are created from these. U is the matrix that gives the new coordinates on the vectors for the rows of our original data (i.e. the cases, in our case the burials). V is in the same sense the matrix for the columns, in our case the different types. These can also be oriented in the coordinate system, so to speak as an ideal case for the occurrence of this form. We can determine similarities of the objects in relation to their properties, but in the same framework we can also determine similarities of the properties in relation to the objects in which they are represented. The last matrix S is a diagonal matrix, i.e. it consists almost exclusively of zeros, only its diagonal has values. In our case, these values represent the information about how much of the variability of the total data set is included in the individual dimensions of the new coordinate system. If all three matrices are multiplied together by means of matrix multiplication, the initial matrix Z is obtained again.\nHow this matrix decomposition works is hard to do and even harder to understand if you haven’t done a semester of matrix algebra. Fortunately, R takes care of that for us. And of course, as we will see later, there is also a command that does all the correspondence analysis for us in one piece. Nevertheless, we will proceed step by step here, and first perform the singular value decomposition for our normalised data:\n\nburial.z&lt;-read.csv2(\"burial_z.csv\",row.names=1)\nburial.svd&lt;-svd(burial.z)\nburial.svd\n\n$d\n[1] 4.082483e-01 4.082483e-01 1.005681e-15\n\n$u\n              [,1]          [,2]       [,3]\n[1,] -7.071068e-01 -4.082483e-01 -0.5530935\n[2,] -1.367413e-16  8.164966e-01 -0.5530935\n[3,]  2.077635e-17  6.299174e-32  0.2868150\n[4,]  7.071068e-01 -4.082483e-01 -0.5530935\n\n$v\n           [,1]       [,2]      [,3]\n[1,]  0.0000000 -0.8164966 0.5773503\n[2,] -0.7071068  0.4082483 0.5773503\n[3,]  0.7071068  0.4082483 0.5773503\n\n\nSVD and Inertia The singular values (eigenvalues) represent the inertia. The eigenvalues\n\nburial.svd$d\n\n[1] 4.082483e-01 4.082483e-01 1.005681e-15\n\n\nThe squared eigenvalues are the inertia of the individual dimensions\n\nburial.svd$d^2\n\n[1] 1.666667e-01 1.666667e-01 1.011395e-30\n\n\nThe sum of the squared eigenvalues is equal to the total of the intertia.\n\nsum(burial.svd$d^2)\n\n[1] 0.3333333\n\n\nIf the inertia of the individual dimensions is divided by the total inertia, the (eigenvalue) proportion of the dimensions is obtained.\n\nburial.svd$d^2/sum(burial.svd$d^2)\n\n[1] 5.000000e-01 5.000000e-01 3.034184e-30",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-procedure-using-a-presenceabsence-matrix",
    "href": "10-chapter.html#correspondence-analysis-procedure-using-a-presenceabsence-matrix",
    "title": "10  Correspondence analysis",
    "section": "10.3 Correspondence analysis: Procedure (using a presence/absence matrix)",
    "text": "10.3 Correspondence analysis: Procedure (using a presence/absence matrix)\n\n10.3.1 Normalization of coordinates\nScaling of the coordinates in such a way that\nThe dimensions are weighted according to their proportion of the total inertia.\nThe rows/columns are weighted according to their proportion of the mass.\n.pull-left[\nRow (sites) Points: \\(r_{ik} = \\frac{u_{ik}*\\sqrt{s_k}}{\\sqrt{p_i}}\\)\n] .pull-right[\nColumn (species) Points: \\(c_{jk} = \\frac{v_{jk}*\\sqrt{s_k}}{\\sqrt{p_j}}\\)\n]\n\\(u\\), \\(v\\) → Matrices of rows/columns from the SVD\n\\(s_k\\) → Diagonal matrix\n\\(p_i\\) , \\(p_j\\) → Masses of rows/columns from the relative frequency",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-procedure-using-a-presenceabsence-matrix-1",
    "href": "10-chapter.html#correspondence-analysis-procedure-using-a-presenceabsence-matrix-1",
    "title": "10  Correspondence analysis",
    "section": "10.4 Correspondence analysis: Procedure (using a presence/absence matrix)",
    "text": "10.4 Correspondence analysis: Procedure (using a presence/absence matrix)\nEverything in R:\n.pull-left[\n\nlibrary(vegan)\n\nburial &lt;- read.csv(\"burials.csv\",\n                   row.names = 1)\nburial.cca &lt;- cca(burial)\nplot(burial.cca, scaling=3)\n\nscaling=3: by default R normalizes only the species (types)\n\nscaling = 1 : Normalization of sites\nscaling = 2 : Normalization of the Species\nscaling = 3 : Symmetrical normalization of sites and species\nscaling = 0 : No normalization ]\n\n.pull-right[\n\n\n\n\n\n\n\n\n\n]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case",
    "href": "10-chapter.html#correspondence-analysis-real-world-case",
    "title": "10  Correspondence analysis",
    "section": "10.5 Correspondence analysis: Real World case",
    "text": "10.5 Correspondence analysis: Real World case\n\n10.5.1 Münsingen Burial Site\n.pull-left[ .small[\n\nmuensingen &lt;- read.csv(\"muensingen_ideal.csv\",\n                       row.names = 1)\nmuensingen.cca &lt;- cca(muensingen)\nplot(muensingen.cca, scaling=3)\n\n] ]\n.pull-right[\n\n\n\n\n\n\n\n\n\n]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case-1",
    "href": "10-chapter.html#correspondence-analysis-real-world-case-1",
    "title": "10  Correspondence analysis",
    "section": "10.6 Correspondence analysis: Real World case",
    "text": "10.6 Correspondence analysis: Real World case\n\n10.6.1 Münsingen Burial Site\n.pull-left[ .tiny[\n\nscores(muensingen.cca, display = \"sites\")\n\n              CA1          CA2\n32   1.606313e+00 -1.452925953\n31   1.417566e+00 -1.191711661\n8b   1.335804e+00 -1.088200431\n12   1.415720e+00 -1.195513769\n8a   1.381076e+00 -1.183971647\n6    1.318179e+00 -1.097469151\n9    1.305596e+00 -1.130528153\n23   1.172513e+00 -0.912136067\n44   7.886929e-01 -0.469460799\n51   1.207199e+00 -0.998288130\n40   1.032187e+00 -0.663168035\n28   4.135180e-01  0.009305833\n62   6.073775e-01 -0.192755350\n91   2.594931e-01  0.273907559\n72   3.852720e-01  0.009685198\n80   4.578284e-01 -0.135341372\n46   4.999726e-01  0.062684002\n48   4.999726e-01  0.062684002\n49   4.664078e-01  0.040744687\n68   2.368297e-01  0.259427802\n79   2.812150e-01  0.075938349\n61   1.788927e-01  0.267615201\n102  1.720921e-02  0.473091324\n81  -5.781215e-02  0.535954589\n84  -4.796386e-05  0.457809401\n86   1.289481e-01  0.324469138\n130 -2.266955e-01  0.659478553\n136 -1.993537e-01  0.662413700\n140 -2.351985e-01  0.669474019\n135 -2.284840e-01  0.688394387\n121 -3.173622e-03  0.500283343\n145 -2.413927e-01  0.629709178\n75  -2.814656e-01  0.701537467\n98  -4.340398e-02  0.475096777\n134 -2.593867e-03  0.404872794\n157 -3.108071e-01  0.614129078\n161 -8.740028e-01  0.494958429\n171 -4.762109e-01  0.691500466\n180 -1.801351e+00 -1.487499183\n181 -1.765745e+00  0.359016975\n164 -1.721644e+00  0.225999084\n168 -9.925575e-01  0.232793904\n149 -4.501877e-01  0.660128687\n184 -1.631521e+00  0.375301510\n211 -4.377063e+00 -6.322384039\n212 -3.835635e+00 -4.831052095\n193 -5.668999e+00 -8.843550371\n214 -5.668999e+00 -8.843550371\n\n\n] ]\n.pull-right[ .tiny[\n\nscores(muensingen.cca, display = \"species\")\n\n                                                   CA1         CA2\nLT.A.Fibel                                  1.26553218 -1.02575507\nHalsring.einfach.geritzt..Vollguss          1.42268370 -1.23712903\nArm.Fussring.einf...vollg.loch.Steckv.      1.39589208 -1.19799158\nArm..Fussring.einfach.geritzt..hohl         1.17884979 -0.91744662\nGlasperlen                                  1.05767259 -0.76556860\nBernsteinkette                              0.90576411 -0.59674656\nArm..Fussring.gerippt.vollguss              1.36065846 -1.16302096\nHirschgeweih                                1.36694980 -1.14649146\nHalsring.m..Muffen                          1.34962749 -1.14072040\nArmring.mit.Muffen                          1.38107585 -1.18397165\nDraht.Fingerring.runder.QS                  0.54905934 -0.42294391\nArm..Fussring.vollguss.massiv               0.30773335  0.02653237\nHalsring.plastisch..vollguss                0.99794588 -0.73387446\nHalsring.hohlblech..geritzt                 1.03218740 -0.66316804\nArm..Fussringe.gerippt.dicht                0.54345897 -0.03844059\nCertosafibel                                0.63923617 -0.20818750\nSchwert                                     0.26898152  0.14758021\nKette                                      -0.08973973  0.23569196\nLanze                                       0.20230485  0.23181940\nLT.B1.Fibel                                 0.21181418  0.25693309\nArmreif.mit.Korallenauflage                 0.04601620  0.42893018\nFingerring.flachblech                      -0.09756829  0.46147527\nSchaukelfingerringe                        -0.32647874  0.58102969\nArm..Fussring.plastisch.gerippt            -0.15984628  0.57814339\nLT.B2.Fibel                                -0.31048691  0.58499166\nArm..Fussring.genoppt..plastisch..Vollguss -0.23211926  0.55870602\nRing..Fuss..Armring.Blech.um.Eisen.Ton     -0.36582667  0.68083308\nHohlbuckelarmringe                         -0.34680816  0.65312662\nFingerring.mehrfach.gewickelt.plastisch    -1.51503177  0.21905089\nLT.C1.Fibel                                -1.47959077  0.33836796\nGlasarmring.gerippt                        -2.65992172 -3.09912747\nGlasarmring.genoppt                        -1.76574493  0.35901698\nGlasarmring.Fadenauflage                   -1.23558643  0.23052896\nGürtelkette                                -1.72630918 -0.66736578\nLT.C2.Fibel                                -4.88767414 -7.21013422\n\n\n] ]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case-2",
    "href": "10-chapter.html#correspondence-analysis-real-world-case-2",
    "title": "10  Correspondence analysis",
    "section": "10.7 Correspondence analysis: Real World case",
    "text": "10.7 Correspondence analysis: Real World case\n\n10.7.1 Münsingen Burial Site\n.pull-left[ .tiny[\n\nplot(muensingen.cca, display = \"sites\")\n\n\n\n\n\n\n\n\n] ]\n.pull-right[ .tiny[\n\nplot(muensingen.cca, display = \"species\")\n\n\n\n\n\n\n\n\n] ]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case-3",
    "href": "10-chapter.html#correspondence-analysis-real-world-case-3",
    "title": "10  Correspondence analysis",
    "section": "10.8 Correspondence analysis: Real World case",
    "text": "10.8 Correspondence analysis: Real World case\n\n10.8.1 Münsingen Burial Site\n.pull-left[ .tiny[\n\nplot(muensingen.cca, choices = c(1,2))\n\n\n\n\n\n\n\n\n] ]\n.pull-right[ .tiny[\n\nplot(muensingen.cca, choices = c(1,3))\n\n\n\n\n\n\n\n\n] ]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case-4",
    "href": "10-chapter.html#correspondence-analysis-real-world-case-4",
    "title": "10  Correspondence analysis",
    "section": "10.9 Correspondence analysis: Real World case",
    "text": "10.9 Correspondence analysis: Real World case\n\n10.9.1 Münsingen Burial Site\n.pull-left[ .tiny[\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmuensingen.species &lt;- data.frame(\n  scores(muensingen.cca)$species\n  )\nggplot(muensingen.species,\n       aes(x=CA1,\n           y=CA2,\n           label=rownames(muensingen.species))) +\n  geom_point() + geom_text_repel(size=2)\n\n] ]\n.pull-right[\n\n\nWarning: ggrepel: 17 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-real-world-case-5",
    "href": "10-chapter.html#correspondence-analysis-real-world-case-5",
    "title": "10  Correspondence analysis",
    "section": "10.10 Correspondence analysis: Real World case",
    "text": "10.10 Correspondence analysis: Real World case\n\n10.10.1 Münsingen Burial Site\n.pull-left[ ]\n.pull-right[ ]\nhttp://tosca.archaeological.science",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "10-chapter.html#correspondence-analysis-interpretation",
    "href": "10-chapter.html#correspondence-analysis-interpretation",
    "title": "10  Correspondence analysis",
    "section": "10.11 Correspondence Analysis: Interpretation",
    "text": "10.11 Correspondence Analysis: Interpretation\n\n10.11.1 Guttman effect (horseshoe, parabola)\n.pull-left[ In archaeology, this is often regarded as evidence of a temporal orientation.\nThe Guttman effect occurs when a process affects the data on multiple levels.\nThe largest influencing factor, given a longer runtime, is mostly the time, but:\nThis does not always have to be the case.\nCheck against other information necessary. ]\n.pull-right[\n\n\n\n\n\n\n\n\n\n]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correspondence analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html",
    "href": "09-chapter.html",
    "title": "9  Cluster Analysis",
    "section": "",
    "text": "9.1 Idea and Basics\nArchaeology is the great divider. One of the most important methods in our discipline is classification. It is so common and so obvious to us that we rarely give it a second thought. Nevertheless, the classification of the total mass of artefacts according to certain criteria, and the then often subsequent comparison of the individual groups with each other, is basically the core of our subject. Often these groups are divided spatially and temporally. The whole concept of cultures, which continue to be important in the European Neolithic, is based on the subdivision of material remains on the basis of similarities and differences.\nIntuitively, archaeologists have thus established classes and groupings since the very beginnings of the discipline. As with many other things, the new archaeology in the 1960s was not entirely happy with this intuitive approach. This period saw the advent of various quantitative and scientific methods in archaeological research. On the one hand, there were calls to replace subjective decisions in the field of classification, but also in general, with objective or at least reproducible systems. On the other hand, there was a significantly larger amount of data that played an important role in the overarching analyses now taking place for the new questions of new archaeology. And for these objects, a large number of different characteristics were also recorded in order to be able to carry out an investigation that was as data-based as possible. They measured what they could measure and tried to create objective recording systems. This led to the formation of very complex data sets that were hardly intuitively comprehensible. Statistical methods were now used to master these records. In addition to simple univariate and bivariate statistics, multivariate statistics were considered particularly advanced and helpful.\nParticularly with regard to the establishment of groups, the procedures of cluster analysis deserve special attention. The basic idea of cluster analysis, which you will see in all the following procedures, but also in those that you will not find here, is essentially the same:\nThis is basically also the fundamental idea of classification as we have already described it above, and as it is also intuitively applied by us. Even if we usually express it in a less complex way.\nIf we want to imagine the whole process geometrically, it is easiest to stick to a two-dimensional example. Or at least to start from here.\nIn the figure above we can see different points in a diagram. Their perception is already influenced by the very suggestive outlines, colouring, and different symbols of the points. However, if you focus on the dots themselves, you will certainly, or most likely, agree with the grouping. In the upper left we have a group of regularly arranged triangles, below and further to the right a more irregular group of dots, and below that still further to the right a group of dots, again regularly arranged. All around are a few stars, which intuitively we would not necessarily assign to one of the groups.\nIn a moment we will get to know procedures with which we can reproduce this order (more or less well) by means of mathematical procedures. And it is no problem for mathematical methods to think in more than two dimensions. Basically, this increase in dimensionality is mostly trivial for mathematical algorithms.\nHere is the rendering of a data structure in three-dimensional space as it may result from three measurements. The representation makes it a little difficult for us to perceive the groups in space, as I suspect that, like me, you only have a two-dimensional monitor. However, if this representation were floating in front of us as a hologram, we would have no problems identifying groups here. The next step, however, becomes difficult: we simply lack the sensory organs as well as the mental abilities to be able to perceive more than three dimensions. This means that both in the representation and in the analysis of data structures that have more than three variables, we are necessarily dependent on simplifications. Or exactly procedures that can sort out groups for us without being subject to our limitations.\nSince this is a fundamental challenge that affects not only archaeology, but many different sciences (after all, the method of grouping is one that is needed by all scientific disciplines in one form or another), there are a large number of different methods to do this, adapted to different conditions or data qualities. We will start with a short introduction to classification methods. Further on, we will deal with a flavour of this cluster analysis that is probably most similar to the way a human mind would approach it, and which will therefore be the easiest for us to understand. In the practical part, I will only refer to one other variant, the rest is left to everyone’s own study. However, in the near future a specific self-study course on cluster analysis will be published by a group of which I am a member. At times, I will therefore refer to exactly this further presentation at this point.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#idea-and-basics",
    "href": "09-chapter.html#idea-and-basics",
    "title": "9  Cluster Analysis",
    "section": "",
    "text": "We measure some distance between the individual items in our study. This distance is in most cases a measure of similarity, for example, how metric variables will match up, or how many nominal variables will correspond to each other.\nWe then use these measurements to group the data so that similar objects end up in the same groups, so that the objects within the groups are more similar than the groups are to each other.\n\n\n\n\n\n\n:width 70%",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#methods",
    "href": "09-chapter.html#methods",
    "title": "9  Cluster Analysis",
    "section": "9.2 Methods",
    "text": "9.2 Methods\nIn the field of cluster analysis, as already indicated, there is a whole zoo of different approaches and methods to group things according to their similarity. A basic parameter for the classification of analysis methods is whether we already know from the outset how many groups we want to achieve in the end, or whether we want to explore the whole possibility of group aggregations.\n\n9.2.1 Hierarchical approach\nThe latter case can basically be called hierarchical. Hierarchical in the sense that we start at one end of the spectrum (either all objects are seen as one group, or we see each object as a group on its own), and then we divide or add together more and more objects until we finally reach the other end of the spectrum. In the intermediate steps we then have a large number of possible other subdivisions.\nHow this is to be understood will certainly become clearer if we look at the individual basic ideas separately. Let’s start with the so-called agglomerative clustering. Here we start with groupings that consist of the smallest possible units: the individual objects. So we start in a subdivision where each object is seen as a group on its own. Or in other words, we have as many groups as we have objects. Now we connect the two most similar objects so that they form a new group. This new group is our first cluster. This leaves us with a group number of only n-1. Since we have combined two objects, we now have one group less than we have individual objects. We now continue this process. In the next step, we again group together the two most similar objects. These can actually be individual objects or groups consisting of several objects. The process ends when we have combined all the objects into one large group, an overall cluster. In all the intermediate steps we have now represented the links between the most similar objects.\nHowever, we can also start the other way round: we start with a single large group where all objects together form the first cluster. Now we work according to the principle of dissimilarity: we divide our large total number of objects into two groups in such a way that they are as dissimilar as possible. Then we look at our two groups separately, and subdivide them again in such a way that two groups are as similar as possible. This is continued until finally every single object is divided into its own group. Again, we have all the intermediate steps, which gives us every possible number of clusters.\nAn example of such a hierarchical approach is the actual hierarchical clustering that is agglomerative in its methodology. The following is an illustration of the process as we have just discussed it, specific to the case of hierarchical clustering. Some intermediate steps are relatively clear and obvious, for others we are still missing some specifications. Above all, we do not currently know on what basis we will determine similarity or dissimilarity, which is nonchalantly named the distance matrix here. We will come back to this point later.\n\n\n\n\n9.2.2 Partitioning approach\nAs already mentioned, another possibility is that we want to achieve a fixed number of groups, or consider them as given. This is called partitioning the cluster. Basically, we have to answer the question of what is the best way to divide a certain data set into a certain number of groups. Basically, such an approach could look like this:\n\nselect n cluster centers randomly.\ncombine data most similar to these cluster centers\nrecalculate the cluster centers if necessary\nDoes anything change?\n\nIf the recalculated midpoints of our individual groups have still changed, i.e. if there has still been an exchange of members between the individual groupings, then we start again at step two. If, however, nothing else has changed in this run, and a recalculation of the cluster centres has again led to the same location, then we have hopefully reached the optimal subdivision, we are done.\nA practical method that is used in partitioning, and which is also widely used in science, is the so-called Kmeans clustering. We will get to know this procedure in more detail at the end of the session. In the following illustration, you can try to understand the algorithm with a graphical example.\n.center[ ]\n\n\n9.2.3 Hierarchical and partitioning: advantages and disadvantages\n\n9.2.3.1 Hierarchical\nAdvantage: One of the main advantages of the hierarchical cluster is that we do not have to specify how many clusters we expect in the end. We can deduce this directly from the data, or from the quality of the grouping, or with other tools. Often, the divisions found, or their sequence in hierarchical order, are reflected in a so-called dendrogram. In this we can trace the relationship and sequence of all the groupings of the individual data. You will get to know such dendrogrammes in a moment.\nDisadvantage: What we have just sold as an advantage can of course also be seen as a disadvantage: we get a large number of classifications and have to choose the best possible one. But what is a much more serious disadvantage is the fact that, due to this procedure, once a classification has been made, it cannot be undone, even if, with a finer subdivision, another cluster would perhaps be much better suited for a specific object. Due to its hierarchical nature, such an object cannot be subsequently reassigned to another cluster.\n\n\n9.2.3.2 Partitioning\nAdvantage: What we have just described as a disadvantage of the hierarchical version can therefore also be seen as an advantage of the partitioning one: the individual groupings are still flexible, since with each change in membership of a grouping, the cluster centres are redefined and a new assignment of the individual elements to the clusters is subsequently carried out. The reassignment of cluster membership is in fact one of the basic principles of this method. The fact that a better solution can be found after each round of clustering can lead to the overall result fitting the data better than would be the case with hierarchical clustering.\nDisadvantage: But this is also connected to the main disadvantage of this method: we have to know in advance how many groups we expect. We have to read this out of the data in a different way. One possibility, which we will discuss briefly at the end, is to first determine an optimal number of clusters with hierarchical clustering, and then to use a partitioning procedure.\nHowever, both methods have another disadvantage: each individual object is inevitably assigned to a cluster. The methods presented here cannot distinguish between objects that can actually be assigned to a group of typical properties and objects that cannot actually be assigned to any group at all. Think of the individual stars in the illustration above: the items marked with * are not actually members of any of the groups. At best, they are group members of the group Outliers. However, normal hierarchical and partitioning procedures will assign each individual object to a group. There are other methods, such as HDBSCAN, where this is not necessarily the case, but for which I would like to refer to an upcoming tutorial on cluster analysis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#distance-calculations",
    "href": "09-chapter.html#distance-calculations",
    "title": "9  Cluster Analysis",
    "section": "9.3 Distance calculations",
    "text": "9.3 Distance calculations\nNow that we have learned some fundamental logic and quite abstract algorithms for group subdivision, let’s move on to something more specific that is related to practice. One of the questions that has already emerged above with the general algorithms is how we can actually determine similarity and dissimilarity with our data. For this we need some kind of measure with which we can determine how close together our objects are, taking as a metaphor the coordinate system with the groups A, B and C that we had above. We will start from this familiar structure, where we begin with a coordinate system made up of variables, and use it to determine proximity and distance, similarity and dissimilarity. In the following, we will also get to know procedures that we can use to determine or measure similarity and dissimilarity between objects that are only described by nominal variables.\n\n9.3.1 How the crow flies (Euclidean distance (metric variables))\nIn relation to normal space, be it two-dimensional as in a coordinate system, or three-dimensional, we can easily determine which objects represent a group and which are distant from them. We have already done this by eye, in the example above.\nWe can also easily determine the distances mathematically if we just remember back a little to school lessons. The values on the individual variables, which here are to be equated with the axes of the coordinate system, can also be understood as rectangular lines. And the actual distance of two points to each other then results from the Pythagorean theorem:\nTheorem of Pythagoras…\n\\(a^2=b^2+c^2\\)\nIf we resolve the whole thing according to the individual coordinates, and take the root, then we get the following formula for two data points I and J, with respect to two variables X and Y:\n\\(d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\\)\nAnd in addition, the graphical representation of this relationship with respect to actually measured variables, in this case height and rim diameter.\n\n\n\n:width 75%\n\n\nThis is what we commonly understand by distance: the Euclidean distance. To be able to apply this, we have to be in the range of real measured values. And these measured values must actually be completely independent of each other. Moreover, the measure (the metric) must also be one that corresponds to our common notion of Euclidean space. That is, twice the value also corresponds to twice the distance. We therefore have ratio-scaled measurement values in relation to differences.\n\n\n9.3.2 How the taxy drives (Manhattan distance)\nBut why bother with complicated squaring when you can simply take the two measured values directly:\n\n\n\n:width 75%\n\n\nWhat we see in this figure is also a measure of the distance between two points. However, we are not using Euclidean distance here. Rather, this is the distance that a taxi driver in New York would have to cover in order to drive a passenger around the various blocks from one address to the other. Therefore, this distance or distance measure is also called Manhattan Metric or City Block Distance.\nIf we imagine that two variables are not independent of each other, that they correlate with each other, then the coordinate system formed by them will not function like a Euclidean space in terms of dissimilarity. The coordinate system itself will be warped or skewed. If we apply a Euclidean metric in such a situation, such as the Euclidean distance, then we would over- or underestimate the distances due to these distortions. One possible solution to this problem is the City Block distance. And it is also very easy to calculate:\n\\(d_{ij} = |x_i - x_j| + |y_i -y_j|\\)\nI am not sure how soon or urgently you will use this kind of distance metric in your research. But it should serve as an indicator that what we normally know as distance is not the only way to express distance. A large number of other distance metrics could certainly be added, but I will leave it to you at this point to find out more if you have the need.\n\n\n9.3.3 When distances can no longer be calculated (non-metric variables, presence/absence matrices)\nThat we must abandon our familiar concept of distances when it comes to variables where there are no real measured values, but only specifications of state, is obvious. What is not obvious in the same way is how to approach this problem. But intuitively one can quite easily come up with a starting point for solving this problem.\nWhat actually matters to us, at least in the context of groupings of similar objects, is similarity. That or, its inverse, dissimilarity. We can think of dissimilarity as distance, while similarity in terms of group membership can mean proximity.\nHowever, it is clear that if we have nominal or ordinal variables, we do not have defined distances between the individual values (I hope you can still remember this). Therefore, these distances cannot be calculated in a Euclidean space. The possible solution, as just indicated, is that we calculate via similarities, via similarity matrices, as we can compute them from contingency tables.\n(I have taken the part on calculating contingent values out of this year’s series of lectures. Therefore, you will not be familiar with this concept. But that is not a big problem, we will introduce the idea again at this point).\nAs you may recall (?), we can visualise the mutual relationships of two variable expressions, or their presence or absence, in a four-field table (think of the Chi Square test):\nExample burial inventories: We have two burials that contain certain artefacts. We can now consider the relationship in which the individual artefacts do or do not occur in each of the graves.\n\n\n\nBurial 1\nBurial 2\n\n\n\n\n\n\n+\n-\n\n\n+\na\nb\n\n\n-\nc\nd\n\n\n\nInstead of the real values, I have entered A, B, C & D here. The cell A marks the number of artefacts that occur in both burial 1 and burial 2. B marks the number of objects that occur in mode 1, but not in mode 2. C is the number of artefacts that occur in mode two, but not in mode one. Finally, d is the number of objects that do not occur in either burial 1 or burial 2.\nThus we now have characteristics with which we can determine which of these burials is more similar to the other. It stands to reason that if the two burials have many features in common, they will be more similar. We shall therefore extend this theoretical consideration with a practical example and use a number of types in our two burials:\n\n\n\nTypes\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nBurial 1\n1\n1\n0\n1\n0\n0\n1\n1\n1\n\n\nBurial 2\n1\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n\nUsing the example above, we can now count how many similarities and how many differences we have with regard to the categories in our four-field representation. With regard to type 1, type 7 and type 9, we can see that they occur in both burials. For our calculation of the difference, it does not matter for the time being for which of the types we have determined this (we do not want to weight the similarities at this point). Therefore, we simply enter the total number, i.e. 3, in our upper left column for common occurrence. And just in this way we can determine that type 3, 5, and type 6 do not appear in any of the two burials. We can also mark this number in the corresponding column row, the lower right. Finally, we note that Type 2, 4, and 8 appear in burial one, but not in burial two. Therefore, we also note this in the corresponding upper right cell. Conversely, there is no situation in which a type only occurs in burial two. The bottom left line therefore receives the value zero.\n\n\n\nBurial 1\nBurial 2\n\n\n\n\n\n\n+\n-\n\n\n+\n3 (a)\n3 (b)\n\n\n-\n0 (c)\n3 (d)\n\n\n\nIt is now quite obvious here that there are 2 diagonals in this table, which raise different interpretations. Our two cases become more similar when the upper left cell or the lower right cell becomes larger. They become more dissimilar when the upper right cell and the lower left cell become larger. The diagonal from top left to bottom right picks up the similarities, the diagonal from top right to bottom left the differences. There are different views on how to evaluate these individual cells, specifically for different situations and research questions. And from this, one can calculate different similarity (or dissimilarity) measures:\nSimple Matching\n\\(d = \\frac{a+d}{a+b+c+d}\\)\nPerhaps the simplest and most straightforward measure of similarity or dissimilarity is also called Simple Matching. It is, as the name suggests, simply about matching. And to calculate this, we add up the values in cells A & D, and divide by the total number of all cases. So we normalise the matches with respect to the total number of cases (artefacts in our case). This means that the more common types are present, the greater the value will be, but also the more types are common not to be present. For me, this largely corresponds to my first reaction to the question of how to judge the similarity of two objects. They must be as similar as possible in their characteristics, both in terms of the presence and absence of characteristics. This coefficient can be between zero and one. It becomes one when all cases are on the diagonal from top left to bottom right, i.e. when all characteristics match. It becomes zero when none of the properties match. In other words, a large value indicates high similarity, a small value indicates low similarity. Thus we have a similarity coefficient. To make it a distance measure, we can subtract it from one, for example. Then we have exactly the opposite case: large values indicate large dissimilarity, small values small dissimilarity.\nFor our example, the Simple Matching Coefficient is:\n\\(c_{SM} = \\frac{3+3}{3+3+0+3}=\\frac{6}{9}=\\frac{2}{3}\\)\n\\(d_{SM} = 1 - c_{SM} = \\frac{1}{3}\\)\nRussel & Rao (RR)\n\\(d = \\frac{a}{a+b+c+d}\\)\nIf we have already developed such an intuitively suitable coefficient for similarity, and derived a distance measure from it, why should we look any further at this point? We can consider the significance of the common absence of a certain artefact for the similarity of our burials. Which of these parameters we choose to use is, after all, a bit of an arbitrary matter. For example, if I examine our burials in terms of the absence of a DeLorean (who knows if Marty MacFly ever passed by here), then the addition of this category makes our two graves more similar. So we have to ask ourselves whether the common absence of an artefact category actually increases similarity in our case. If we come to the decision that this is not the case, then we can simply omit it from the calculation of similarity.\nWith this, we have worked out another similarity coefficient, which has also been proposed before by Russel & Rao. Here, the common dissimilarity is missing from the upper part of the formula, so we normalise only the common occurrence by the total number of objects.\nIn our example, this would result in the following values:\n\\(c_{RR} = \\frac{3}{3+3+0+3}=\\frac{3}{9}=\\frac{1}{3}\\)\n\\(d_{RR} = 1 - c_{RR} = \\frac{2}{3}\\)\nJaccard\n\\(d = \\frac{a}{a+b+c}\\)\nThe problem with the Russel & Rao coefficient is that somehow we still have the DeLorean in there. Although it no longer appears as a possibility to make the similarity stronger (i.e. in the upper part of the formula), it is still present in the lower part. That means, for all burials that don’t have a DeLorean (don’t worry, my references to “back to the future” will stop soon), the similarity values will be reduced by adding this category. This may also not make too much of a meaningful contribution to the comparison of two individual cases, or in this case, burials.\nDepending on how you understand it, the Jaccard index here either goes one step further, or maybe not. What it does is that it also removes the common non-existence from the lower part of the formula. It is therefore no longer weighted at all to what extent any objects do not occur in the individual cases. The only thing that is of interest is how many similarities and how many differences we have in relation to the objects that actually occur in both cases. Or, more generally, the properties that are common to both objects compared to the properties that are actually present in both objects.\nMathematically, the Jaccard coefficient will generally lie somewhere between Simple Matching and Russel & Rao. In our example, the result is as follows:\n\\(c_{J} = \\frac{3}{3+3+0}=\\frac{3}{6}=\\frac{1}{2}\\)\n\\(d_{RR} = 1 - c_{RR} = \\frac{1}{2}\\)\n\n9.3.3.1 Which similarity value should one choose?\nBesides the ones mentioned here, there is a large zoo of other possible similarity measures. For example, Simpson, Sneath, Tanimoto, Dice,… How should one find one’s way around here, or rather, which one should one choose? Certainly, it is useful here to start from basic considerations of the similarity sought. If a variable is symmetrical, i.e. both positive and negative values have the same weighting (e.g. gender), then we can use the Simple Matching Index. In the case of non-equality of variable values, or the fact that, for example, a negative value has less information content (I have tried to illustrate this a bit), then one should choose an asymmetric similarity measure, such as Jaccard. For most questions of similarity of archaeological objects it should apply. Nevertheless, it is always worthwhile to keep an eye out for a better similarity measure for the specific question one has.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#distance-calculation-in-r",
    "href": "09-chapter.html#distance-calculation-in-r",
    "title": "9  Cluster Analysis",
    "section": "9.4 Distance calculation in R",
    "text": "9.4 Distance calculation in R\nWe have now learned two measures of difference for metric variables, as well as three measures of similarity and dissimilarity in relation to nominal categories. So far, this has only been in theory, and you can only get a feeling for it if you try it out in practice. That’s why we’ll do it now in the following. We start with the two distance measures for metric values. Basically, R has the command dist(). At least for most standard distance measures this is sufficient. It understands Euclidean distance (which is of course the default setting) as well as Manhattan distance.\nFor this we use a data set that we have already seen above in the three-dimensional representation. These are various leather objects from a medieval excavation. Among other information, we also find the length, width and thickness of the leather pieces. This is data that can tell us about the possible original use of the objects, but above all about their subsequent fragmentation. And they are directly measured values, they actually represent physical measured distances of distances. This means that a Euclidean distance is absolutely the obvious choice here.\nFile: leather.csv\n\nleather &lt;- read.csv(\"leather.csv\")\ndist(leather[1:6,c(\"length\",\"width\",\"thickness\")],method=\"euclid\")\n\n          1         2         3         4         5\n2  4.475757                                        \n3  8.525843  4.080490                              \n4  8.576275  8.983034 10.511066                    \n5  9.797597  5.961753  3.257438  8.827820          \n6 11.892754 12.074088 12.985053  3.324169 10.697668\n\n\nIn the example above, I now only display the distance matrix for the first six entries in our leather data set. What we can see is that on the one hand this distance matrix has the shape of a triangle, and on the other hand the rows start with the number 1, but the columns with the number 2. The reason for this is quickly resolved: the distance from object one to object two is the same as the distance from object two to one. This property is called isotropy. Therefore, we only have to record the distances once in each case, which results in this diagonal structure. Let us now look at the values for the Manhattan distance:\n\ndist(leather[1:6,c(\"length\",\"width\",\"thickness\")],method=\"manhattan\")\n\n      1     2     3     4     5\n2  6.18                        \n3 11.90  5.72                  \n4 10.95 12.77 13.65            \n5 14.03  7.85  4.13  9.52      \n6 15.24 17.06 17.94  4.31 13.81\n\n\nThe first thing you notice is that the Manhattan distance has fewer decimal places. This is simply due to the fact that we are only adding two values together here, and that the original measured values did not have any greater accuracy than the second digit after the decimal point. Furthermore, all distance values are clearly larger. This is of course due to the fact that our taxi had to drive around the corner. And the ranking can also be different: Please look at the distance from 1 to 3 in Euclidean distance, and in Manhattan distance, and compare the distance with that between 1 and 4. You will see that here in Euclidean distance 3 is closer to 1 than 4, whereas in Manhattan distance 3 is further away from 1 than 4. So it definitely makes a difference which distance metrics you use. However, from my practice, in a case like this, I would simply choose the Euclidean distance.\nLet us now look at the different possible similarity measures and the distance measures based on them. To do this, we load a data set that represents the (simulated) presence and absence of different types in some graves of a cemetery.\nFile: burial_pa.csv\n\nburials &lt;- read.csv(\"burial_pa.csv\", row.names = 1)\nburials[1:2,]\n\n        V1 V2 V3 V4 V5 V6 V7 V8 V9\nburial1  1  1  0  1  0  0  1  1  1\nburial2  1  0  0  0  0  0  1  0  1\n\n\nYou will probably recognise this first case: it is the one we calculated by hand above. However, we have five more burials in our small data set.\nIn order to have a larger selection of similarity measures, I will load an external library at this point. This is the library vegan, which only indirectly has something to do with vegan nutrition. It is much more a library that originates from ecologoIy research, whose questions are similar in many ways to the questions of archaeology. Therefore, this package is often very practical for working on archaeological problems. Here we will remain only very coarsely on the surface, and make use of the distance measures. However, you will come across the package again in the next session.\nTo load a package that offers additional functionality, use the command library(). (maybe this is repetition for you).\n\nlibrary(vegan)\n\nNow we can use commands from this package. One of these commands is the command designdist(), which offers us a wider range of possible similarity measures. Here we can now use our desired formula for calculating the distance in various ways. Using the form I have given to you, we need to add the parameter abcd = TRUE.\nLet’s start with the simple matching index.\n\ndesigndist(burials, \"1 - (a+c)/(a+b+c+d)\", abcd = TRUE)\n\n          burial1   burial2   burial3   burial4   burial5   burial6\nburial2 0.6666667                                                  \nburial3 0.6666667 0.6666667                                        \nburial4 0.4444444 0.4444444 0.4444444                              \nburial5 0.4444444 0.4444444 0.4444444 0.4444444                    \nburial6 0.5555556 0.5555556 0.5555556 0.5555556 0.5555556          \nburial7 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n\n\nPlease note that we introduce -1 there, because otherwise we would calculate the similarity. But we want to calculate the dissimilarity (= distance). Therefore, we have to subtract the number of similarity from one. In the same way we can now work for other distance measures.\n\n# Russel & Rao\ndesigndist(burials, \"1 - (a)/(a+b+c+d)\", abcd = TRUE)\n\n          burial1   burial2   burial3   burial4   burial5   burial6\nburial2 0.6666667                                                  \nburial3 0.7777778 1.0000000                                        \nburial4 0.5555556 0.7777778 0.7777778                              \nburial5 0.6666667 0.8888889 0.7777778 0.6666667                    \nburial6 0.6666667 0.7777778 1.0000000 0.7777778 0.6666667          \nburial7 0.6666667 0.8888889 0.6666667 0.6666667 0.6666667 0.7777778\n\n\nAnd finally for the Jaccard index. This is also implemented directly in vegan in a function called vegdist(). I will run this here too for comparison:\n\ndesigndist(burials, \"1 - (a)/(a+b+c)\", abcd = TRUE)\n\n          burial1   burial2   burial3   burial4   burial5   burial6\nburial2 0.5000000                                                  \nburial3 0.7142857 1.0000000                                        \nburial4 0.4285714 0.6666667 0.6666667                              \nburial5 0.6250000 0.8571429 0.6666667 0.5714286                    \nburial6 0.5714286 0.6000000 1.0000000 0.7142857 0.5000000          \nburial7 0.6666667 0.8750000 0.5000000 0.6250000 0.6250000 0.7500000\n\nvegdist(burials,method=\"jaccard\")\n\n          burial1   burial2   burial3   burial4   burial5   burial6\nburial2 0.5000000                                                  \nburial3 0.7142857 1.0000000                                        \nburial4 0.4285714 0.6666667 0.6666667                              \nburial5 0.6250000 0.8571429 0.6666667 0.5714286                    \nburial6 0.5714286 0.6000000 1.0000000 0.7142857 0.5000000          \nburial7 0.6666667 0.8750000 0.5000000 0.6250000 0.6250000 0.7500000\n\n\n\nThe inventories of different (hypothetical) settlements are given.\nCalculate the appropriate distance matrix.\nFile: inv_settlement.csv\n\n\n\nSolution\n\nFirst, let’s take a look at this data set:\n\nsettlements &lt;- read.csv2(\"inv_settlement.csv\", row.names = 1)\nhead(settlements)\n\n  cups bowls beaker pots\n1    0     1      0    1\n2    1     0      1    1\n3    1     1      0    1\n4    1     0      1    1\n5    1     1      1    0\n6    0     1      1    1\n\n\nIt appears to be an presence absence dataset.Actually, one would have to definitively reassure this observation again, i.e. not just look at the header of the date record, but I can assure you that this is correct in this case.\nAs already indicated, a Jaccard metric is certainly the best choice in such a case. We now have the option to call this directly with vegdist() or to use our function designdist(). In practice, vegdist() is a bit shorter to write, so we use that function here.\n\nvegdist(settlements, method = \"ja\")\n\n           1         2         3         4         5         6         7\n2  0.7500000                                                            \n3  0.3333333 0.5000000                                                  \n4  0.7500000 0.0000000 0.5000000                                        \n5  0.7500000 0.5000000 0.5000000 0.5000000                              \n6  0.3333333 0.5000000 0.5000000 0.5000000 0.5000000                    \n7  0.6666667 0.3333333 0.7500000 0.3333333 0.7500000 0.3333333          \n9  0.6666667 0.7500000 0.7500000 0.7500000 0.3333333 0.3333333 0.6666667\n10 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.5000000\n11 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.5000000\n12 0.5000000 0.2500000 0.2500000 0.2500000 0.2500000 0.2500000 0.5000000\n13 0.3333333 0.5000000 0.5000000 0.5000000 0.5000000 0.0000000 0.3333333\n14 0.5000000 0.6666667 0.6666667 0.6666667 1.0000000 0.6666667 0.5000000\n16 0.0000000 0.7500000 0.3333333 0.7500000 0.7500000 0.3333333 0.6666667\n17 0.0000000 0.7500000 0.3333333 0.7500000 0.7500000 0.3333333 0.6666667\n18 1.0000000 0.6666667 1.0000000 0.6666667 0.6666667 0.6666667 0.5000000\n19 0.3333333 0.5000000 0.0000000 0.5000000 0.5000000 0.5000000 0.7500000\n           9        10        11        12        13        14        16\n2                                                                       \n3                                                                       \n4                                                                       \n5                                                                       \n6                                                                       \n7                                                                       \n9                                                                       \n10 1.0000000                                                            \n11 1.0000000 0.0000000                                                  \n12 0.5000000 0.7500000 0.7500000                                        \n13 0.3333333 0.6666667 0.6666667 0.2500000                              \n14 1.0000000 0.0000000 0.0000000 0.7500000 0.6666667                    \n16 0.6666667 0.5000000 0.5000000 0.5000000 0.3333333 0.5000000          \n17 0.6666667 0.5000000 0.5000000 0.5000000 0.3333333 0.5000000 0.0000000\n18 0.5000000 1.0000000 1.0000000 0.7500000 0.6666667 1.0000000 1.0000000\n19 0.7500000 0.6666667 0.6666667 0.2500000 0.5000000 0.6666667 0.3333333\n          17        18\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n9                     \n10                    \n11                    \n12                    \n13                    \n14                    \n16                    \n17                    \n18 1.0000000          \n19 0.3333333 1.0000000",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#hierarchical-clustering",
    "href": "09-chapter.html#hierarchical-clustering",
    "title": "9  Cluster Analysis",
    "section": "9.5 Hierarchical clustering",
    "text": "9.5 Hierarchical clustering\n\n9.5.1 Cluster analysis by hand\nNow that we have calculated the distances (a not insignificant step that also definitely strongly influences the overall result), we can proceed to perform the actual cluster analysis. As with the calculation of the distance matrix, many of the steps for performing the actual cluster analysis are very similar, at least for hierarchical clustering. The main differences are again how distances are calculated, in terms of the new distances that result from combining multiple objects into a cluster.\nBut let’s leave that aside for a moment, we will come back to the differences in a moment. First, let’s go through the general process. To do this, we will use the ‘single linkage’ method. At this point, just enjoy the beauty of a statistical term for now. What s’single linkage’ means is best explained in contrast with other procedures, which we will come to in a moment.\nHere we use the example of different types of margarine that I have taken from the book “Multivariate Statistik” by Backhaus et al.\nFirst of all, it is important which distance matrix you have. In the end, it will not be that important, because in the actual process we will only evaluate which objects are closest to each other. But in the given case, the distance matrix consists of a quasi Euclidean dissimilarity matrix based on user surveys. We simply use it here like a Euclidean distance matrix. Here, too, the structure is such that we omit the upper diagonal (which contains the same values as the lower part), and that the individual margarine varieties are slightly offset in the rows and columns. In the rows the first type is missing, in the columns the last. This is comparable to the representation that R gives us.\n\n\n\n\nRama\nHoma\nFlora\nSB\n\n\n\n\nHoma\n6\n\n\n\n\n\nFlora\n4\n6\n\n\n\n\nSB\n56\n26\n44\n\n\n\nWeihnachtsbutter\n75\n41\n59\n11\n\n\n\nWe will now carry out a hierarchical clustering here. This means, if you remember above, that in the first stage we assume that each individual object has its own group. And as you may also remember, the first step is now that we determine which of the individual objects are most similar to each other in order to combine them into a cluster. Since we have a distance matrix here, we are looking for the shortest distance. As your eagle eye will have discovered, the shortest distance is between Flora and Rama. We combine these two into a new group, our first cluster. We also note the distance between these two objects (i.e. 4) at which we have made the grouping here. This will become important again later.\nclustering: {4}\nNow we come to the question of how we recalculate the distance between the data in relation to our new grouping. I have already named the process single linkage. In this variant, we are looking for the smallest distance that any member of our cluster has to all other objects (currently all other margarine varieties) as the distance for the entire group (in this case composed of two margarine varieties) to all other objects.\nSo we look at our table, and especially at the columns where our two new cluster members appear. The distance from Rama and Flora to Homa is 6 in both cases, in which case we don’t have to think at all about which new distance to take. We can ignore the distances between Rama and Flora, because we combine the two. What is relevant now is the distance of our new cluster to the SB variety. To this, Rama has a distance of 56, while Flora has a distance of 44. Whatever these units of measurement would mean.\nSo now we look for the smallest distance of all our class members to this variety, as the new total distance of our cluster. So we look at the nearest neighbour in each case. In this case, it is Flora with a distance of 44. Thus, the new cluster to be created, consisting of our two margarine varieties, has a total distance to SB of 44. Do the same with the last margarine variety and we have all our new distances together and can create a new distance matrix.\n\n\n\n\nRama\nHoma\nFlora\nSB\n\n\n\n\nHoma\n6\n\n\n\n\n\nFlora\n4\n6\n\n\n\n\nSB\n56\n26\n44\n\n\n\nWeihnachtsbutter\n75\n41\n59\n11\n\n\n\n\n\n\n\nRama, Flora\nHoma\nSB\n\n\n\n\nHoma\n6\n\n\n\n\nSB\n44\n26\n\n\n\nWeihnachtsbutter\n59\n41\n11\n\n\n\nIt is easy to see that in this new distance matrix, the smallest distance is between our cluster (Rama, Flora) and the Homa variety. So we now combine them into our new cluster, and note the cluster distance of six.\nclustering: {4, 6}\nAnd now again, as before, we create a new distance matrix. In the case of the single linkage method, by selecting the smallest distances in each case.\n\n\n\n\nRama, Flora, Homa\nSB\n\n\n\n\nSB\n26\n\n\n\nWeihnachtsbutter\n41\n11\n\n\n\nFinally, in the now very shrunken distance matrix, the smallest distance in the next step is the one between the SB variety and the Weihnachtsbutter variety. So we combine these two into a new cluster, and note the distance at which we did that.\nclustering: {4, 6, 11}\nThe smallest distance between our cluster (SB, Weihnachtsbutter) and the cluster (Rama, Flora, Homa) is now 26. And basically we are done with that now.\n\n\n\n\nRama, Flora, Homa\n\n\n\n\nSB, Weihnachtsbutter\n26\n\n\n\nThe final step is to combine these two clusters, which we do at the distance of 26. Finally, we also note this distance for our entire cluster solution.\nclustering: {4, 6, 11, 26}\n\n\n9.5.2 Dendrogram\nNow that we have carried out the cluster analysis (wasn’t that difficult, was it), we need a suitable way to present the result. For this purpose, the dendrogram has proved to be a good way of presenting the results. It is no coincidence that this term comes from the Latin word for tree. A dendrogram serves to visualise the processes of a cluster analysis, or the hierarchy in which these clusters are located.\n\n\n\n\n\n\n\n\n\nSo we see here, so to speak, a sequence of what we have just done. You can read the whole thing from the bottom up if you want to follow this sequence. First we united Rama and Flora into one group. Then Homa was added. Then SB and Weihnachtsbutter were joined together, and finally all the individual objects united into one big cluster. The heights at which these combinations are entered correspond to the distances at which we made this combination.\nclustering: {4, 6, 11, 26}\nIf we now look at the whole thing from top to bottom, we can see that our entire data set primarily breaks down into two groups: on the right-hand side we have SB and Weihnachtsbutter, which are obviously more similar to each other than to any other of the margarine varieties. On the left side we have these (Homa, Rama, Flora). The differences in the distances of the groups between the individual margarine varieties are not very different. The only big step we can see here is the one at the end. That is, the cluster on the left is very clearly different from the cluster on the right. At this point, due to the data situation, but also due to the very suitable representation, one would probably divide the data into two groups that correspond to our two initial clusters. So we have now carried out a cluster analysis completely from front to back. And with that, you have all the things together that you need to understand cluster analyses, as well as to implement them yourself. Except for the command in R. But before I give you that, you need to take a brief look at a few other cluster methods.\n\n\n9.5.3 Other methods\nThere is a very wide range of possible ways in which hierarchical clustering is carried out specifically. They all differ primarily in how the distances are calculated that result when two data are combined in a cluster. Here is just a small selection:\nComplete linkage process:\nThis is basically the reverse of the single linkage method. Not the closest neighbour to a patch, but the furthest neighbour to a cluster is noted as the new distance for the entire cluster when data is merged.\nAverage Linkage Procedure\nIn this case, neither the nearest nor the furthest neighbour is evaluated, but the average distance of all data is taken into account as the new distance.\nWard method\nThis method is a little different from the ones just described. It is also based on a distance matrix, but in this case the selection process is the decisive one. The objects are grouped together for which this grouping would result in the lowest variance of the newly created cluster. Accordingly, no new distances are actually calculated here. This sounds a bit more complicated than what we just went through. Admittedly. However, this method is also very popular, and not without reason: it produces the groupings that come closest to what one would select by eye. In this context, one speaks of natural groups.\nTo visualise the differences, let’s use the groupings we have seen before:\n\n\n\n:width 70%\n\n\nNow we go through these groups step by step for different cluster algorithms and look at what kind of groups they would produce for this arrangement of data.\nSingle Linkage\n\nIn the case of single linkage, first the 6 outliers are separated from all other values. Next we see a separation of C to A and B. However, the procedure does not succeed in separating A & B in a meaningful way because they are very close to each other. It is also said that single linkage tends to chain formation. This means that the nearest neighbour is always chosen, i.e., from the left uppermost B the procedure jumps over to the right lowermost member of A without any problems. What still worked very well with our margarines has to be taken with greater caution in more difficult applications with more data.\nAverage linkage\nThe average linkage method is more successful. Here, A, B and C are finally separated. However, with this method B is also divided into two groups, even before B & A separate. Here we do not have such a clear chain formation (no such clear stairs in the dendrogram), but the result is also unsatisfactory, and in this case the outliers also separate quite late from the actual groups.\n\nWard\nThe best way (or the way we would do it, too) to cluster is with the Ward method. Here, groups A, B and C are the first to separate. And also with a clear distance to all other separations. However, this method is forced to assign the outliers to a group, which is then also carried out (perhaps reluctantly, it is so difficult to look into an algorithm).\n\nWhich method we may use to fuse the individual clusters also depends on what kind of distance matrix we have. If we are working with similarity measures, the Ward method (as well as the centroid and median methods) cannot be used meaningfully. In this case, the average linkage method can be used.\n\n\n9.5.4 Praktische Durchführung in R\n\n\n9.5.5 With metric data (Distance matrix)\nIf we have metric data and can therefore use a real distance matrix as a basis, then basically (unless you have really good reasons) there is no way around the Ward method if you want to stay with hierarchical clustering.\nAdvantage: usually finds “natural” groups of approximately same size best\nDisadvantage: is only applicable for metrically scaled variables\nWhat I have sold above as an advantage can also be seen as a disadvantage: ward is bad at identifying very small groups, but tends to lump them together with larger groups.\nLet’s do the whole thing once directly in R, using our leather data as an example. If you have not yet done so, please leave this data in:\n\nleather &lt;- read.csv(\"leather.csv\")\n\nNext, we calculate the distance matrix. At this point, however, we only consider the length, width and thickness. There are other data in the data set that are not important for our grouping at this point.\n\nleather.dist &lt;- dist(leather[,c(\"length\",\n                                \"width\",\n                                \"thickness\")],\n                     method=\"euclid\")\n\nFinally, we feed our distance matrix into the cluster analysis method: hclust(). For this we need to explicitly state that we want to use Ward’s method.\n\nleather.hclust &lt;- hclust(leather.dist, method = \"ward\")\n\nThe \"ward\" method has been renamed to \"ward.D\"; note new \"ward.D2\"\n\n\nAnd with that, the cluster analysis has already been carried out. All that remains is to present the result. For now, we will limit ourselves to a denrogramme.\n\nplot(leather.hclust)\n\n\n\n\n\n\n\n\nWe can see that first two objects (27 and 49) are separated. Then two large groups result, in which the rightmost one is subdivided again a little further down. Whether we finally consider the subdivision of the rightmost group into two subgroups and further subdivisions to be relevant for us depends on various factors. On the one hand, we can look at the data at this point, and from information that we have not then used for classification, try to make an interpretation of the groups found in the cluster here. In all cases, this will certainly provide the best justification for selecting a particular cluster number. Science always beats pure statistics. Another way to be able to identify a good number of clusters we will get to know in a moment. But first, let’s look at the same approach in the realm of nominal data and similarity (or dissimilarity) matrices.\n(What you may also see is the little warning message that says that the ward method has now been renamed ward.D. The method listed here as Ward is not the one originally proposed in the publication by Ward (1963). If you want to use exactly this one, then you have to explicitly specify Ward.D2. It does make sense to take this into account, as the results can differ significantly. Feel free to try out what happens when you cluster the leather objects with Ward.D2.)\n\n\n9.5.6 with nominal data (Dissimilarity matrix)\nIf we start from nominal data and therefore cannot calculate a real distance matrix, but only a dissimilarity matrix, then a good choice is certainly the average linkage variant of the cluster analysis. In this case, the new distance is formed by the average of all pairwise compared members of two clusters (or single objects). In contrast to single or complete linkage, not only individual neighbours but all members of a cluster are included in the assessment of the new distances to all other objects.\nAdvantage: can also be used with nominally scaled variables, takes into account all elements of a cluster when redetermining the distances\nDisadvantage: Not as well suited as Ward to create “natural” groups\nFor demonstration purposes, we use the presence-absence matrix for the burials that we already used in the introductory example. Basically, the procedure here is no different from the case where we can use metric variables. Again, the first thing we have to do is to determine a distance matrix (in this case a dissimilarity matrix) between all objects. For this we use the vegdist() function of vegan, with the Jaccard method. We then pass this matrix to the hclust() function. And again we specify the method explicitly, we use average.\n\nburials &lt;- read.csv(\"burial_pa.csv\", row.names = 1)\n\nburials.dist &lt;- vegdist(burials,\n                     method=\"jacc\")\n\nburials.hclust&lt;-hclust(burials.dist,method=\"average\")\n\nFinally, we can also plot the dendrogram in this case simply by using the plot() command.\n\nplot(burials.hclust)\n\n\n\n\n\n\n\n\nOur small test data set does not show too much variability, so cluster analysis is not particularly exciting at this point. However, the data set is clear, so we can simply look at it in connection with the cluster analysis.\n\nburials.hclust\n\n\nCall:\nhclust(d = burials.dist, method = \"average\")\n\nCluster method   : average \nDistance         : jaccard \nNumber of objects: 7 \n\n\nIn the Analysis class, the group of burial three and burial seven are separated from all other objects. These differ from the other data sets primarily in variable one and two. Next, object five and six separate themselves, which have three concordances, and then a chain of 2, 1, and 4 is actually formed, which is certainly mainly due to the last 3 variables (“V7”, “V8”, and “V9”) of our simulated data set.\nNow that we have learned about the possibility in R (finally I have lifted the veil) with which we can calculate cluster analysis here, two questions remain open: how do I determine the proper number of clusters, and how do I use the results of my cluster analysis (beyond the pure dendrogram) for further analyses and representations.\nLet us first (very cursorily) address the question of the number of clusters.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#number-of-clusters",
    "href": "09-chapter.html#number-of-clusters",
    "title": "9  Cluster Analysis",
    "section": "9.6 Number of Clusters",
    "text": "9.6 Number of Clusters\nSo the question is how many groups we actually want to have at the end of our clustering. Sometimes this question can be answered directly from the scientific background, and these are also the situations where cluster analysis can be most successful. If we already know that a certain number of groups should emerge, then it is usually also the case that it is much easier to identify these groups.\nBut even if we do not know in advance how many groups we can expect, it still makes sense to approach the resulting groupings with contextual knowledge, to interpret them, so as not to be dependent solely on number games. Finally, one can also try to read the dendrogramme and derive a meaningful number of groups from it. The criterion that I would like to present here does basically nothing else, but makes it a little easier for the user. It is called: elbow criterion.\nIn dendrograms, we can observe that some clusterings occur at significantly larger distances from each other than others. If such a large jump in distance is followed by significantly smaller distances, this means that these cluster distances represent rather small subdivisions. Often this takes the form of a staircase, as we can see from the single linkage. These links (or subdivisions, depending on which direction of the dendrogram one is coming from) are certainly less significant than those that happen after a large jump. To visualise this better, one can visualise the distances at which the clustering occurs.\nThe variable that holds the result of a cluster analysis has several other bits of information stored. For example, the height at which the cluster groupings took place.\n\nleather.hclust$height\n\n [1]   0.2692582   0.3001666   0.3163858   0.3168596   0.4134005   0.4716991\n [7]   0.5834381   0.6327717   0.6575713   0.8015610   0.8101852   0.9340771\n[13]   1.0894494   1.1056672   1.1112384   1.2816006   1.3428934   1.3652286\n[19]   1.4243946   1.5251722   1.5952062   1.8872829   2.0136782   2.0909630\n[25]   2.0977726   2.1580349   2.3312702   2.4234389   2.4477594   2.7754099\n[31]   3.2550858   4.1743981   4.3136458   4.5522688   4.8097554   5.2936066\n[37]   7.2905470   8.6182051   9.4921020   9.7341971   9.8335414  10.8425070\n[43]  11.0793745  12.0008333  18.6200777  28.3504012  29.6217128  42.0407188\n[49]  79.9956782 137.5943981 157.8697407\n\n\nThe whole sequence is in the order in which the clusters were put together. I.e., the smallest distances (and thus the clusters that were merged first) are found at the beginning, the latest ( nearer to the root of the dendrogram) are found at the end. At this end are also those subdivisions that coincide with the smallest number of groups (and the largest group size). If we now plot these higher ones in reverse order as a line diagram, we can observe points at which we can detect a clear kink in the sequence of cluster heights. This kink, which represents the situation when we come from a large step to a small step in the cluster development, is also called an elbow. And these need to be identified. Let’s take a look at this for our leather data set:\n\nplot(rev(leather.hclust$height)[1:10],type=\"l\")\n\n\n\n\n\n\n\n\nFor the sake of clarity, I will only show the first ten clusters. If you look closely, you can see several elbows here. This makes it clear that this elbow criterion is not only not informed by domain knowledge, but is also shaped by subjective decisions. Nevertheless, it can be a good indication for making a decision. In this case, the earliest clear elbow might be the one at a cluster count of 5. However, I would have little quarrel with it if you preferred the four. But if we take the five, we would now determine that we would expect five clusters for our analysis.\n\n9.6.1 Tree care and visualisation\nIf we have now decided on a certain number of clusters, we might want to pick out the membership of the individual objects to these clusters, for example, to display them in scatter plots with their group membership. We will come to this in this section.\nThe function to determine the assignment of objects to certain clusters at a certain cluster height is called cuttree(). As the first parameter, this function expects the result of a cluster analysis. Furthermore, you can either specify the number of desired groups or the height at which these groups should be cut (the same as you just visualised for the elbow criterion). It is usually easiest to enter the number of groups at this point. The result is then a vector that reflects the membership of the clusters that are present at this height of the tree, or according to the number of clusters that you want to identify.\n\ncutree(leather.hclust,5)\n\n [1] 1 1 2 3 1 3 2 2 2 1 1 2 1 2 2 1 1 1 1 3 4 4 4 3 2 2 5 2 2 2 2 1 1 2 2 3 2 2\n[39] 1 1 1 3 1 2 2 1 2 2 5 1 1 1\n\n\nThis sequence of numbers comes in the same order as the data originally used to perform the cluster analysis. It can therefore be very handy to append the result to the original data set, in a new column, to keep track of the cluster membership just calculated. Let’s do this with our leather dataset:\n\nleather$clusters &lt;- cutree(leather.hclust,4)\n\nNow, in addition to the actual measurements, which we have already used to determine the clustering, we also have the cluster membership stored in the data set. And we can use this to enhance the representation of the variables with a representation of cluster membership, either indicated by colours, or by symbols. I’m going to go for the colours here. And I will produce what is called a pair plot, that is, I will plot the relationships of several variables against each other. All that is needed is an input for the command plot() that has more than two columns:\n\nplot(leather[,c(\"length\", \"width\", \"thickness\")],\n     col=rainbow(4)[leather$clusters])\n\n\n\n\n\n\n\n\nFor the actual colour representation I use a colour palette that I create with the help of the command rainbow(). Here I enter the number of colours I want the rainbow spectrum to be divided into (perhaps you remember, we already went through this at the very beginning in Graphic Representation). From this colour spectrum I then select one of the colours according to my class order, in our example 1-5 (because we have a maximum of five clusters).\n\nCeramics with various decorative elements\nGiven are ceramic artefacts with different properties.\nDetermine which distance measure is appropriate, calculate the distance matrix and carry out a cluster analysis using a suitable method.\nDetermine a good cluster solution and display the dendrogram.\nFile: ceramics.csv\n\n\n\nSolution\n\nFirst, let’s again take a look at this data set:\n\nceramics &lt;- read.csv2(\"ceramics.csv\", row.names = 1)\nhead(ceramics)\n\n  Gegenstaendige.Kerben senkrechtes.Fischgraetmuster Waagerechte.Linien\n1                     1                            1                  0\n2                     1                            1                  1\n3                     1                            1                  1\n4                     1                            1                  0\n5                     0                            1                  1\n6                     0                            1                  0\n  Horizontal.angeordnete.Ritzlinien Vertikales.Leiterbandmuster\n1                                 0                           0\n2                                 0                           0\n3                                 0                           0\n4                                 0                           1\n5                                 1                           0\n6                                 0                           1\n  Haengende.halbmondfoermige.Linienfuehrung geritztes.Kreuz Waagrechte.Oesen\n1                                         1               1                0\n2                                         0               1                0\n3                                         0               1                1\n4                                         0               1                0\n5                                         0               1                0\n6                                         0               0                0\n  Schlickrauhung haengende.gefuellte.Dreiecke verdickte.Randlippe\n1              1                            1                   1\n2              0                            1                   1\n3              0                            1                   1\n4              1                            1                   1\n5              0                            1                   1\n6              0                            1                   0\n  plastische.Zier\n1               0\n2               0\n3               0\n4               0\n5               0\n6               0\n\n\nAgain, we are dealing with a presence absence data set. As already explained, in this case it is best to use the Jaccard metric.\n\nlibrary(vegan)\nceramics.dist &lt;- vegdist(ceramics, method = \"ja\")\n\nWith the distance matrix we can now perform the actual cluster analysis. Since we are dealing with a non-metric data set, we play it safe and use ‘average linkage’ as the cluster algorithm.\n\nceramics.clust &lt;- hclust(ceramics.dist)\n\nLet’s take a look at the dendrogram:\n\nplot(ceramics.clust)\n\n\n\n\n\n\n\n\nThe data set does not seem to have any really unique cluster solutions. This is perhaps not least due to the fact that I have simulated the data randomly. Nevertheless, we proceed canonically and try to identify the best possible cluster separation with the help of the elbow criterion.\n\nplot(rev(ceramics.clust$height), type=\"l\")\n\n\n\n\n\n\n\n\nEven this representation does not provide a really clear picture, which does not surprise us very much, since it directly follows the dendrogram.\nHere is a pro tip on how to identify the right number of clusters. For this we do not start from the hierarchical dendrogram as before, but ask for individual cluster solutions how well the individual objects fit to the cluster assigned to them. This is done on the so-called silhouette criterion, which we will not go into here in particular, but for which I refer to the general tutorial on clustering in archaeology that will be coming out soon. Until then, we simply apply this cluster criterion. For this we need to load another package that deals specifically with cluster analysis. Appropriately, this package is called cluster. I will comment on the individual steps in the code itself.\n\nlibrary(cluster) # Load package\n\nmax_cluster &lt;- 20 # We look at (the last) 20 cluster solutions\n\ngoodness_of_fit &lt;- vector(length = max_cluster)\n\nfor(i in 2:max_cluster) { # perform the following for each cluster solution\n  this_solution &lt;- cutree(ceramics.clust, i)  # cut the tree at the given number of clusters\n  this_silhouette &lt;- silhouette(this_solution, ceramics.dist) # Calculate the silhouette\n  goodness_of_fit[i] &lt;- mean(this_silhouette[,3]) # calculate the average silhouette value\n}\ngoodness_of_fit\n\n [1] 0.0000000 0.3083618 0.2676181 0.2380518 0.2393591 0.2762821 0.2749820\n [8] 0.2913602 0.3528556 0.3373536 0.3779374 0.3491796 0.3228159 0.3060295\n[15] 0.2808977 0.2548108 0.2312083 0.1863703 0.1639863 0.1581892\n\n\nThe result is a value that reflects the average fit of each individual to their assigned cluster. The numerical values alone may be difficult to comprehend, so we will show the entire result again as a bar chart:\n\nbarplot(goodness_of_fit, names.arg = 1:max_cluster)\n\n\n\n\n\n\n\n\nFrom this analysis, it appears that either two or eleven groups are probably the most useful cluster solution. With all possible intermediate levels. We opt here for a cluster solution based on two groups.\nFor the visualisation of the complex data, we can use a method that we will only go into more intensively in the next session: correspondence analysis. At this point, just enjoy the beautiful point cloud for now.\n\nplot(cca(ceramics), type=\"n\")\npoints(cca(ceramics), col = cutree(ceramics.clust,2))\n\n\n\n\n\n\n\n\nWhen dividing our data into two groups, they can be well separated from each other in the correspondence analysis graph.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#non-hierarchical-clustering",
    "href": "09-chapter.html#non-hierarchical-clustering",
    "title": "9  Cluster Analysis",
    "section": "9.7 Non-hierarchical clustering",
    "text": "9.7 Non-hierarchical clustering\nIf we can already start with a given number of desired clusters, then we can also use non-hierarchical clustering methods. Or, we can first use a hierarchical clustering method and then use the resulting optimal number of clusters to perform a non-hierarchical clustering method.\nHere we use the so-called Kmeans clustering. In each step, the clusters are reassembled and new distances are calculated. If the solution is as optimal as possible, the procedure stops (see above).\nAs the name Kmeans already implies, a mean value is determined here at a certain point. Such a mean value only really makes sense for metric variables. Therefore, this procedure is also limited to situations in which we can specify such a metric.\nFile: andean_sites.csv\nFirst of all, in the well-known way, hierarchical clustering. We have counts, so we can use the Ward method.\n\nandean &lt;- read.csv2(\"andean_sites.csv\", row.names = 1)\nandean.hclust&lt;-hclust(dist(andean),method=\"ward\")\n\nThe \"ward\" method has been renamed to \"ward.D\"; note new \"ward.D2\"\n\nplot(rev(andean.hclust$height),type=\"l\")\n\n\n\n\n\n\n\n\nFrom the representation, an optimal cluster number of, for example, three can be deduced. We use this as a default for the non-hierarchical clustering method Kmeans. The command for this is kmeans(). The first parameter is the data matrix, the second is the desired number of clusters.\n\nandean.kmeans&lt;-kmeans(andean,3)\nplot(andean,col=andean.kmeans$cluster)\n\n\n\n\n\n\n\n\nThe last command is a visualisation in which all variables are displayed against all others, as we have already seen in other contexts (pairwise plot). At this point, I pass on the cluster solution as colours so that the points are displayed differently. What we can see is that our cluster solution can be distinguished well, especially with regard to the amount of maize. At this point we have to do without in dendrograms for representation, since we do not have a hierarchical order that could be represented here.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "09-chapter.html#schluss-und-überleitung",
    "href": "09-chapter.html#schluss-und-überleitung",
    "title": "9  Cluster Analysis",
    "section": "9.8 Schluss und Überleitung",
    "text": "9.8 Schluss und Überleitung\nAs has become clear in the last example, but above all in the exercise example, it is often difficult to represent data sets in a few (2) dimensions, showing structures that depend on many variables and their relationships to each other. We often have such problems in relation to archaeological situations where, for example, we can identify how particular burials differ in terms of their features and classify them into groups. The grouping can be done with the help of cluster analysis. Visualisation, on the other hand, is a different field. This is the field we will tackle in the next chapter: how can I represent complex structures in as few dimensions as possible, and at the same time pick out the essentials of the data set.\nOverall, such procedures are called ordination methods. In the field of metric data, one of the most frequently used approaches here is principal component analysis. For nominal data, which we often have, there is an alternative: correspondence analysis. You can look forward to the many scatter plots that await you in the following chapter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  }
]